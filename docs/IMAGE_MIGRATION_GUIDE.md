# å›¾ç‰‡è¿ç§»å¤„ç†æŒ‡å—

## ğŸ“‹ å›¾ç‰‡ç±»å‹

### 1. **å°é¢å›¾ç‰‡ï¼ˆCover Imageï¼‰**
- å­—æ®µï¼š`ArticlePage.cover`
- æ¥æºï¼šæ—§æ•°æ®åº“çš„ `article.img` å­—æ®µ
- å­˜å‚¨ï¼šWagtailçš„ `CustomImage` æ¨¡å‹

### 2. **æ­£æ–‡å›¾ç‰‡ï¼ˆInline Imagesï¼‰**
- ä½ç½®ï¼šæ–‡ç« æ­£æ–‡HTMLä¸­çš„ `<img>` æ ‡ç­¾
- æ¥æºï¼šæ—§æ•°æ®åº“çš„ `article_info.info` å­—æ®µ
- é—®é¢˜ï¼šå¯èƒ½ä»æŒ‡å‘æ—§æœåŠ¡å™¨URL

---

## âœ… å½“å‰å®ç°ï¼ˆå·²æœ‰åŠŸèƒ½ï¼‰

### å°é¢å›¾ç‰‡å¤„ç† âœ…

```python
# apps/news/management/commands/import_old_articles.py

def download_and_create_image(self, image_url, title):
    """ä¸‹è½½å›¾ç‰‡å¹¶åˆ›å»ºCustomImage"""
    if not image_url or not image_url.startswith('http'):
        return None

    try:
        # 1. ä¸‹è½½å›¾ç‰‡
        response = requests.get(image_url, timeout=10)
        response.raise_for_status()

        # 2. è·å–æ–‡ä»¶å
        filename = os.path.basename(image_url.split('?')[0])
        
        # 3. åˆ›å»ºCustomImageå¯¹è±¡
        image = CustomImage(title=title[:100])
        image.file.save(filename, ContentFile(response.content), save=True)
        
        return image
    except Exception as e:
        # è®°å½•å¤±è´¥ä½†ä¸ä¸­æ–­å¯¼å…¥
        return None
```

**ç‰¹ç‚¹**ï¼š
- âœ… è‡ªåŠ¨ä¸‹è½½å›¾ç‰‡åˆ°æœåŠ¡å™¨
- âœ… ä¿å­˜ä¸ºWagtailçš„CustomImage
- âœ… å¤±è´¥ä¸ä¸­æ–­æ•´ä½“å¯¼å…¥
- âœ… æ”¯æŒè·³è¿‡å›¾ç‰‡ä¸‹è½½ï¼ˆ`--skip-images`ï¼‰

---

## âš ï¸ éœ€è¦å®Œå–„çš„éƒ¨åˆ†

### æ­£æ–‡å›¾ç‰‡å¤„ç† âš ï¸

**å½“å‰é—®é¢˜**ï¼š
```python
def convert_html_to_richtext(self, html):
    """è½¬æ¢HTMLä¸ºWagtail RichTextæ ¼å¼"""
    # å½“å‰åªæ¸…ç†HTMLï¼Œæ²¡æœ‰å¤„ç†å›¾ç‰‡URL
    soup = BeautifulSoup(html, 'html.parser')
    for tag in soup(['script', 'style']):
        tag.decompose()
    return str(soup)
```

**é—®é¢˜**ï¼š
```html
<!-- å¯¼å…¥åçš„æ­£æ–‡å¯èƒ½åŒ…å«æ—§æœåŠ¡å™¨çš„å›¾ç‰‡URL -->
<img src="http://old-server.com/uploads/2024/image.jpg">
<!-- å¦‚æœæ—§æœåŠ¡å™¨å…³é—­ï¼Œå›¾ç‰‡å°†æ— æ³•æ˜¾ç¤º -->
```

---

## ğŸ¯ å®Œæ•´è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šä¸‹è½½æ‰€æœ‰æ­£æ–‡å›¾ç‰‡ï¼ˆæ¨èï¼‰âœ¨

**ä¼˜åŠ¿**ï¼š
- âœ… å®Œå…¨ç‹¬ç«‹ï¼Œä¸ä¾èµ–æ—§æœåŠ¡å™¨
- âœ… å›¾ç‰‡æ°¸ä¹…ä¿å­˜
- âœ… åŠ è½½é€Ÿåº¦å¿«

**åŠ£åŠ¿**ï¼š
- âŒ å¯¼å…¥æ—¶é—´é•¿
- âŒ å ç”¨å­˜å‚¨ç©ºé—´
- âŒ å¯èƒ½æœ‰å›¾ç‰‡ä¸‹è½½å¤±è´¥

**å®ç°æ­¥éª¤**ï¼š

```python
def convert_html_to_richtext(self, html):
    """è½¬æ¢HTMLå¹¶ä¸‹è½½å›¾ç‰‡"""
    if not html:
        return ''
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # ç§»é™¤scriptå’Œstyle
    for tag in soup(['script', 'style']):
        tag.decompose()
    
    # å¤„ç†æ‰€æœ‰imgæ ‡ç­¾
    if not self.options['skip_images']:
        for img_tag in soup.find_all('img'):
            old_url = img_tag.get('src')
            if old_url and old_url.startswith('http'):
                # ä¸‹è½½å¹¶æ›¿æ¢URL
                new_image = self.download_and_create_image(
                    old_url, 
                    'inline-image'
                )
                if new_image:
                    # æ›¿æ¢ä¸ºæ–°çš„å›¾ç‰‡URL
                    img_tag['src'] = new_image.file.url
                else:
                    # ä¸‹è½½å¤±è´¥ï¼Œä¿ç•™åŸURLæˆ–ç§»é™¤
                    pass
    
    return str(soup)
```

### æ–¹æ¡ˆ2ï¼šURLé‡å†™ï¼ˆå¿«é€Ÿæ–¹æ¡ˆï¼‰âš¡

**ä¼˜åŠ¿**ï¼š
- âœ… å¯¼å…¥é€Ÿåº¦å¿«
- âœ… ä¸å ç”¨å­˜å‚¨ç©ºé—´
- âœ… å®ç°ç®€å•

**åŠ£åŠ¿**ï¼š
- âŒ ä¾èµ–æ—§æœåŠ¡å™¨
- âŒ éœ€è¦é…ç½®åå‘ä»£ç†

**å®ç°**ï¼š
```python
def convert_html_to_richtext(self, html):
    """é‡å†™å›¾ç‰‡URL"""
    if not html:
        return ''
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # ç§»é™¤scriptå’Œstyle
    for tag in soup(['script', 'style']):
        tag.decompose()
    
    # é‡å†™å›¾ç‰‡URL
    for img_tag in soup.find_all('img'):
        old_url = img_tag.get('src')
        if old_url:
            # æ›¿æ¢ä¸ºä»£ç†URL
            if old_url.startswith('http://old-server.com'):
                img_tag['src'] = old_url.replace(
                    'http://old-server.com',
                    'https://new-server.com/legacy-images'
                )
    
    return str(soup)
```

### æ–¹æ¡ˆ3ï¼šæ··åˆæ–¹æ¡ˆï¼ˆå¹³è¡¡ï¼‰âš–ï¸

**ç­–ç•¥**ï¼š
- å°é¢å›¾ç‰‡ï¼šä¸‹è½½åˆ°æœ¬åœ° âœ…
- æ­£æ–‡å›¾ç‰‡ï¼šå…ˆå°è¯•ä¸‹è½½ï¼Œå¤±è´¥åˆ™ä¿ç•™åŸURL

**å®ç°**ï¼š
```python
def download_inline_image(self, image_url):
    """ä¸‹è½½æ­£æ–‡å›¾ç‰‡ï¼ˆå¸¦é‡è¯•å’Œé™çº§ï¼‰"""
    try:
        # å°è¯•ä¸‹è½½
        image = self.download_and_create_image(image_url, 'inline')
        if image:
            return image.file.url
        else:
            return image_url  # å¤±è´¥åˆ™è¿”å›åŸURL
    except:
        return image_url  # å¤±è´¥åˆ™è¿”å›åŸURL
```

---

## ğŸ› ï¸ æ¨èå®ç°

### å¢å¼ºç‰ˆå›¾ç‰‡å¤„ç†

æˆ‘å»ºè®®åˆ›å»ºä¸€ä¸ªå¢å¼ºç‰ˆçš„ `convert_html_to_richtext` å‡½æ•°ï¼š

```python
def convert_html_to_richtext(self, html):
    """è½¬æ¢HTMLä¸ºWagtail RichTextæ ¼å¼å¹¶å¤„ç†å›¾ç‰‡"""
    if not html:
        return ''

    try:
        soup = BeautifulSoup(html, 'html.parser')
        
        # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
        for tag in soup(['script', 'style']):
            tag.decompose()

        # å¤„ç†å›¾ç‰‡ï¼ˆå¦‚æœæœªè·³è¿‡å›¾ç‰‡ä¸‹è½½ï¼‰
        if not self.options.get('skip_images'):
            image_count = 0
            for img_tag in soup.find_all('img'):
                old_src = img_tag.get('src')
                
                # è·³è¿‡ç©ºsrcæˆ–ç›¸å¯¹è·¯å¾„
                if not old_src:
                    continue
                
                # è½¬æ¢ç›¸å¯¹è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
                if not old_src.startswith('http'):
                    old_src = urljoin(self.options.get('old_site_url', ''), old_src)
                
                # ä¸‹è½½å›¾ç‰‡
                try:
                    new_image = self.download_and_create_image(
                        old_src,
                        f'inline-image-{image_count}'
                    )
                    
                    if new_image:
                        # æ›¿æ¢ä¸ºæ–°URL
                        img_tag['src'] = new_image.file.url
                        # ä¿ç•™altå±æ€§
                        if not img_tag.get('alt'):
                            img_tag['alt'] = f'Image {image_count}'
                        image_count += 1
                    else:
                        # ä¸‹è½½å¤±è´¥ï¼Œæ ¹æ®é€‰é¡¹å†³å®š
                        if self.options.get('remove_failed_images'):
                            img_tag.decompose()  # ç§»é™¤å›¾ç‰‡
                        else:
                            pass  # ä¿ç•™åŸURL
                            
                except Exception as e:
                    self.stdout.write(self.style.WARNING(
                        f'  æ­£æ–‡å›¾ç‰‡å¤„ç†å¤±è´¥: {old_src[:50]}... - {str(e)}'
                    ))
        
        # å…¶ä»–æ¸…ç†...
        cleaned_html = str(soup)
        return cleaned_html
        
    except Exception as e:
        self.stdout.write(self.style.WARNING(f'  HTMLè½¬æ¢è­¦å‘Š: {str(e)}'))
        return html
```

---

## ğŸ“Š å¯¼å…¥å‘½ä»¤å‚æ•°

### å½“å‰å‚æ•°
```bash
python manage.py import_old_articles \
  --file data/migration/exports/articles.json \
  --skip-images              # è·³è¿‡æ‰€æœ‰å›¾ç‰‡ä¸‹è½½
  --limit 10                 # é™åˆ¶å¯¼å…¥æ•°é‡
  --batch-size 100           # æ‰¹å¤„ç†å¤§å°
```

### å»ºè®®æ–°å¢å‚æ•°
```bash
python manage.py import_old_articles \
  --skip-cover-images        # è·³è¿‡å°é¢å›¾ç‰‡
  --skip-inline-images       # è·³è¿‡æ­£æ–‡å›¾ç‰‡
  --remove-failed-images     # ç§»é™¤ä¸‹è½½å¤±è´¥çš„å›¾ç‰‡
  --old-site-url http://old.com  # æ—§ç«™ç‚¹URLï¼ˆç”¨äºç›¸å¯¹è·¯å¾„è½¬æ¢ï¼‰
  --max-image-size 5242880   # æœ€å¤§å›¾ç‰‡å¤§å°ï¼ˆ5MBï¼‰
  --image-timeout 30         # å›¾ç‰‡ä¸‹è½½è¶…æ—¶ï¼ˆç§’ï¼‰
```

---

## ğŸ“ å›¾ç‰‡å­˜å‚¨ä½ç½®

### Wagtailé»˜è®¤å­˜å‚¨
```python
# CustomImage ä¿å­˜è·¯å¾„
MEDIA_ROOT/images/
  â”œâ”€â”€ original_images/
  â”‚   â””â”€â”€ image_abc123.jpg      # åŸå§‹å›¾ç‰‡
  â””â”€â”€ images/                   # å„ç§å°ºå¯¸çš„ç¼©ç•¥å›¾
      â”œâ”€â”€ image_abc123.2e16d0ba.fill-200x200.jpg
      â””â”€â”€ image_abc123.2e16d0ba.fill-800x600.jpg
```

### é…ç½®ï¼ˆsettings.pyï¼‰
```python
# åª’ä½“æ–‡ä»¶é…ç½®
MEDIA_URL = '/media/'
MEDIA_ROOT = BASE_DIR / 'media'

# Wagtailå›¾ç‰‡é…ç½®
WAGTAILIMAGES_MAX_UPLOAD_SIZE = 10 * 1024 * 1024  # 10MB
```

---

## ğŸ” å›¾ç‰‡ç»Ÿè®¡

### å¯¼å…¥å®Œæˆåçš„ç»Ÿè®¡

```python
print(f"å›¾ç‰‡å¯¼å…¥ç»Ÿè®¡ï¼š")
print(f"  å°é¢å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {self.stats['images_downloaded']}")
print(f"  å°é¢å›¾ç‰‡ä¸‹è½½å¤±è´¥: {self.stats['images_failed']}")
print(f"  æ­£æ–‡å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {self.stats['inline_images_downloaded']}")
print(f"  æ­£æ–‡å›¾ç‰‡ä¸‹è½½å¤±è´¥: {self.stats['inline_images_failed']}")
```

---

## ğŸš€ å®æ–½æ­¥éª¤

### æ­¥éª¤1ï¼šæ£€æŸ¥æ—§ç³»ç»Ÿå›¾ç‰‡æƒ…å†µ

```bash
# 1. æŸ¥çœ‹æœ‰å¤šå°‘æ–‡ç« æœ‰å°é¢å›¾ç‰‡
ssh root@121.41.73.49 "docker exec \$(docker ps | grep mysql | awk '{print \$1}') \
  mysql -ujrhb -p'6VSPmPbuFGnZO1%C' jrhb -e \
  'SELECT COUNT(*), COUNT(img) FROM article WHERE img IS NOT NULL AND img != \"\"'"

# 2. æŸ¥çœ‹å›¾ç‰‡URLæ¨¡å¼
ssh root@121.41.73.49 "docker exec \$(docker ps | grep mysql | awk '{print \$1}') \
  mysql -ujrhb -p'6VSPmPbuFGnZO1%C' jrhb -e \
  'SELECT img FROM article WHERE img IS NOT NULL LIMIT 10'"
```

### æ­¥éª¤2ï¼šæµ‹è¯•å¯¼å…¥ï¼ˆå¸¦å›¾ç‰‡ï¼‰

```bash
# æµ‹è¯•10æ¡ï¼Œä¸‹è½½å›¾ç‰‡
python manage.py import_old_articles --test --limit=10

# æµ‹è¯•10æ¡ï¼Œè·³è¿‡å›¾ç‰‡
python manage.py import_old_articles --test --limit=10 --skip-images
```

### æ­¥éª¤3ï¼šæŸ¥çœ‹å›¾ç‰‡æ˜¯å¦æˆåŠŸ

```python
# Django Shell
from apps.news.models import ArticlePage
from apps.media.models import CustomImage

# æŸ¥çœ‹å¯¼å…¥çš„æ–‡ç« 
articles = ArticlePage.objects.all()[:10]
for a in articles:
    print(f"{a.title[:30]}: cover={a.cover is not None}")

# æŸ¥çœ‹å›¾ç‰‡æ•°é‡
print(f"æ€»å›¾ç‰‡æ•°: {CustomImage.objects.count()}")
```

### æ­¥éª¤4ï¼šæ­£å¼å¯¼å…¥

```bash
# åˆ†æ‰¹å¯¼å…¥ï¼Œå¸¦å›¾ç‰‡
python manage.py import_old_articles --batch-size=1000
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### å¹¶å‘ä¸‹è½½å›¾ç‰‡

```python
from concurrent.futures import ThreadPoolExecutor

def download_images_batch(self, image_urls):
    """æ‰¹é‡å¹¶å‘ä¸‹è½½å›¾ç‰‡"""
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {
            executor.submit(
                self.download_and_create_image, url, f'img-{i}'
            ): url for i, url in enumerate(image_urls)
        }
        
        results = {}
        for future in futures:
            url = futures[future]
            try:
                results[url] = future.result()
            except Exception as e:
                results[url] = None
        
        return results
```

---

## ğŸ“ æ€»ç»“

### æ¨èæ–¹æ¡ˆï¼ˆæ ¹æ®æƒ…å†µé€‰æ‹©ï¼‰

| åœºæ™¯ | æ–¹æ¡ˆ | å‘½ä»¤ |
|------|------|------|
| **å®Œå…¨è¿ç§»** | ä¸‹è½½æ‰€æœ‰å›¾ç‰‡ | `--batch-size=1000` |
| **å¿«é€Ÿæµ‹è¯•** | è·³è¿‡å›¾ç‰‡ | `--skip-images --limit=100` |
| **æ—§æœåŠ¡å™¨ä¿ç•™** | URLé‡å†™ | è‡ªå®šä¹‰å®ç° |
| **æ··åˆæ–¹æ¡ˆ** | å°é¢ä¸‹è½½ï¼Œæ­£æ–‡ä¿ç•™ | `--skip-inline-images` |

### ä¸‹ä¸€æ­¥

æ‚¨æƒ³è¦å“ªç§æ–¹æ¡ˆï¼Ÿæˆ‘å¯ä»¥ï¼š

1. âœ… **å®Œå–„å½“å‰è„šæœ¬**ï¼šå¢å¼ºæ­£æ–‡å›¾ç‰‡å¤„ç†
2. âœ… **å…ˆæµ‹è¯•å¯¼å…¥**ï¼šçœ‹çœ‹å½“å‰å°é¢å›¾ç‰‡ä¸‹è½½æ•ˆæœ
3. âœ… **æ£€æŸ¥æ—§ç³»ç»Ÿ**ï¼šæŸ¥çœ‹å›¾ç‰‡URLæ ¼å¼å’Œæ•°é‡
4. âœ… **æä¾›é…ç½®å»ºè®®**ï¼šå­˜å‚¨ã€æ€§èƒ½ç­‰

**æ‚¨å¸Œæœ›å¦‚ä½•å¤„ç†å›¾ç‰‡ï¼Ÿ**

