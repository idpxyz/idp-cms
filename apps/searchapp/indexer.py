class ArticleIndexer:
    """æ–‡ç« ç´¢å¼•å™¨ - å°† Wagtail é¡µé¢è½¬æ¢ä¸º OpenSearch æ–‡æ¡£"""
    
    def __init__(self, target_site=None, enable_hotness_tagging=True):
        """
        åˆå§‹åŒ–ç´¢å¼•å™¨
        :param target_site: ç›®æ ‡ç«™ç‚¹æ ‡è¯†ç¬¦ï¼Œå¦‚æœæŒ‡å®šï¼Œå°†è¦†ç›–page.get_site()çš„ç»“æœ
        :param enable_hotness_tagging: æ˜¯å¦å¯ç”¨çƒ­åº¦æ ‡è®°ï¼ˆåŠ¨æ€è®¡ç®—hot/trendingï¼‰
        """
        self.target_site = target_site
        self.enable_hotness_tagging = enable_hotness_tagging
    
    def to_doc(self, page) -> dict:
        """å°†é¡µé¢è½¬æ¢ä¸ºç´¢å¼•æ–‡æ¡£"""
        # æ ‡ç­¾
        try:
            tags = list(page.tags.values_list("name", flat=True)) if hasattr(page, "tags") else []
        except Exception:
            tags = []
        
        # åˆ†ç±»ï¼ˆä½¿ç”¨ slugï¼‰
        categories = []
        try:
            if hasattr(page, "categories"):
                categories = list(page.categories.values_list("slug", flat=True))
        except Exception:
            categories = []
        
        # ç«™ç‚¹æ ‡è¯†
        if self.target_site:
            site_identifier = self.target_site
        else:
            try:
                site_identifier = page.get_site().hostname
            except Exception:
                site_identifier = "localhost"
        
        # é¢‘é“ä¿¡æ¯
        primary_channel_slug = None
        try:
            if getattr(page, "channel", None):
                primary_channel_slug = getattr(page.channel, "slug", None)
        except Exception:
            primary_channel_slug = None
        
        # URL ä¸æ—¶é—´
        try:
            url = page.url
        except Exception:
            url = None
        # ç»Ÿä¸€å‘å¸ƒæ—¶é—´é€»è¾‘ï¼šä½¿ç”¨WagtailåŸç”Ÿæ—¶é—´ä½œä¸ºä¸»è¦æº
        publish_at = None
        try:
            # ä¼˜å…ˆä½¿ç”¨ Wagtail çš„ first_published_atï¼ˆæ›´å¯é ï¼‰
            wagtail_time = getattr(page, "first_published_at", None)
            custom_time = getattr(page, "publish_at", None)
            
            if wagtail_time:
                publish_at = wagtail_time.isoformat()
            elif custom_time:
                # å›é€€åˆ°è‡ªå®šä¹‰æ—¶é—´ï¼ˆå…¼å®¹ç°æœ‰æ•°æ®ï¼‰
                publish_at = custom_time.isoformat()
        except Exception:
            publish_at = None
        
        # è¯­è¨€
        lang_code = "zh"
        try:
            if getattr(page, "language", None):
                lang_code = getattr(page.language, "code", "zh")
        except Exception:
            pass
        
        # æ„å»ºåŸºç¡€æ–‡æ¡£
        doc = {
            "article_id": str(page.id),
            "slug": getattr(page, "slug", None),
            "site": site_identifier,
            "tenant": site_identifier,
            "url": url,
            "title": getattr(page, "title", ""),
            "summary": getattr(page, "summary", getattr(page, "excerpt", "")),
            "body": self._extract_body_text(page),
            # ğŸ¯ æ–°å¢é‡è¦æœç´¢å­—æ®µ
            "excerpt": getattr(page, "excerpt", ""),  # ä¸“é—¨çš„æ–‡ç« æ‘˜è¦
            "search_description": getattr(page, "search_description", ""),  # SEOæè¿°
            "seo_title": getattr(page, "seo_title", ""),  # SEOæ ‡é¢˜
            "topics": self._extract_topics(page),  # ä¸»é¢˜æ ‡ç­¾
            "author": getattr(page, "author_name", ""),
            "tags": tags,
            "categories": categories,
            "primary_channel_slug": primary_channel_slug or "recommend",
            # Ensure compatibility with query templates expecting 'channel' and 'publish_time'
            "channel": primary_channel_slug or "recommend",
            # é¢„ç•™ï¼špath/secondary å¯åœ¨éœ€è¦æ—¶æ‰©å±•
            "has_video": bool(getattr(page, "has_video", False)),
            "region": getattr(page, "region", "global"),
            "first_published_at": publish_at,
            "publish_time": publish_at,
            # ğŸ”¥ æ–°å¢ç»Ÿè®¡å­—æ®µç”¨äºæ’åºå’Œè¿‡æ»¤
            "view_count": getattr(page, "view_count", 0),
            "comment_count": getattr(page, "comment_count", 0),
            "like_count": getattr(page, "like_count", 0),
            "favorite_count": getattr(page, "favorite_count", 0),
            "reading_time": getattr(page, "reading_time", 0),
            "updated_at": getattr(page, "updated_at", None),
            "source_type": getattr(page, "source_type", "internal"),
            "pop_1h": 0.0, "pop_24h": 0.0, "ctr_1h": 0.0, "ctr_24h": 0.0,
            "quality_score": 1.0,
            "lang": lang_code,
            # ğŸ¯ æ·»åŠ Heroæ ‡è®°å’Œå…¶ä»–é‡è¦å­—æ®µç”¨äºè¿‡æ»¤
            "is_hero": bool(getattr(page, "is_hero", False)),
            "is_featured": bool(getattr(page, "is_featured", False)),
            "weight": float(getattr(page, "weight", 0)),
        }
        
        # ğŸ”¥ çƒ­åº¦æ ‡è®°ï¼šåŠ¨æ€è®¡ç®—å¹¶æ·»åŠ è™šæ‹Ÿé¢‘é“æ ‡ç­¾
        if self.enable_hotness_tagging:
            doc = self._add_hotness_tags(doc, page)
        
        return doc
    
    def _extract_body_text(self, page) -> str:
        """
        ä»é¡µé¢ä¸­æå–bodyæ–‡æœ¬å†…å®¹
        """
        try:
            # é¦–å…ˆå°è¯•ä»å®é™…çš„bodyå­—æ®µè·å–å†…å®¹
            if hasattr(page, 'body') and page.body:
                import re
                # ä»RichTextFieldä¸­ç§»é™¤HTMLæ ‡ç­¾ï¼Œæå–çº¯æ–‡æœ¬
                text_content = re.sub(r'<[^>]+>', '', str(page.body))
                # æ¸…ç†å¤šä½™çš„ç©ºç™½
                text_content = ' '.join(text_content.split())
                if text_content:
                    return text_content[:2000]  # é™åˆ¶é•¿åº¦ï¼Œé¿å…ç´¢å¼•è¿‡å¤§
            
            # å›é€€åˆ°åŸæœ‰é€»è¾‘
            return getattr(page, "search_description", None) or getattr(page, "introduction", "") or ""
        except Exception:
            # å‡ºé”™æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œä¸å½±å“ç´¢å¼•è¿‡ç¨‹
            return ""
    
    def _extract_topics(self, page) -> list:
        """
        ä»é¡µé¢ä¸­æå–topicsä¸»é¢˜æ ‡ç­¾
        """
        topics = []
        try:
            if hasattr(page, 'topics') and page.topics:
                for topic in page.topics.all():
                    if hasattr(topic, 'name'):
                        topics.append(topic.name)
                    elif hasattr(topic, 'title'):
                        topics.append(topic.title)
                    else:
                        topics.append(str(topic))
        except Exception:
            pass
        return topics
    
    def _add_hotness_tags(self, doc: dict, page) -> dict:
        """
        æ·»åŠ çƒ­åº¦æ ‡è®°ï¼ŒåŠ¨æ€ç”Ÿæˆ hot/trending è™šæ‹Ÿé¢‘é“æ ‡ç­¾
        """
        try:
            from apps.core.services.hotness_calculator import get_hotness_score
            
            article_id = str(page.id)
            site = doc.get('site', 'localhost')
            
            # è·å–çƒ­åº¦è¯„åˆ†å’Œåˆ†ç±»
            hotness_score, category = get_hotness_score(article_id, site)
            
            # æ·»åŠ çƒ­åº¦ç›¸å…³å­—æ®µ
            doc.update({
                'hotness_score': hotness_score,
                'hotness_category': category,
            })
            
            # ğŸ¯ å…³é”®ï¼šæ ¹æ®åˆ†ç±»è®¾ç½® channel å­—æ®µ
            if category == 'hot':
                doc['channel'] = 'hot'
            elif category == 'trending':
                doc['channel'] = 'trending'
            # else: ä¿æŒåŸæœ‰çš„ primary_channel_slug
            
            # ä¿ç•™åŸå§‹é¢‘é“ä¿¡æ¯ï¼ˆç”¨äºå…¶ä»–æŸ¥è¯¢ï¼‰
            if 'channel' in doc and doc['channel'] in ['hot', 'trending']:
                doc['original_channel'] = doc.get('primary_channel_slug', 'recommend')
            
        except Exception as e:
            # å®¹é”™ï¼šå¦‚æœçƒ­åº¦è®¡ç®—å¤±è´¥ï¼Œä¸å½±å“åŸºç¡€ç´¢å¼•
            import logging
            logging.getLogger(__name__).warning(f"çƒ­åº¦æ ‡è®°å¤±è´¥ {article_id}: {e}")
            doc.update({
                'hotness_score': 0.0,
                'hotness_category': 'normal',
            })
        
        return doc

# å‘åå…¼å®¹
def article_to_doc(page) -> dict:
    """å‘åå…¼å®¹çš„å‡½æ•°"""
    indexer = ArticleIndexer()
    return indexer.to_doc(page)
