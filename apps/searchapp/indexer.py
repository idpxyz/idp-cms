class ArticleIndexer:
    """æ–‡ç« ç´¢å¼•å™¨ - å°† Wagtail é¡µé¢è½¬æ¢ä¸º OpenSearch æ–‡æ¡£"""
    
    def __init__(self, target_site=None, enable_hotness_tagging=True):
        """
        åˆå§‹åŒ–ç´¢å¼•å™¨
        :param target_site: ç›®æ ‡ç«™ç‚¹æ ‡è¯†ç¬¦ï¼Œå¦‚æœæŒ‡å®šï¼Œå°†è¦†ç›–page.get_site()çš„ç»“æœ
        :param enable_hotness_tagging: æ˜¯å¦å¯ç”¨çƒ­åº¦æ ‡è®°ï¼ˆåŠ¨æ€è®¡ç®—hot/trendingï¼‰
        """
        self.target_site = target_site
        self.enable_hotness_tagging = enable_hotness_tagging
    
    def to_doc(self, page) -> dict:
        """å°†é¡µé¢è½¬æ¢ä¸ºç´¢å¼•æ–‡æ¡£"""
        # æ ‡ç­¾
        try:
            tags = list(page.tags.values_list("name", flat=True)) if hasattr(page, "tags") else []
        except Exception:
            tags = []
        
        # åˆ†ç±»ï¼ˆä½¿ç”¨ slugï¼‰
        categories = []
        try:
            if hasattr(page, "categories"):
                categories = list(page.categories.values_list("slug", flat=True))
        except Exception:
            categories = []
        
        # ç«™ç‚¹æ ‡è¯†
        if self.target_site:
            site_identifier = self.target_site
        else:
            try:
                site_identifier = page.get_site().hostname
            except Exception:
                site_identifier = "localhost"
        
        # é¢‘é“ä¿¡æ¯
        primary_channel_slug = None
        try:
            if getattr(page, "channel", None):
                primary_channel_slug = getattr(page.channel, "slug", None)
        except Exception:
            primary_channel_slug = None
        
        # URL ä¸æ—¶é—´
        try:
            url = page.url
        except Exception:
            url = None
        # ç»Ÿä¸€å‘å¸ƒæ—¶é—´é€»è¾‘ï¼šä½¿ç”¨WagtailåŸç”Ÿæ—¶é—´ä½œä¸ºä¸»è¦æº
        publish_at = None
        try:
            # ä¼˜å…ˆä½¿ç”¨ Wagtail çš„ first_published_atï¼ˆæ›´å¯é ï¼‰
            wagtail_time = getattr(page, "first_published_at", None)
            custom_time = getattr(page, "publish_at", None)
            
            if wagtail_time:
                publish_at = wagtail_time.isoformat()
            elif custom_time:
                # å›é€€åˆ°è‡ªå®šä¹‰æ—¶é—´ï¼ˆå…¼å®¹ç°æœ‰æ•°æ®ï¼‰
                publish_at = custom_time.isoformat()
        except Exception:
            publish_at = None
        
        # è¯­è¨€
        lang_code = "zh"
        try:
            if getattr(page, "language", None):
                lang_code = getattr(page.language, "code", "zh")
        except Exception:
            pass
        
        # ğŸ–¼ï¸ æå–å›¾ç‰‡URLï¼ˆå°é¢æˆ–æ­£æ–‡ç¬¬ä¸€å¼ å›¾ç‰‡ï¼‰
        image_url, cover_image_url = self._extract_image_urls(page)
        
        # æ„å»ºåŸºç¡€æ–‡æ¡£
        doc = {
            "article_id": str(page.id),
            "slug": getattr(page, "slug", None),
            "site": site_identifier,
            "tenant": site_identifier,
            "url": url,
            "title": getattr(page, "title", ""),
            "summary": getattr(page, "summary", getattr(page, "excerpt", "")),
            "body": self._extract_body_text(page),
            # ğŸ¯ æ–°å¢é‡è¦æœç´¢å­—æ®µ
            "excerpt": getattr(page, "excerpt", ""),  # ä¸“é—¨çš„æ–‡ç« æ‘˜è¦
            "search_description": getattr(page, "search_description", ""),  # SEOæè¿°
            "seo_title": getattr(page, "seo_title", ""),  # SEOæ ‡é¢˜
            "topics": self._extract_topics(page),  # ä¸»é¢˜æ ‡ç­¾
            "author": getattr(page, "author_name", ""),
            "tags": tags,
            "categories": categories,
            "primary_channel_slug": primary_channel_slug or "recommend",
            # Ensure compatibility with query templates expecting 'channel' and 'publish_time'
            "channel": primary_channel_slug or "recommend",
            # é¢„ç•™ï¼špath/secondary å¯åœ¨éœ€è¦æ—¶æ‰©å±•
            "has_video": bool(getattr(page, "has_video", False)),
            "region": getattr(page, "region", "global"),
            "first_published_at": publish_at,
            "publish_time": publish_at,
            # ğŸ”¥ æ–°å¢ç»Ÿè®¡å­—æ®µç”¨äºæ’åºå’Œè¿‡æ»¤
            "view_count": getattr(page, "view_count", 0),
            "comment_count": getattr(page, "comment_count", 0),
            "like_count": getattr(page, "like_count", 0),
            "favorite_count": getattr(page, "favorite_count", 0),
            "reading_time": getattr(page, "reading_time", 0),
            "updated_at": getattr(page, "updated_at", None),
            "source_type": getattr(page, "source_type", "internal"),
            "pop_1h": 0.0, "pop_24h": 0.0, "ctr_1h": 0.0, "ctr_24h": 0.0,
            "quality_score": 1.0,
            "lang": lang_code,
            # ğŸ¯ æ·»åŠ Heroæ ‡è®°å’Œå…¶ä»–é‡è¦å­—æ®µç”¨äºè¿‡æ»¤
            "is_hero": bool(getattr(page, "is_hero", False)),
            "is_featured": bool(getattr(page, "is_featured", False)),
            "weight": float(getattr(page, "weight", 0)),
            # ğŸ–¼ï¸ å›¾ç‰‡å­—æ®µ
            "image_url": image_url,
            "cover_image_url": cover_image_url,
        }
        
        # ğŸ”¥ çƒ­åº¦æ ‡è®°ï¼šåŠ¨æ€è®¡ç®—å¹¶æ·»åŠ è™šæ‹Ÿé¢‘é“æ ‡ç­¾
        if self.enable_hotness_tagging:
            doc = self._add_hotness_tags(doc, page)
        
        return doc
    
    def _extract_body_text(self, page) -> str:
        """
        ä»é¡µé¢ä¸­æå–bodyæ–‡æœ¬å†…å®¹
        """
        try:
            # é¦–å…ˆå°è¯•ä»å®é™…çš„bodyå­—æ®µè·å–å†…å®¹
            if hasattr(page, 'body') and page.body:
                import re
                # ä»RichTextFieldä¸­ç§»é™¤HTMLæ ‡ç­¾ï¼Œæå–çº¯æ–‡æœ¬
                text_content = re.sub(r'<[^>]+>', '', str(page.body))
                # æ¸…ç†å¤šä½™çš„ç©ºç™½
                text_content = ' '.join(text_content.split())
                if text_content:
                    return text_content[:2000]  # é™åˆ¶é•¿åº¦ï¼Œé¿å…ç´¢å¼•è¿‡å¤§
            
            # å›é€€åˆ°åŸæœ‰é€»è¾‘
            return getattr(page, "search_description", None) or getattr(page, "introduction", "") or ""
        except Exception:
            # å‡ºé”™æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œä¸å½±å“ç´¢å¼•è¿‡ç¨‹
            return ""
    
    def _extract_topics(self, page) -> list:
        """
        ä»é¡µé¢ä¸­æå–topicsä¸»é¢˜æ ‡ç­¾
        """
        topics = []
        try:
            if hasattr(page, 'topics') and page.topics:
                for topic in page.topics.all():
                    if hasattr(topic, 'name'):
                        topics.append(topic.name)
                    elif hasattr(topic, 'title'):
                        topics.append(topic.title)
                    else:
                        topics.append(str(topic))
        except Exception:
            pass
        return topics
    
    def _extract_image_urls(self, page) -> tuple:
        """
        æå–å›¾ç‰‡URL
        ä¼˜å…ˆä½¿ç”¨å°é¢å›¾ç‰‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»æ­£æ–‡æå–ç¬¬ä¸€å¼ å›¾ç‰‡
        è¿”å›: (image_url, cover_image_url) å…ƒç»„
        """
        cover_image_url = None
        image_url = None
        
        try:
            # 1ï¸âƒ£ å°è¯•è·å–å°é¢å›¾ç‰‡
            if hasattr(page, 'cover') and page.cover:
                cover_image_url = self._get_image_url(page.cover)
                image_url = cover_image_url  # ä¼˜å…ˆä½¿ç”¨å°é¢å›¾ç‰‡
            
            # 2ï¸âƒ£ å¦‚æœæ²¡æœ‰å°é¢ï¼Œå°è¯•ä»æ­£æ–‡æå–ç¬¬ä¸€å¼ å›¾ç‰‡
            if not image_url and hasattr(page, 'extract_first_image_from_body'):
                first_image = page.extract_first_image_from_body()
                if first_image:
                    image_url = self._get_image_url(first_image)
        except Exception as e:
            import logging
            logging.getLogger(__name__).warning(
                f"æå–æ–‡ç«  {page.id} å›¾ç‰‡URLæ—¶å‡ºé”™: {e}"
            )
        
        return image_url, cover_image_url
    
    def _get_image_url(self, image) -> str:
        """
        ä»Wagtail Imageå¯¹è±¡è·å–URL
        ä½¿ç”¨URLConfigç¡®ä¿è¿”å›æµè§ˆå™¨å¯è®¿é—®çš„å®Œæ•´URL
        """
        if not image:
            return None
        
        try:
            # è·å–åŸå§‹æ–‡ä»¶è·¯å¾„ï¼ˆMinIOä¸­çš„è·¯å¾„ï¼‰
            if hasattr(image, 'file') and image.file:
                file_path = str(image.file.name)
                
                # ä½¿ç”¨URLConfigæ„å»ºåª’ä½“ä»£ç†URL
                from apps.core.url_config import URLConfig
                # for_internal=False ç¡®ä¿è¿”å›æµè§ˆå™¨å¯è®¿é—®çš„URL
                return URLConfig.build_media_proxy_url(file_path, for_internal=False)
        except Exception as e:
            import logging
            logging.getLogger(__name__).warning(
                f"è·å–å›¾ç‰‡URLæ—¶å‡ºé”™: {e}"
            )
        
        return None
    
    def _add_hotness_tags(self, doc: dict, page) -> dict:
        """
        æ·»åŠ çƒ­åº¦æ ‡è®°ï¼ŒåŠ¨æ€ç”Ÿæˆ hot/trending è™šæ‹Ÿé¢‘é“æ ‡ç­¾
        """
        try:
            from apps.core.services.hotness_calculator import get_hotness_score
            
            article_id = str(page.id)
            site = doc.get('site', 'localhost')
            
            # è·å–çƒ­åº¦è¯„åˆ†å’Œåˆ†ç±»
            hotness_score, category = get_hotness_score(article_id, site)
            
            # æ·»åŠ çƒ­åº¦ç›¸å…³å­—æ®µ
            doc.update({
                'hotness_score': hotness_score,
                'hotness_category': category,
            })
            
            # ğŸ¯ å…³é”®ï¼šæ ¹æ®åˆ†ç±»è®¾ç½® channel å­—æ®µ
            if category == 'hot':
                doc['channel'] = 'hot'
            elif category == 'trending':
                doc['channel'] = 'trending'
            # else: ä¿æŒåŸæœ‰çš„ primary_channel_slug
            
            # ä¿ç•™åŸå§‹é¢‘é“ä¿¡æ¯ï¼ˆç”¨äºå…¶ä»–æŸ¥è¯¢ï¼‰
            if 'channel' in doc and doc['channel'] in ['hot', 'trending']:
                doc['original_channel'] = doc.get('primary_channel_slug', 'recommend')
            
        except Exception as e:
            # å®¹é”™ï¼šå¦‚æœçƒ­åº¦è®¡ç®—å¤±è´¥ï¼Œä¸å½±å“åŸºç¡€ç´¢å¼•
            import logging
            logging.getLogger(__name__).warning(f"çƒ­åº¦æ ‡è®°å¤±è´¥ {article_id}: {e}")
            doc.update({
                'hotness_score': 0.0,
                'hotness_category': 'normal',
            })
        
        return doc

# å‘åå…¼å®¹
def article_to_doc(page) -> dict:
    """å‘åå…¼å®¹çš„å‡½æ•°"""
    indexer = ArticleIndexer()
    return indexer.to_doc(page)
