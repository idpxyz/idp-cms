"""
ä»æ—§MySQLæ•°æ®åº“å¯¼å…¥æ–‡ç« åˆ°Wagtail CMS

ä½¿ç”¨æ–¹æ³•ï¼š
    # æµ‹è¯•æ¨¡å¼ï¼šå¯¼å…¥å‰10æ¡
    python manage.py import_old_articles --test --limit=10

    # æ­£å¼å¯¼å…¥ï¼šåˆ†æ‰¹å¤„ç†
    python manage.py import_old_articles --batch-size=1000

    # ä»æŒ‡å®šä½ç½®ç»§ç»­
    python manage.py import_old_articles --start-from=5000
"""

from django.core.management.base import BaseCommand, CommandError
from django.utils import timezone
from django.utils.text import slugify
from django.db import transaction
from django.core.files.base import ContentFile
from apps.news.models import ArticlePage
from apps.core.models import Channel, Category
from apps.media.models import CustomImage
from wagtail.models import Page, Site
import json
import requests
import os
from datetime import datetime
from pathlib import Path
import re
from bs4 import BeautifulSoup
import time


class Command(BaseCommand):
    help = 'ä»æ—§MySQLæ•°æ®åº“å¯¼å…¥æ–‡ç« åˆ°Wagtail'

    def __init__(self):
        super().__init__()
        self.stats = {
            'total': 0,
            'success': 0,
            'skipped': 0,
            'failed': 0,
            'cover_images_downloaded': 0,
            'cover_images_failed': 0,
            'inline_images_downloaded': 0,
            'inline_images_failed': 0,
        }
        self.category_mapping = {}  # å°†åœ¨prepare_environmentä¸­åŠ è½½

    def add_arguments(self, parser):
        parser.add_argument(
            '--file',
            type=str,
            default='data/migration/exports/articles.json',
            help='å¯¼å…¥çš„JSONæ–‡ä»¶è·¯å¾„'
        )
        parser.add_argument(
            '--limit',
            type=int,
            help='é™åˆ¶å¯¼å…¥æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰'
        )
        parser.add_argument(
            '--batch-size',
            type=int,
            default=100,
            help='æ‰¹å¤„ç†å¤§å°'
        )
        parser.add_argument(
            '--start-from',
            type=int,
            default=0,
            help='ä»ç¬¬Næ¡å¼€å§‹å¯¼å…¥ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰'
        )
        parser.add_argument(
            '--test',
            action='store_true',
            help='æµ‹è¯•æ¨¡å¼ï¼ˆé»˜è®¤åªå¯¼å…¥10æ¡ï¼‰'
        )
        parser.add_argument(
            '--skip-images',
            action='store_true',
            help='è·³è¿‡æ‰€æœ‰å›¾ç‰‡ä¸‹è½½'
        )
        parser.add_argument(
            '--skip-inline-images',
            action='store_true',
            help='è·³è¿‡æ­£æ–‡å›¾ç‰‡ä¸‹è½½ï¼ˆä»…ä¸‹è½½å°é¢å›¾ï¼‰'
        )
        parser.add_argument(
            '--old-site-url',
            type=str,
            default='http://www.hubeitoday.com.cn',
            help='æ—§ç«™ç‚¹URLï¼ˆç”¨äºæ‹¼æ¥ç›¸å¯¹è·¯å¾„å›¾ç‰‡ï¼‰'
        )
        parser.add_argument(
            '--channel-slug',
            type=str,
            default='news',
            help='ç›®æ ‡é¢‘é“çš„slug'
        )

    def handle(self, *args, **options):
        self.options = options
        
        # æµ‹è¯•æ¨¡å¼
        if options['test'] and not options.get('limit'):
            options['limit'] = 10

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_path = Path(options['file'])
        if not file_path.exists():
            raise CommandError(f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')

        self.stdout.write(self.style.SUCCESS(f'å¼€å§‹å¯¼å…¥æ–‡ç« ä»: {file_path}'))
        
        # å‡†å¤‡å¯¼å…¥ç¯å¢ƒ
        self.prepare_environment()

        # è¯»å–æ•°æ®
        self.stdout.write('è¯»å–æ•°æ®æ–‡ä»¶...')
        with open(file_path, 'r', encoding='utf-8') as f:
            articles = json.load(f)

        self.stats['total'] = len(articles)
        
        # åº”ç”¨é™åˆ¶
        start = options['start_from']
        end = start + options['limit'] if options.get('limit') else len(articles)
        articles_to_import = articles[start:end]

        self.stdout.write(self.style.SUCCESS(
            f'å‡†å¤‡å¯¼å…¥ {len(articles_to_import)} ç¯‡æ–‡ç«  '
            f'(æ€»æ•°: {self.stats["total"]}, èŒƒå›´: {start}-{end})'
        ))

        # å¼€å§‹å¯¼å…¥
        start_time = time.time()
        
        for i, old_article in enumerate(articles_to_import, start=1):
            self.import_article(old_article, i, len(articles_to_import))

        # è¾“å‡ºç»Ÿè®¡
        elapsed = time.time() - start_time
        self.print_statistics(elapsed)

    def prepare_environment(self):
        """å‡†å¤‡å¯¼å…¥ç¯å¢ƒ"""
        # è·å–ç«™ç‚¹å’Œçˆ¶é¡µé¢
        try:
            self.site = Site.objects.get(is_default_site=True)
            self.parent_page = self.site.root_page
        except Site.DoesNotExist:
            raise CommandError('æ‰¾ä¸åˆ°é»˜è®¤ç«™ç‚¹')

        # åŠ è½½åˆ†ç±»æ˜ å°„è¡¨
        mapping_file = Path('data/migration/category_mapping_complete.json')
        if mapping_file.exists():
            with open(mapping_file, 'r', encoding='utf-8') as f:
                mapping_data = json.load(f)
                self.category_mapping = mapping_data.get('mapping', {})
            self.stdout.write(f'å·²åŠ è½½åˆ†ç±»æ˜ å°„è¡¨: {len(self.category_mapping)} ä¸ªåˆ†ç±»')
        else:
            self.stdout.write(self.style.WARNING(
                'è­¦å‘Š: æœªæ‰¾åˆ°åˆ†ç±»æ˜ å°„è¡¨ï¼Œå°†ä½¿ç”¨é»˜è®¤é¢‘é“'
            ))

        # ç¼“å­˜æ‰€æœ‰é¢‘é“
        self.channels_cache = {ch.id: ch for ch in Channel.objects.all()}
        self.stdout.write(f'å·²ç¼“å­˜ {len(self.channels_cache)} ä¸ªé¢‘é“')

        # è·å–é»˜è®¤é¢‘é“ï¼ˆç”¨äºæœªæ˜ å°„çš„åˆ†ç±»ï¼‰
        channel_slug = self.options.get('channel_slug', 'society')
        try:
            self.default_channel = Channel.objects.get(slug=channel_slug)
            self.stdout.write(f'é»˜è®¤é¢‘é“: {self.default_channel.name}')
        except Channel.DoesNotExist:
            self.stdout.write(self.style.WARNING(
                f'é¢‘é“ "{channel_slug}" ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨ç¬¬ä¸€ä¸ªé¢‘é“'
            ))
            self.default_channel = Channel.objects.first()
            if not self.default_channel:
                raise CommandError('æ•°æ®åº“ä¸­æ²¡æœ‰ä»»ä½•é¢‘é“ï¼Œè¯·å…ˆåˆ›å»ºé¢‘é“')

        # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
        self.image_dir = Path('data/migration/images')
        self.image_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºæ—¥å¿—ç›®å½•
        self.log_dir = Path('data/migration/logs')
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        self.error_log = self.log_dir / f'errors_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

    def get_channel_for_category(self, cate_id):
        """æ ¹æ®æ—§åˆ†ç±»IDè·å–å¯¹åº”çš„æ–°é¢‘é“"""
        if not cate_id:
            return self.default_channel
        
        # æŸ¥æ‰¾æ˜ å°„
        cate_id_str = str(cate_id)
        if cate_id_str in self.category_mapping:
            target_channel_id = self.category_mapping[cate_id_str].get('target_channel_id')
            if target_channel_id and target_channel_id in self.channels_cache:
                return self.channels_cache[target_channel_id]
        
        # æœªæ‰¾åˆ°æ˜ å°„ï¼Œä½¿ç”¨é»˜è®¤é¢‘é“
        return self.default_channel

    def import_article(self, old_article, index, total):
        """å¯¼å…¥å•ç¯‡æ–‡ç« """
        article_id = old_article.get('id', 'unknown')
        
        try:
            # ç”Ÿæˆslug
            slug = self.generate_slug(old_article)

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if ArticlePage.objects.filter(slug=slug).exists():
                self.stats['skipped'] += 1
                self.stdout.write(self.style.WARNING(
                    f'[{index}/{total}] è·³è¿‡å·²å­˜åœ¨: {slug}'
                ))
                return

            # ä½¿ç”¨äº‹åŠ¡
            with transaction.atomic():
                # æ ¹æ®åˆ†ç±»IDè·å–é¢‘é“
                cate_id = old_article.get('cate_id')
                target_channel = self.get_channel_for_category(cate_id)

                # ä¸‹è½½å›¾ç‰‡
                cover_image = None
                if not self.options['skip_images'] and old_article.get('img'):
                    cover_image = self.download_and_create_image(
                        old_article['img'],
                        old_article.get('title', 'Untitled')
                    )

                # å¤„ç†æ­£æ–‡å†…å®¹
                body_html = old_article.get('info', '') or old_article.get('content', '')
                body_richtext = self.convert_html_to_richtext(body_html)

                # åˆ›å»ºæ–‡ç« é¡µé¢
                article = ArticlePage(
                    title=self.clean_text(old_article.get('title', 'Untitled')),
                    slug=slug,
                    excerpt=self.clean_text(old_article.get('seo_desc', ''))[:500],
                    body=body_richtext,
                    cover=cover_image,
                    channel=target_channel,  # ä½¿ç”¨æ˜ å°„åçš„é¢‘é“
                    author_name=self.clean_text(old_article.get('author', ''))[:64],
                    has_video=bool(old_article.get('video')),
                    meta_keywords=self.clean_text(old_article.get('seo_keys', ''))[:255],
                    seo_title=self.clean_text(
                        old_article.get('seo_title') or old_article.get('title', '')
                    ),
                    search_description=self.clean_text(old_article.get('seo_desc', ''))[:300],
                    external_url=old_article.get('fromurl', ''),
                    first_published_at=self.parse_timestamp(old_article.get('add_time')),
                    last_published_at=self.parse_timestamp(old_article.get('last_time')),
                    live=(old_article.get('status') == 1 or old_article.get('status') == '1'),
                )

                # æ·»åŠ åˆ°é¡µé¢æ ‘
                self.parent_page.add_child(instance=article)

                # å¤„ç†æ ‡ç­¾
                if old_article.get('tags'):
                    self.add_tags(article, old_article['tags'])

                # ä¿å­˜
                article.save_revision().publish() if article.live else article.save_revision()

                self.stats['success'] += 1
                self.stdout.write(self.style.SUCCESS(
                    f'[{index}/{total}] âœ“ å¯¼å…¥æˆåŠŸ: {article.title[:50]}'
                ))

        except Exception as e:
            self.stats['failed'] += 1
            error_msg = f'å¯¼å…¥å¤±è´¥ [ID: {article_id}]: {str(e)}'
            self.stdout.write(self.style.ERROR(f'[{index}/{total}] âœ— {error_msg}'))
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            with open(self.error_log, 'a', encoding='utf-8') as f:
                f.write(f'{datetime.now().isoformat()}: {error_msg}\n')
                f.write(f'  æ•°æ®: {json.dumps(old_article, ensure_ascii=False)}\n\n')

    def generate_slug(self, article):
        """ç”Ÿæˆå”¯ä¸€çš„slug"""
        # ä¼˜å…ˆä½¿ç”¨URLä¸­çš„slug
        if article.get('url'):
            url = article['url'].strip('/')
            slug = url.split('/')[-1]
            slug = slug.split('.')[0]  # ç§»é™¤æ‰©å±•å
            # æ¸…ç†slug
            slug = re.sub(r'[^\w\s-]', '', slug).strip().lower()
            slug = re.sub(r'[-\s]+', '-', slug)
        else:
            # ä»æ ‡é¢˜ç”Ÿæˆ
            slug = slugify(article.get('title', 'article'))

        # ä½¿ç”¨IDä½œä¸ºåç¼€ç¡®ä¿å”¯ä¸€æ€§
        if article.get('id'):
            slug = f"{slug}-{article['id']}"

        # ç¡®ä¿å”¯ä¸€æ€§
        base_slug = slug
        counter = 1
        while ArticlePage.objects.filter(slug=slug).exists():
            slug = f'{base_slug}-{counter}'
            counter += 1

        return slug[:255]  # Django slugå­—æ®µé€šå¸¸é™åˆ¶255å­—ç¬¦

    def download_and_create_image(self, image_url, title, image_type='cover'):
        """ä¸‹è½½å›¾ç‰‡å¹¶åˆ›å»ºCustomImage
        
        Args:
            image_url: å›¾ç‰‡URLï¼ˆç›¸å¯¹æˆ–ç»å¯¹è·¯å¾„ï¼‰
            title: å›¾ç‰‡æ ‡é¢˜
            image_type: å›¾ç‰‡ç±»å‹ï¼ˆ'cover'å°é¢æˆ–'inline'æ­£æ–‡ï¼‰
        """
        if not image_url:
            return None

        # å¤„ç†ç›¸å¯¹è·¯å¾„
        if not image_url.startswith('http'):
            # è·³è¿‡å ä½å›¾
            if 'placeholder' in image_url.lower():
                return None
            # æ‹¼æ¥å®Œæ•´URL
            old_site_url = self.options.get('old_site_url', 'http://www.hubeitoday.com.cn')
            image_url = old_site_url.rstrip('/') + '/' + image_url.lstrip('/')

        try:
            # ä¸‹è½½å›¾ç‰‡
            response = requests.get(image_url, timeout=15)
            response.raise_for_status()

            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆæœ€å¤§10MBï¼‰
            content_length = len(response.content)
            if content_length > 10 * 1024 * 1024:
                raise Exception(f'å›¾ç‰‡è¿‡å¤§: {content_length / 1024 / 1024:.1f}MB')

            # è·å–æ–‡ä»¶å
            filename = os.path.basename(image_url.split('?')[0])
            if not filename or len(filename) > 100:
                ext = 'jpg'
                if 'image/png' in response.headers.get('Content-Type', ''):
                    ext = 'png'
                filename = f'{image_type}_{int(time.time())}_{hash(image_url) % 10000}.{ext}'

            # åˆ›å»ºCustomImage
            image = CustomImage(
                title=title[:100],
            )
            image.file.save(
                filename,
                ContentFile(response.content),
                save=True
            )

            # æ›´æ–°ç»Ÿè®¡
            if image_type == 'cover':
                self.stats['cover_images_downloaded'] += 1
            else:
                self.stats['inline_images_downloaded'] += 1
            
            return image

        except Exception as e:
            # æ›´æ–°å¤±è´¥ç»Ÿè®¡
            if image_type == 'cover':
                self.stats['cover_images_failed'] += 1
            else:
                self.stats['inline_images_failed'] += 1
                
            self.stdout.write(self.style.WARNING(
                f'  {image_type}å›¾ç‰‡ä¸‹è½½å¤±è´¥: {image_url[:50]}... - {str(e)}'
            ))
            return None

    def convert_html_to_richtext(self, html):
        """è½¬æ¢HTMLä¸ºWagtail RichTextæ ¼å¼å¹¶å¤„ç†å›¾ç‰‡"""
        if not html:
            return ''

        try:
            # ä½¿ç”¨BeautifulSoupæ¸…ç†HTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
            for tag in soup(['script', 'style']):
                tag.decompose()

            # å¤„ç†æ­£æ–‡å›¾ç‰‡ï¼ˆå¦‚æœæœªè·³è¿‡ï¼‰
            if not self.options.get('skip_images') and not self.options.get('skip_inline_images'):
                for img_tag in soup.find_all('img'):
                    old_src = img_tag.get('src')
                    if not old_src:
                        continue
                    
                    # ä¸‹è½½å›¾ç‰‡å¹¶æ›¿æ¢URL
                    new_image = self.download_and_create_image(
                        old_src,
                        'inline-image',
                        image_type='inline'
                    )
                    
                    if new_image:
                        # æ›¿æ¢ä¸ºæ–°çš„å›¾ç‰‡URL
                        img_tag['src'] = new_image.file.url
                        # ä¿ç•™æˆ–æ·»åŠ altå±æ€§
                        if not img_tag.get('alt'):
                            img_tag['alt'] = ''
                    else:
                        # ä¸‹è½½å¤±è´¥ï¼Œä¿ç•™åŸURL
                        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                        if not old_src.startswith('http'):
                            old_site_url = self.options.get('old_site_url', 'http://www.hubeitoday.com.cn')
                            img_tag['src'] = old_site_url.rstrip('/') + '/' + old_src.lstrip('/')

            # è·å–æ¸…ç†åçš„HTML
            cleaned_html = str(soup)
            
            return cleaned_html
        except Exception as e:
            self.stdout.write(self.style.WARNING(f'  HTMLè½¬æ¢è­¦å‘Š: {str(e)}'))
            return html

    def add_tags(self, article, tags_string):
        """æ·»åŠ æ ‡ç­¾"""
        try:
            # åˆ†å‰²æ ‡ç­¾å­—ç¬¦ä¸²
            tags = [t.strip() for t in re.split(r'[,ï¼Œ;ï¼›]', tags_string) if t.strip()]
            
            # æ·»åŠ æ ‡ç­¾
            for tag_name in tags[:10]:  # é™åˆ¶æœ€å¤š10ä¸ªæ ‡ç­¾
                article.tags.add(tag_name)
        except Exception as e:
            self.stdout.write(self.style.WARNING(f'  æ ‡ç­¾æ·»åŠ å¤±è´¥: {str(e)}'))

    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ''
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = str(text).strip()
        # ç§»é™¤nullå­—ç¬¦
        text = text.replace('\x00', '')
        
        return text

    def parse_timestamp(self, timestamp):
        """æ—¶é—´æˆ³è½¬datetime"""
        if not timestamp:
            return timezone.now()
        
        try:
            # å°è¯•è½¬æ¢ä¸ºæ•´æ•°æ—¶é—´æˆ³
            ts = int(timestamp)
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¯«ç§’æ—¶é—´æˆ³
            if ts > 10000000000:
                ts = ts / 1000
            return datetime.fromtimestamp(ts, tz=timezone.utc)
        except (ValueError, TypeError, OSError):
            return timezone.now()

    def print_statistics(self, elapsed):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        self.stdout.write('\n' + '=' * 80)
        self.stdout.write(self.style.SUCCESS('ğŸ“Š å¯¼å…¥å®Œæˆï¼'))
        self.stdout.write('=' * 80)
        
        # æ–‡ç« ç»Ÿè®¡
        self.stdout.write('\nğŸ“„ æ–‡ç« ç»Ÿè®¡:')
        total = self.stats["success"] + self.stats["failed"] + self.stats["skipped"]
        self.stdout.write(f'  æ€»è®¡:         {total}')
        self.stdout.write(self.style.SUCCESS(f'  âœ“ æˆåŠŸ:      {self.stats["success"]}'))
        self.stdout.write(self.style.WARNING(f'  âŠ˜ è·³è¿‡:      {self.stats["skipped"]}'))
        self.stdout.write(self.style.ERROR(f'  âœ— å¤±è´¥:      {self.stats["failed"]}'))
        
        # å›¾ç‰‡ç»Ÿè®¡
        self.stdout.write('\nğŸ“¸ å›¾ç‰‡ç»Ÿè®¡:')
        cover_total = self.stats["cover_images_downloaded"] + self.stats["cover_images_failed"]
        inline_total = self.stats["inline_images_downloaded"] + self.stats["inline_images_failed"]
        
        self.stdout.write(f'  å°é¢å›¾ç‰‡:')
        self.stdout.write(f'    âœ“ æˆåŠŸ:    {self.stats["cover_images_downloaded"]}')
        self.stdout.write(f'    âœ— å¤±è´¥:    {self.stats["cover_images_failed"]}')
        if cover_total > 0:
            success_rate = (self.stats["cover_images_downloaded"] / cover_total) * 100
            self.stdout.write(f'    æˆåŠŸç‡:    {success_rate:.1f}%')
        
        self.stdout.write(f'  æ­£æ–‡å›¾ç‰‡:')
        self.stdout.write(f'    âœ“ æˆåŠŸ:    {self.stats["inline_images_downloaded"]}')
        self.stdout.write(f'    âœ— å¤±è´¥:    {self.stats["inline_images_failed"]}')
        if inline_total > 0:
            success_rate = (self.stats["inline_images_downloaded"] / inline_total) * 100
            self.stdout.write(f'    æˆåŠŸç‡:    {success_rate:.1f}%')
        
        # æ—¶é—´ç»Ÿè®¡
        self.stdout.write('\nâ±ï¸  æ—¶é—´ç»Ÿè®¡:')
        self.stdout.write(f'  æ€»ç”¨æ—¶:       {elapsed:.2f} ç§’ ({elapsed/60:.1f} åˆ†é’Ÿ)')
        
        if self.stats['success'] > 0:
            avg_time = elapsed / self.stats['success']
            self.stdout.write(f'  å¹³å‡é€Ÿåº¦:     {avg_time:.2f} ç§’/ç¯‡')
        
        # é”™è¯¯æ—¥å¿—
        if self.stats['failed'] > 0:
            self.stdout.write('\n' + self.style.WARNING(f'âš ï¸  é”™è¯¯æ—¥å¿—: {self.error_log}'))
        
        self.stdout.write('\n' + '=' * 80)

