"""
æ–‡ç« æ¨èAPIç«¯ç‚¹

åŒ…å«åŸºäºæ–‡ç« çš„æ™ºèƒ½æ¨èåŠŸèƒ½
"""

from rest_framework.decorators import api_view
from rest_framework.response import Response
from rest_framework import status

from apps.news.models import ArticlePage
from apps.core.models import Channel
from ..utils import (
    validate_site_parameter,
    apply_field_filtering,
    apply_include_expansion,
    generate_etag
)
from ...utils.rate_limit import ARTICLES_RATE_LIMIT
from ...utils.cache_performance import monitor_cache_performance


@api_view(["GET"])
@ARTICLES_RATE_LIMIT
@monitor_cache_performance("article_recommendations")
def article_recommendations(request, article_slug):
    """
    åŸºäºæ–‡ç« çš„æ™ºèƒ½æ¨èæ¥å£
    
    ä¸ºå½“å‰æ–‡ç« æ¨èç›¸å…³æ–‡ç« ï¼Œä½¿ç”¨å¤šç§æ¨èç­–ç•¥ï¼š
    1. åŒé¢‘é“ç›¸å…³æ–‡ç«  (40%)
    2. åŒæ ‡ç­¾ç›¸å…³æ–‡ç«  (30%) 
    3. çƒ­é—¨æ–‡ç« æ¨è (20%)
    4. æœ€æ–°æ–‡ç« æ¨è (10%)
    
    æ”¯æŒå‚æ•°ï¼š
    - site: ç«™ç‚¹æ ‡è¯†ï¼ˆä¸»æœºåæˆ–site_idï¼‰
    - limit: è¿”å›æ•°é‡ï¼Œé»˜è®¤6ï¼Œæœ€å¤§20
    - exclude_id: æ’é™¤çš„æ–‡ç« IDï¼ˆé€šå¸¸æ˜¯å½“å‰æ–‡ç« ï¼‰
    - exclude_channel: æ’é™¤æŒ‡å®šé¢‘é“çš„æ–‡ç« ï¼ˆç”¨äºé¿å…ä¸ç›¸å…³æ–‡ç« é‡å¤ï¼‰
    - fields: å­—æ®µç™½åå•é€‰æ‹©
    - include: å…³è”å±•å¼€
    """
    try:
        # 1. éªŒè¯ç«™ç‚¹å‚æ•°
        site = validate_site_parameter(request)
        if not site:
            return Response(
                {"error": "Invalid or missing site parameter"}, 
                status=status.HTTP_400_BAD_REQUEST
            )
        
        # 2. è·å–æŸ¥è¯¢å‚æ•°
        fields = request.query_params.get("fields", "").split(",") if request.query_params.get("fields") else []
        includes = request.query_params.get("include", "").split(",") if request.query_params.get("include") else []
        limit = min(int(request.query_params.get("limit", 6)), 20)  # é™åˆ¶æœ€å¤§20æ¡
        exclude_id = request.query_params.get("exclude_id")
        exclude_channel = request.query_params.get("exclude_channel")  # ğŸ¯ æ–°å¢ï¼šæ’é™¤æŒ‡å®šé¢‘é“
        
        # 3. è·å–å½“å‰æ–‡ç« ä¿¡æ¯
        try:
            # ç®€åŒ–æŸ¥è¯¢ï¼Œå…ˆä¸é™åˆ¶ç«™ç‚¹å±‚æ¬¡ç»“æ„
            current_article = ArticlePage.objects.live().select_related('channel').prefetch_related('tags').get(slug=article_slug)
        except ArticlePage.DoesNotExist:
            return Response(
                {"error": "Article not found"}, 
                status=status.HTTP_404_NOT_FOUND
            )
        
        # 4. æ„å»ºæ¨èæŸ¥è¯¢ - æ™ºèƒ½å¤šç­–ç•¥æ¨è
        base_queryset = ArticlePage.objects.live().filter(
            path__startswith=site.root_page.path
        ).exclude(id=current_article.id)
        
        # æ’é™¤æŒ‡å®šID
        if exclude_id:
            try:
                base_queryset = base_queryset.exclude(id=int(exclude_id))
            except (ValueError, TypeError):
                pass
        
        # 5. å¤šç­–ç•¥æ¨èç®—æ³•
        recommendations = []
        
        # ç­–ç•¥1: åŒé¢‘é“æ¨è (40% = 2-3ç¯‡)
        # ğŸ¯ å¦‚æœæŒ‡å®šäº†exclude_channelï¼Œåˆ™è·³è¿‡åŒé¢‘é“æ¨è
        if current_article.channel and not (exclude_channel and current_article.channel.slug == exclude_channel):
            channel_limit = max(1, int(limit * 0.4))
            channel_articles = base_queryset.filter(
                channel=current_article.channel
            ).select_related('channel', 'cover').order_by(
                '-is_featured',  # ç²¾é€‰ä¼˜å…ˆ
                '-weight',       # é«˜æƒé‡ä¼˜å…ˆ
                '-first_published_at'  # æœ€æ–°ä¼˜å…ˆ
            )[:channel_limit]
            
            recommendations.extend(list(channel_articles))
        elif exclude_channel:
            # ğŸ¯ å¦‚æœæ’é™¤äº†å½“å‰é¢‘é“ï¼Œå¢åŠ è·¨é¢‘é“æ¨è
            # è·å–å…¶ä»–é¢‘é“çš„çƒ­é—¨æ–‡ç« æ¥å¡«è¡¥ç©ºç¼º
            cross_channel_limit = max(2, int(limit * 0.4))
            exclude_ids = [current_article.id]
            if exclude_id:
                try:
                    exclude_ids.append(int(exclude_id))
                except (ValueError, TypeError):
                    pass
            
            cross_channel_articles = base_queryset.exclude(
                id__in=exclude_ids
            ).exclude(
                channel__slug=exclude_channel  # æ’é™¤æŒ‡å®šé¢‘é“
            ).select_related('channel', 'cover').order_by(
                '-is_featured',
                '-weight',
                '-first_published_at'
            )[:cross_channel_limit]
            
            recommendations.extend(list(cross_channel_articles))
        
        # ç­–ç•¥2: åŒæ ‡ç­¾æ¨è (30% = 1-2ç¯‡)
        if hasattr(current_article, 'tags') and current_article.tags.exists():
            tag_limit = max(1, int(limit * 0.3))
            current_tags = list(current_article.tags.all())
            
            if current_tags:
                # æ’é™¤å·²æ¨èçš„æ–‡ç« 
                exclude_ids = [art.id for art in recommendations] + [current_article.id]
                if exclude_id:
                    try:
                        exclude_ids.append(int(exclude_id))
                    except (ValueError, TypeError):
                        pass
                
                tag_query = base_queryset.filter(
                    tags__in=current_tags
                ).exclude(
                    id__in=exclude_ids
                )
                
                # ğŸ¯ å¦‚æœæŒ‡å®šäº†exclude_channelï¼Œä¹Ÿæ’é™¤è¯¥é¢‘é“çš„æ–‡ç« 
                if exclude_channel:
                    tag_query = tag_query.exclude(channel__slug=exclude_channel)
                
                tag_articles = tag_query.select_related('channel', 'cover').distinct().order_by(
                    '-weight', '-first_published_at'
                )[:tag_limit]
                
                recommendations.extend(list(tag_articles))
        
        # ç­–ç•¥3: çƒ­é—¨æ–‡ç« è¡¥å…… (20% = 1ç¯‡)
        if len(recommendations) < limit:
            hot_limit = max(1, int(limit * 0.2))
            exclude_ids = [art.id for art in recommendations] + [current_article.id]
            if exclude_id:
                try:
                    exclude_ids.append(int(exclude_id))
                except (ValueError, TypeError):
                    pass
            
            hot_query = base_queryset.exclude(
                id__in=exclude_ids
            ).filter(
                is_featured=True  # çƒ­é—¨/ç²¾é€‰æ–‡ç« 
            )
            
            # ğŸ¯ å¦‚æœæŒ‡å®šäº†exclude_channelï¼Œä¹Ÿæ’é™¤è¯¥é¢‘é“çš„æ–‡ç« 
            if exclude_channel:
                hot_query = hot_query.exclude(channel__slug=exclude_channel)
            
            hot_articles = hot_query.select_related('channel', 'cover').order_by(
                '-weight', '-first_published_at'
            )[:hot_limit]
            
            recommendations.extend(list(hot_articles))
        
        # ç­–ç•¥4: æœ€æ–°æ–‡ç« è¡¥å…… (10% = 1ç¯‡ï¼Œå¡«æ»¡å‰©ä½™ä½ç½®)
        if len(recommendations) < limit:
            remaining = limit - len(recommendations)
            exclude_ids = [art.id for art in recommendations] + [current_article.id]
            if exclude_id:
                try:
                    exclude_ids.append(int(exclude_id))
                except (ValueError, TypeError):
                    pass
            
            recent_query = base_queryset.exclude(
                id__in=exclude_ids
            )
            
            # ğŸ¯ å¦‚æœæŒ‡å®šäº†exclude_channelï¼Œä¹Ÿæ’é™¤è¯¥é¢‘é“çš„æ–‡ç« 
            if exclude_channel:
                recent_query = recent_query.exclude(channel__slug=exclude_channel)
            
            recent_articles = recent_query.select_related('channel', 'cover').order_by(
                '-first_published_at'
            )[:remaining]
            
            recommendations.extend(list(recent_articles))
        
        # 6. é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
        recommendations = recommendations[:limit]
        
        # 7. åºåˆ—åŒ–æ¨èæ–‡ç« 
        serialized_articles = []
        for article in recommendations:
            article_data = {
                "id": article.id,
                "title": article.title,
                "slug": article.slug,
                "excerpt": getattr(article, 'introduction', ''),
                "publish_at": article.first_published_at.isoformat() if article.first_published_at else None,
                "channel_slug": getattr(article.channel, 'slug', '') if article.channel else '',
                "is_featured": getattr(article, 'is_featured', False),
                "weight": getattr(article, 'weight', 0),
                # æ¨èç†ç”±
                "recommendation_reason": _get_recommendation_reason(article, current_article)
            }
            
            # åº”ç”¨å­—æ®µè¿‡æ»¤
            if fields:
                article_data = apply_field_filtering(article_data, fields)
            
            # åº”ç”¨å…³è”å±•å¼€
            if includes:
                article_data = apply_include_expansion(article_data, includes, article, site)
            
            serialized_articles.append(article_data)
        
        # 8. æ„å»ºå“åº”
        response_data = {
            "recommendations": serialized_articles,
            "meta": {
                "article_slug": article_slug,
                "article_id": current_article.id,
                "total": len(serialized_articles),
                "limit": limit,
                "strategy": "multi_strategy_v1",
                "site": site.hostname,
                "site_id": site.id
            }
        }
        
        # 9. ç”Ÿæˆç¼“å­˜ç›¸å…³å¤´éƒ¨
        response = Response(response_data)
        
        # Cache-Control: æ¨èç¼“å­˜è¾ƒçŸ­ï¼Œä¿è¯å†…å®¹æ–°é²œ
        response["Cache-Control"] = "public, s-maxage=300, stale-while-revalidate=150"
        
        # ETag
        etag = generate_etag(response_data)
        response["ETag"] = f'"{etag}"'
        
        # Surrogate-Key
        surrogate_keys = [
            f"site:{site.hostname}", 
            f"article:{article_slug}",
            "recommendations"
        ]
        if current_article.channel:
            surrogate_keys.append(f"channel:{current_article.channel.slug}")
        response["Surrogate-Key"] = " ".join(surrogate_keys)
        
        return response
        
    except Exception as e:
        return Response(
            {"error": f"Internal server error: {str(e)}"}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )


def _get_recommendation_reason(recommended_article, current_article):
    """
    è·å–æ¨èç†ç”±
    
    Args:
        recommended_article: è¢«æ¨èçš„æ–‡ç« 
        current_article: å½“å‰æ–‡ç« 
        
    Returns:
        str: æ¨èç†ç”±æ–‡æœ¬
    """
    reasons = []
    
    # åŒé¢‘é“
    if (recommended_article.channel and current_article.channel and 
        recommended_article.channel.id == current_article.channel.id):
        reasons.append(f"åŒé¢‘é“Â·{recommended_article.channel.name}")
    
    # ç²¾é€‰æ–‡ç« 
    if getattr(recommended_article, 'is_featured', False):
        reasons.append("ç¼–è¾‘ç²¾é€‰")
    
    # é«˜æƒé‡
    if getattr(recommended_article, 'weight', 0) > 50:
        reasons.append("çƒ­é—¨æ–‡ç« ")
    
    # åŒæ ‡ç­¾ (è¿™é‡Œå¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘æ¥æ£€æŸ¥æ ‡ç­¾é‡å )
    if (hasattr(current_article, 'tags') and hasattr(recommended_article, 'tags') and
        current_article.tags.exists() and recommended_article.tags.exists()):
        # ç®€åŒ–æ£€æŸ¥ï¼šæœ‰æ ‡ç­¾é‡å 
        current_tag_ids = set(current_article.tags.values_list('id', flat=True))
        recommended_tag_ids = set(recommended_article.tags.values_list('id', flat=True))
        if current_tag_ids & recommended_tag_ids:  # æœ‰äº¤é›†
            reasons.append("ç›¸å…³è¯é¢˜")
    
    # å¦‚æœæ²¡æœ‰ç‰¹æ®ŠåŸå› ï¼ŒåŸºäºæ—¶é—´
    if not reasons:
        from django.utils import timezone
        import datetime
        if (recommended_article.first_published_at and 
            recommended_article.first_published_at > timezone.now() - datetime.timedelta(days=1)):
            reasons.append("æœ€æ–°å‘å¸ƒ")
        else:
            reasons.append("ç›¸å…³å†…å®¹")
    
    return " â€¢ ".join(reasons) if reasons else "æ¨èå†…å®¹"
