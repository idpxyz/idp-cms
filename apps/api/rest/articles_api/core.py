"""
æ ¸å¿ƒæ–‡ç« APIç«¯ç‚¹

åŒ…å«æ–‡ç« åˆ—è¡¨å’Œæ–‡ç« è¯¦æƒ…çš„æ ¸å¿ƒåŠŸèƒ½
"""

from rest_framework.decorators import api_view
from rest_framework.response import Response
from rest_framework import status
from wagtail.rich_text import expand_db_html

from apps.news.models import ArticlePage
from apps.core.site_utils import get_site_from_request
from ..utils import (
    validate_site_parameter,
    apply_field_filtering,
    apply_include_expansion,
    apply_filtering,
    apply_ordering,
    generate_cache_key,
    generate_etag,
    generate_surrogate_keys
)
from ...utils.rate_limit import ARTICLES_RATE_LIMIT
from ...utils.cache_performance import monitor_cache_performance


@api_view(["GET"])
@ARTICLES_RATE_LIMIT
@monitor_cache_performance("articles_list")
def articles_list(request):
    """
    è·å–æ–‡ç« åˆ—è¡¨
    
    æ”¯æŒå‚æ•°ï¼š
    - site: ç«™ç‚¹æ ‡è¯†ï¼ˆä¸»æœºåæˆ–site_idï¼‰
    - fields: å­—æ®µç™½åå•é€‰æ‹©
    - include: å…³è”å±•å¼€
    - channel: é¢‘é“è¿‡æ»¤
    - region: åœ°åŒºè¿‡æ»¤
    - categories: åˆ†ç±»è¿‡æ»¤ï¼ˆå¤šé€‰ï¼Œé€—å·åˆ†éš”ï¼‰
    - topics: ä¸“é¢˜è¿‡æ»¤ï¼ˆå¤šé€‰ï¼Œé€—å·åˆ†éš”ï¼‰
    - q: æœç´¢å…³é”®è¯
    - is_featured: æ˜¯å¦ç½®é¡¶
    - since: æ—¶é—´è¿‡æ»¤
    - order: æ’åº
    - page: åˆ†é¡µ
    - size: æ¯é¡µå¤§å°
    """
    try:
        # 1. éªŒè¯ç«™ç‚¹å‚æ•°
        site = validate_site_parameter(request)
        if not site:
            return Response(
                {"error": "Invalid or missing site parameter"}, 
                status=status.HTTP_400_BAD_REQUEST
            )
        
        # 2. è·å–æŸ¥è¯¢å‚æ•°
        fields = request.query_params.get("fields", "").split(",") if request.query_params.get("fields") else []
        includes = request.query_params.get("include", "").split(",") if request.query_params.get("include") else []
        page = int(request.query_params.get("page", 1))
        size = min(int(request.query_params.get("size", 20)), 100)  # é™åˆ¶æœ€å¤§100æ¡
        
        # 3. æ„å»ºåŸºç¡€æŸ¥è¯¢ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
        queryset = ArticlePage.objects.live().filter(path__startswith=site.root_page.path)
        
        # 4. åº”ç”¨è¿‡æ»¤
        queryset = apply_filtering(queryset, request.query_params)
        
        # 5. åº”ç”¨æ’åº
        queryset = apply_ordering(queryset, request.query_params.get("order", "-publish_at"))
        
        # 6. æ€§èƒ½ä¼˜åŒ–ï¼šé¢„å–å…³è”æ•°æ®ï¼Œé¿å…N+1æŸ¥è¯¢
        queryset = queryset.select_related('channel', 'region').prefetch_related('tags', 'categories', 'topics')
        
        # 7. åˆ†é¡µ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé¿å…é‡å¤countæŸ¥è¯¢
        total_count = queryset.count()
        start = (page - 1) * size
        end = start + size
        articles = queryset[start:end]
        
        # 8. åºåˆ—åŒ–æ•°æ® - æ‰¹é‡å¤„ç†ï¼Œé¿å…é‡å¤æ•°æ®åº“æŸ¥è¯¢
        serialized_articles = []
        for article in articles:
            article_data = {
                "id": article.id,
                "title": article.title,
                "slug": article.slug,
                "excerpt": getattr(article, 'introduction', ''),
                "publish_at": article.first_published_at.isoformat() if article.first_published_at else None,
                "updated_at": article.last_published_at.isoformat() if article.last_published_at else None,
                "channel_slug": getattr(article.channel, 'slug', '') if article.channel else '',
                "region": getattr(article.region, 'name', '') if article.region else '',
                "topics": [{"slug": topic.slug, "title": topic.title} for topic in article.topics.all()] if hasattr(article, 'topics') else [],
                "category_names": article.get_category_names() if hasattr(article, 'get_category_names') else [],
                "is_featured": getattr(article, 'is_featured', False),
                "weight": getattr(article, 'weight', 0),
                "allow_aggregate": getattr(article, 'allow_aggregate', True),
                "canonical_url": getattr(article, 'canonical_url', ''),
                "source_site": site.id if hasattr(article, 'source_site') and article.source_site else site.id
            }
            
            # åº”ç”¨å­—æ®µè¿‡æ»¤
            if fields:
                article_data = apply_field_filtering(article_data, fields)
            
            # åº”ç”¨å…³è”å±•å¼€
            if includes:
                article_data = apply_include_expansion(article_data, includes, article, site)
            
            serialized_articles.append(article_data)
        
        # 9. æ„å»ºå“åº” - ä½¿ç”¨å·²è®¡ç®—çš„total_count
        response_data = {
            "items": serialized_articles,
            "pagination": {
                "page": page,
                "size": size,
                "total": total_count,
                "has_next": end < total_count,
                "has_prev": page > 1
            },
            "meta": {
                "site": site.hostname,
                "site_id": site.id
            }
        }
        
        # 10. æ£€æŸ¥æ¡ä»¶è¯·æ±‚
        from ..utils import should_return_304, get_last_modified, generate_etag_with_cache
        
        # è·å–æœ€åä¿®æ”¹æ—¶é—´
        last_modified = get_last_modified(articles)
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"articles_list:{site.id}:{page}:{size}"
        
        # ç”ŸæˆETagï¼ˆä¼˜å…ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        etag = generate_etag_with_cache(cache_key, response_data, last_modified, 120)
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿”å›304
        if should_return_304(request, etag):
            response = Response(status=status.HTTP_304_NOT_MODIFIED)
            response["ETag"] = f'"{etag}"'
            return response
        
        # 11. ç”Ÿæˆç¼“å­˜ç›¸å…³å¤´éƒ¨
        response = Response(response_data)
        
        # Cache-Control
        response["Cache-Control"] = "public, s-maxage=120, stale-while-revalidate=60"
        
        # ETag
        response["ETag"] = f'"{etag}"'
        
        # Last-Modified
        if last_modified:
            response["Last-Modified"] = last_modified.strftime('%a, %d %b %Y %H:%M:%S GMT')
        
        # Surrogate-Key
        surrogate_keys = generate_surrogate_keys(site, articles)
        response["Surrogate-Key"] = " ".join(surrogate_keys)
        
        return response
        
    except Exception as e:
        return Response(
            {"error": f"Internal server error: {str(e)}"}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )


@api_view(["GET"])
@ARTICLES_RATE_LIMIT
@monitor_cache_performance("article_detail")
def article_detail(request, slug):
    """
    è·å–æ–‡ç« è¯¦æƒ…
    
    æ”¯æŒå‚æ•°ï¼š
    - site: ç«™ç‚¹æ ‡è¯†ï¼ˆä¸»æœºåæˆ–site_idï¼‰
    - fields: å­—æ®µç™½åå•é€‰æ‹©
    - include: å…³è”å±•å¼€
    """
    # ğŸš€ æ€§èƒ½ç›‘æ§ï¼šè®°å½•å¼€å§‹æ—¶é—´
    import time
    start_time = time.time()
    
    try:
        # 1. éªŒè¯ç«™ç‚¹å‚æ•°
        site = validate_site_parameter(request)
        if not site:
            return Response(
                {"error": "Invalid or missing site parameter"}, 
                status=status.HTTP_400_BAD_REQUEST
            )
        
        # 2. è·å–æŸ¥è¯¢å‚æ•°
        fields = request.query_params.get("fields", "").split(",") if request.query_params.get("fields") else []
        includes = request.query_params.get("include", "").split(",") if request.query_params.get("include") else []
        
        # 3. æŸ¥è¯¢æ–‡ç«  - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆåŒæ—¶å…¼å®¹ slug æˆ– æ•°å­—IDï¼‰
        db_query_start = time.time()
        
        queryset = ArticlePage.objects.live().descendant_of(
            site.root_page
        ).select_related(
            'channel', 'region'
        ).prefetch_related('tags')
        
        article = None
        try:
            # ä¼˜å…ˆæŒ‰ slug ç²¾ç¡®åŒ¹é…
            article = queryset.get(slug=slug)
            db_query_time = (time.time() - db_query_start) * 1000
            print(f"ğŸ” DB query time for slug '{slug}': {db_query_time:.2f}ms")
        except ArticlePage.DoesNotExist:
            # è‹¥ slug çœ‹èµ·æ¥æ˜¯æ•°å­—ï¼Œåˆ™æŒ‰IDå›é€€
            if str(slug).isdigit():
                try:
                    article = queryset.get(id=int(slug))
                except ArticlePage.DoesNotExist:
                    article = None
            else:
                article = None
        
        if not article:
            return Response(
                {"error": "Article not found"}, 
                status=status.HTTP_404_NOT_FOUND
            )
        
        # 4. åºåˆ—åŒ–æ•°æ® - ä½¿ç”¨é¢„å–çš„å…³è”æ•°æ®
        article_data = {
            "id": article.id,
            "title": article.title,
            "slug": article.slug,
            "excerpt": getattr(article, 'excerpt', ''),
            "body": expand_db_html(article.body).replace('http://authoring:8000/api/media/proxy', '/api/media-proxy') if hasattr(article, 'body') else '',
            "publish_at": article.first_published_at.isoformat() if article.first_published_at else None,
            "updated_at": article.last_published_at.isoformat() if article.last_published_at else None,
            "channel_slug": getattr(article.channel, 'slug', '') if article.channel else '',
            "region": getattr(article.region, 'name', '') if article.region else '',
            "is_featured": getattr(article, 'is_featured', False),
            "weight": getattr(article, 'weight', 0),
            "allow_aggregate": getattr(article, 'allow_aggregate', True),
            "canonical_url": getattr(article, 'canonical_url', ''),
            "external_article_url": getattr(article, 'external_article_url', ''),
            # SEO å…ƒæ•°æ®
            "seo": {
                "keywords": article.get_seo_keywords() if hasattr(article, 'get_seo_keywords') else '',
                "og_image_url": article.get_og_image_url() if hasattr(article, 'get_og_image_url') else None,
                "structured_data": article.get_structured_data() if hasattr(article, 'get_structured_data') else None,
            },
            "source_site": site.id if hasattr(article, 'source_site') and article.source_site else site.id,
            "author_name": getattr(article, 'author_name', ''),
            "has_video": getattr(article, 'has_video', False),
            "language": getattr(article.language, 'code', 'zh') if hasattr(article, 'language') and article.language else 'zh'
        }
        
        # åº”ç”¨å­—æ®µè¿‡æ»¤
        if fields:
            article_data = apply_field_filtering(article_data, fields)
        
        # åº”ç”¨å…³è”å±•å¼€
        if includes:
            article_data = apply_include_expansion(article_data, includes, article, site)
        
        # 5. æ„å»ºå“åº”
        response_data = {
            "article": article_data,
            "meta": {
                "site": site.hostname,
                "site_id": site.id
            }
        }
        
        # 6. æ£€æŸ¥æ¡ä»¶è¯·æ±‚
        from ..utils import should_return_304, get_last_modified, generate_etag_with_cache
        
        # è·å–æœ€åä¿®æ”¹æ—¶é—´
        last_modified = get_last_modified(article)
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"article_detail:{site.id}:{article.slug}"
        
        # ç”ŸæˆETagï¼ˆä¼˜å…ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        etag = generate_etag_with_cache(cache_key, response_data, last_modified, 120)
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿”å›304
        if should_return_304(request, etag):
            response = Response(status=status.HTTP_304_NOT_MODIFIED)
            response["ETag"] = f'"{etag}"'
            return response
        
        # 7. ç”Ÿæˆç¼“å­˜ç›¸å…³å¤´éƒ¨
        response = Response(response_data)
        
        # Cache-Control
        response["Cache-Control"] = "public, s-maxage=120, stale-while-revalidate=60"
        
        # ETag
        response["ETag"] = f'"{etag}"'
        
        # Last-Modified
        if last_modified:
            response["Last-Modified"] = f"{last_modified.strftime('%a, %d %b %Y %H:%M:%S GMT')}"
        
        # Surrogate-Key
        surrogate_keys = generate_surrogate_keys(site, [article])
        response["Surrogate-Key"] = " ".join(surrogate_keys)
        
        # ğŸš€ æ€§èƒ½ç›‘æ§ï¼šè®°å½•æ€»æ—¶é—´
        total_time = (time.time() - start_time) * 1000
        print(f"âš¡ Total article_detail time for '{slug}': {total_time:.2f}ms")
        
        return response
        
    except Exception as e:
        return Response(
            {"error": f"Internal server error: {str(e)}"}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )
