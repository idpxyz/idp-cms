"""
é—¨æˆ·èšåˆAPIç«¯ç‚¹

åŒ…å«é—¨æˆ·èšåˆæ–‡ç« çš„APIå®ç°
"""

from rest_framework.decorators import api_view
from rest_framework.response import Response
from rest_framework import status

from apps.core.site_utils import get_site_from_request
from apps.searchapp.client import get_client
from apps.searchapp.simple_index import get_index_name
from ..utils import apply_field_filtering, generate_etag
from ..utils.rate_limit import PORTAL_ARTICLES_RATE_LIMIT


@api_view(["GET"])
@PORTAL_ARTICLES_RATE_LIMIT
def portal_articles(request):
    """
    é—¨æˆ·èšåˆæ–‡ç« æ¥å£
    
    åªè¿”å›æ‘˜è¦ï¼Œä¸è¿”å›æ­£æ–‡ï¼Œç”¨äºé—¨æˆ·èšåˆå±•ç¤º
    æ”¯æŒå‚æ•°ï¼š
    - allow_aggregate: æ˜¯å¦å…è®¸èšåˆï¼ˆé»˜è®¤trueï¼‰
    - fields: å­—æ®µç™½åå•é€‰æ‹©
    - channel: é¢‘é“è¿‡æ»¤
    - region: åœ°åŒºè¿‡æ»¤
    - q: æœç´¢å…³é”®è¯
    - is_featured: æ˜¯å¦ç½®é¡¶
    - since: æ—¶é—´è¿‡æ»¤
    - order: æ’åº
    - page: åˆ†é¡µ
    - size: æ¯é¡µå¤§å°
    """
    try:
        # 1. å‚æ•°
        allow_aggregate = request.query_params.get("allow_aggregate", "true").lower() == "true"
        fields = request.query_params.get("fields", "").split(",") if request.query_params.get("fields") else []
        channel = request.query_params.get("channel")
        categories = request.query_params.get("categories")  # é€—å·åˆ†éš”
        page = max(1, int(request.query_params.get("page", 1)))
        size = min(int(request.query_params.get("size", 20)), 100)
        start_from = (page - 1) * size

        # 2. ç«™ç‚¹ä¸ç´¢å¼•
        site = get_site_from_request(request)
        client = get_client()
        index = get_index_name(site)  # ğŸ¯ ä½¿ç”¨ç®€åŒ–ç´¢å¼•

        # 3. OpenSearch æŸ¥è¯¢æ„å»º
        must = []
        if channel:
            must.append({"term": {"primary_channel_slug.keyword": channel}})
        if categories:
            cats = [c.strip() for c in categories.split(',') if c.strip()]
            if cats:
                must.append({"terms": {"categories": cats}})
        body = {
            "query": {"bool": {"must": must or [{"match_all": {}}]}},
            "sort": [{"first_published_at": {"order": "desc"}}, {"article_id": {"order": "desc"}}],
            "from": start_from,
            "size": size,
            "track_total_hits": True,
        }

        # 4. æ‰§è¡ŒæŸ¥è¯¢
        res = client.search(index=index, body=body)
        hits = res.get("hits", {})
        total = hits.get("total", {}).get("value", 0)

        # 5. åºåˆ—åŒ– - å¢å¼ºå›¾ç‰‡æ•°æ®è·å–
        items = []
        article_ids = []
        
        # æ”¶é›†æ–‡ç« IDç”¨äºæ‰¹é‡æŸ¥è¯¢å›¾ç‰‡
        for h in hits.get("hits", []):
            s = h.get("_source", {})
            article_id = s.get("article_id") or h.get("_id")
            article_ids.append(article_id)
        
        # æ‰¹é‡æŸ¥è¯¢æ–‡ç« çš„å°é¢å›¾ç‰‡å’Œç»Ÿè®¡æ•°æ®
        article_data = {}
        if article_ids:
            from apps.news.models.article import ArticlePage
            articles_query = ArticlePage.objects.filter(
                id__in=article_ids
            ).select_related('cover').values(
                'id', 'cover__file', 'cover__title',
                'view_count', 'comment_count', 'like_count', 'favorite_count', 'reading_time',
                'author_name', 'is_featured', 'weight'
            )
            
            # ä½¿ç”¨å­˜å‚¨åç«¯ç»Ÿä¸€ç”Ÿæˆåª’ä½“URLï¼Œé¿å…ç¡¬ç¼–ç  /media å‰ç¼€
            from apps.core.storages import PublicMediaStorage
            storage = PublicMediaStorage()

            for article in articles_query:
                article_id = str(article['id'])
                data = {
                    'view_count': article['view_count'] or 0,
                    'comment_count': article['comment_count'] or 0,
                    'like_count': article['like_count'] or 0,
                    'favorite_count': article['favorite_count'] or 0,
                    'reading_time': article['reading_time'] or 1,
                    'author_name': article['author_name'] or '',
                    'is_featured': article['is_featured'] or False,
                    'weight': article['weight'] or 0,
                    'cover_url': '',
                    'cover_title': ''
                }
                
                # å¤„ç†å°é¢å›¾ç‰‡
                if article['cover__file']:
                    data['cover_url'] = storage.url(article['cover__file'])
                    data['cover_title'] = article['cover__title'] or ''
                
                article_data[article_id] = data
        
        # æ„å»ºå“åº”é¡¹ç›®
        for h in hits.get("hits", []):
            s = h.get("_source", {})
            article_id = s.get("article_id") or h.get("_id")
            
            # è·å–æ–‡ç« æ•°æ®ï¼ˆå›¾ç‰‡å’Œç»Ÿè®¡ä¿¡æ¯ï¼‰
            data = article_data.get(str(article_id), {})
            cover_url = data.get('cover_url', '')
            
            item = {
                "id": article_id,
                "title": s.get("title"),
                "slug": s.get("slug"),
                "excerpt": s.get("summary") or "",
                "cover_url": cover_url,  # ä»æ•°æ®åº“è·å–å®é™…å›¾ç‰‡
                "image_url": cover_url,  # å…¼å®¹æ€§å­—æ®µ
                "publish_at": s.get("first_published_at") or s.get("publish_time"),
                "channel_slug": s.get("primary_channel_slug") or s.get("channel"),
                "region": s.get("region"),
                "source_site": site,
                "source_url": s.get("url") or "",
                "canonical_url": s.get("url") or "",
                "author": data.get('author_name') or s.get("author"),  # ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ä¸­çš„ä½œè€…
                "has_video": s.get("has_video", False),  # æ·»åŠ è§†é¢‘æ ‡è¯†
                "is_featured": data.get('is_featured', False),  # æ·»åŠ æ¨èæ ‡è¯†
                "weight": data.get('weight', 0),  # æ·»åŠ æƒé‡å­—æ®µ
                # ç»Ÿè®¡æ•°æ®
                "view_count": data.get('view_count', 0),
                "comment_count": data.get('comment_count', 0), 
                "like_count": data.get('like_count', 0),
                "favorite_count": data.get('favorite_count', 0),
                "reading_time": data.get('reading_time', 1),
            }
            if fields:
                item = apply_field_filtering(item, fields)
            items.append(item)

        # 6. å“åº”
        response_data = {
            "items": items,
            "pagination": {
                "page": page,
                "size": size,
                "total": total,
                "has_next": (page * size) < total,
                "has_prev": page > 1,
            },
            "meta": {"type": "portal_aggregation", "allow_aggregate": allow_aggregate, "site": site},
        }

        response = Response(response_data)
        response["Cache-Control"] = "public, s-maxage=120, stale-while-revalidate=60"
        etag = generate_etag(response_data)
        response["ETag"] = f'"{etag}"'
        response["Surrogate-Key"] = "portal:aggregation articles:all"
        return response

    except Exception as e:
        return Response({"error": f"Internal server error: {str(e)}"}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
