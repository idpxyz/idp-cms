"""
æ–‡ç« APIç«¯ç‚¹ - æ–°è®¾è®¡è§„èŒƒå®ç°

å®ç°è®¾è®¡æ–‡æ¡£ä¸­è¦æ±‚çš„REST APIè§„èŒƒï¼š
- æ”¯æŒå­—æ®µç™½åå•é€‰æ‹©
- æ”¯æŒå…³è”å±•å¼€
- æ”¯æŒå¤šç§è¿‡æ»¤å’Œæ’åº
- å®ç°ç¼“å­˜ç­–ç•¥
"""

from rest_framework.decorators import api_view
from rest_framework.response import Response
from rest_framework import status
from apps.news.models import ArticlePage, Topic
from apps.core.models import Channel, Region, SiteSettings, Category
from apps.core.site_utils import get_site_from_request
from apps.searchapp.client import get_client
from apps.searchapp.simple_index import get_index_name  # ğŸ¯ ä½¿ç”¨ç®€åŒ–ç´¢å¼•
from wagtail.rich_text import expand_db_html
from .utils import (
    validate_site_parameter,
    apply_field_filtering,
    apply_include_expansion,
    apply_filtering,
    apply_ordering,
    generate_cache_key,
    generate_etag,
    generate_surrogate_keys
)
from apps.api.serializers.taxonomy import ArticleWithTaxonomySerializer
from ..utils.rate_limit import (
    ARTICLES_RATE_LIMIT,
    CHANNELS_RATE_LIMIT,
    REGIONS_RATE_LIMIT,
    SITE_SETTINGS_RATE_LIMIT,
    PORTAL_ARTICLES_RATE_LIMIT
)
from ..utils.cache_performance import monitor_cache_performance


@api_view(["GET"])
@ARTICLES_RATE_LIMIT
@monitor_cache_performance("articles_list")
def articles_list(request):
    """
    è·å–æ–‡ç« åˆ—è¡¨
    
    æ”¯æŒå‚æ•°ï¼š
    - site: ç«™ç‚¹æ ‡è¯†ï¼ˆä¸»æœºåæˆ–site_idï¼‰
    - fields: å­—æ®µç™½åå•é€‰æ‹©
    - include: å…³è”å±•å¼€
    - channel: é¢‘é“è¿‡æ»¤
    - region: åœ°åŒºè¿‡æ»¤
    - categories: åˆ†ç±»è¿‡æ»¤ï¼ˆå¤šé€‰ï¼Œé€—å·åˆ†éš”ï¼‰
    - topics: ä¸“é¢˜è¿‡æ»¤ï¼ˆå¤šé€‰ï¼Œé€—å·åˆ†éš”ï¼‰
    - q: æœç´¢å…³é”®è¯
    - is_featured: æ˜¯å¦ç½®é¡¶æ¨è
    - is_hero: æ˜¯å¦é¦–é¡µè½®æ’­
    - since: æ—¶é—´è¿‡æ»¤
    - order: æ’åº
    - page: åˆ†é¡µ
    - size: æ¯é¡µå¤§å°
    """
    try:
        # 1. éªŒè¯ç«™ç‚¹å‚æ•°
        site = validate_site_parameter(request)
        if not site:
            return Response(
                {"error": "Invalid or missing site parameter"}, 
                status=status.HTTP_400_BAD_REQUEST
            )
        
        # 2. è·å–æŸ¥è¯¢å‚æ•°
        fields = request.query_params.get("fields", "").split(",") if request.query_params.get("fields") else []
        includes = request.query_params.get("include", "").split(",") if request.query_params.get("include") else []
        page = int(request.query_params.get("page", 1))
        size = min(int(request.query_params.get("size", 20)), 100)  # é™åˆ¶æœ€å¤§100æ¡
        
        # 3. æ„å»ºåŸºç¡€æŸ¥è¯¢ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
        queryset = ArticlePage.objects.live().filter(path__startswith=site.root_page.path)
        
        # 4. åº”ç”¨è¿‡æ»¤
        queryset = apply_filtering(queryset, request.query_params)
        
        # 5. åº”ç”¨æ’åº
        queryset = apply_ordering(queryset, request.query_params.get("order", "-publish_at"))
        
        # 6. æ€§èƒ½ä¼˜åŒ–ï¼šé¢„å–å…³è”æ•°æ®ï¼Œé¿å…N+1æŸ¥è¯¢
        queryset = queryset.select_related('channel', 'region', 'cover').prefetch_related('topics', 'tags', 'categories')
        
        # 7. åˆ†é¡µ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé¿å…é‡å¤countæŸ¥è¯¢
        total_count = queryset.count()
        start = (page - 1) * size
        end = start + size
        articles = queryset[start:end]
        
        # 8. åºåˆ—åŒ–æ•°æ® - æ‰¹é‡å¤„ç†ï¼Œé¿å…é‡å¤æ•°æ®åº“æŸ¥è¯¢
        serialized_articles = []
        for article in articles:
            article_data = {
                "id": article.id,
                "title": article.title,
                "slug": article.slug,
                "excerpt": getattr(article, 'introduction', ''),
                "publish_at": article.first_published_at.isoformat() if article.first_published_at else None,
                "updated_at": article.last_published_at.isoformat() if article.last_published_at else None,
                "channel_slug": getattr(article.channel, 'slug', '') if article.channel else '',
                "region": getattr(article.region, 'name', '') if article.region else '',
                "topic_slug": '',  # topicsæ˜¯å¤šå¯¹å¤šå­—æ®µï¼Œæš‚æ—¶ç•™ç©º
                "topic_title": '',  # topicsæ˜¯å¤šå¯¹å¤šå­—æ®µï¼Œæš‚æ—¶ç•™ç©º
                "category_names": article.get_category_names() if hasattr(article, 'get_category_names') else [],
                "is_featured": getattr(article, 'is_featured', False),
                "is_hero": getattr(article, 'is_hero', False),
                "weight": getattr(article, 'weight', 0),
                "allow_aggregate": getattr(article, 'allow_aggregate', True),
                "canonical_url": getattr(article, 'canonical_url', ''),
                "source_site": site.id if hasattr(article, 'source_site') and article.source_site else site.id
            }
            
            # åº”ç”¨å­—æ®µè¿‡æ»¤
            if fields:
                article_data = apply_field_filtering(article_data, fields)
            
            # åº”ç”¨å…³è”å±•å¼€
            if includes:
                article_data = apply_include_expansion(article_data, includes, article, site)
            
            serialized_articles.append(article_data)
        
        # 9. æ„å»ºå“åº” - ä½¿ç”¨å·²è®¡ç®—çš„total_count
        response_data = {
            "items": serialized_articles,
            "pagination": {
                "page": page,
                "size": size,
                "total": total_count,
                "has_next": end < total_count,
                "has_prev": page > 1
            },
            "meta": {
                "site": site.hostname,
                "site_id": site.id
            }
        }
        
        # 9. æ£€æŸ¥æ¡ä»¶è¯·æ±‚
        from .utils import should_return_304, get_last_modified, generate_etag_with_cache
        
        # è·å–æœ€åä¿®æ”¹æ—¶é—´
        last_modified = get_last_modified(articles)
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"articles_list:{site.id}:{page}:{size}"
        
        # ç”ŸæˆETagï¼ˆä¼˜å…ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        etag = generate_etag_with_cache(cache_key, response_data, last_modified, 120)
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿”å›304
        if should_return_304(request, etag):
            response = Response(status=status.HTTP_304_NOT_MODIFIED)
            response["ETag"] = f'"{etag}"'
            return response
        
        # 10. ç”Ÿæˆç¼“å­˜ç›¸å…³å¤´éƒ¨
        response = Response(response_data)
        
        # Cache-Control
        response["Cache-Control"] = "public, s-maxage=120, stale-while-revalidate=60"
        
        # ETag
        response["ETag"] = f'"{etag}"'
        
        # Last-Modified
        if last_modified:
            response["Last-Modified"] = last_modified.strftime('%a, %d %b %Y %H:%M:%S GMT')
        
        # Surrogate-Key
        surrogate_keys = generate_surrogate_keys(site, articles)
        response["Surrogate-Key"] = " ".join(surrogate_keys)
        
        return response
        
    except Exception as e:
        return Response(
            {"error": f"Internal server error: {str(e)}"}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )


@api_view(["GET"])
@ARTICLES_RATE_LIMIT
@monitor_cache_performance("article_detail")
def article_detail(request, slug):
    """
    è·å–æ–‡ç« è¯¦æƒ…
    
    æ”¯æŒå‚æ•°ï¼š
    - site: ç«™ç‚¹æ ‡è¯†ï¼ˆä¸»æœºåæˆ–site_idï¼‰
    - fields: å­—æ®µç™½åå•é€‰æ‹©
    - include: å…³è”å±•å¼€
    """
    try:
        # 1. éªŒè¯ç«™ç‚¹å‚æ•°
        site = validate_site_parameter(request)
        if not site:
            return Response(
                {"error": "Invalid or missing site parameter"}, 
                status=status.HTTP_400_BAD_REQUEST
            )
        
        # 2. è·å–æŸ¥è¯¢å‚æ•°
        fields = request.query_params.get("fields", "").split(",") if request.query_params.get("fields") else []
        includes = request.query_params.get("include", "").split(",") if request.query_params.get("include") else []
        
        # 3. æŸ¥è¯¢æ–‡ç«  - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆåŒæ—¶å…¼å®¹ slug æˆ– æ•°å­—IDï¼‰
        queryset = ArticlePage.objects.live().descendant_of(
            site.root_page
        ).select_related(
            'channel', 'region'
        ).prefetch_related('tags')
        
        article = None
        try:
            # ä¼˜å…ˆæŒ‰ slug ç²¾ç¡®åŒ¹é…
            article = queryset.get(slug=slug)
        except ArticlePage.DoesNotExist:
            # è‹¥ slug çœ‹èµ·æ¥æ˜¯æ•°å­—ï¼Œåˆ™æŒ‰IDå›é€€
            if str(slug).isdigit():
                try:
                    article = queryset.get(id=int(slug))
                except ArticlePage.DoesNotExist:
                    article = None
            else:
                article = None
        
        if not article:
            return Response(
                {"error": "Article not found"}, 
                status=status.HTTP_404_NOT_FOUND
            )
        
        # 4. åºåˆ—åŒ–æ•°æ® - ä½¿ç”¨é¢„å–çš„å…³è”æ•°æ®
        article_data = {
            "id": article.id,
            "title": article.title,
            "slug": article.slug,
            "excerpt": getattr(article, 'introduction', ''),
            "body": expand_db_html(article.body).replace('http://authoring:8000', 'http://192.168.8.195:8000') if hasattr(article, 'body') and article.body else '',
            "publish_at": article.first_published_at.isoformat() if article.first_published_at else None,
            "updated_at": article.last_published_at.isoformat() if article.last_published_at else None,
            "channel_slug": getattr(article.channel, 'slug', '') if article.channel else '',
            "region": getattr(article.region, 'name', '') if article.region else '',
            "is_featured": getattr(article, 'is_featured', False),
            "weight": getattr(article, 'weight', 0),
            "allow_aggregate": getattr(article, 'allow_aggregate', True),
            "canonical_url": getattr(article, 'canonical_url', ''),
            "source_site": site.id if hasattr(article, 'source_site') and article.source_site else site.id,
            "author_name": getattr(article, 'author_name', ''),
            "has_video": getattr(article, 'has_video', False),
            "language": getattr(article.language, 'code', 'zh') if hasattr(article, 'language') and article.language else 'zh'
        }
        
        # åº”ç”¨å­—æ®µè¿‡æ»¤
        if fields:
            article_data = apply_field_filtering(article_data, fields)
        
        # åº”ç”¨å…³è”å±•å¼€
        if includes:
            article_data = apply_include_expansion(article_data, includes, article, site)
        
        # 5. æ„å»ºå“åº”
        response_data = {
            "article": article_data,
            "meta": {
                "site": site.hostname,
                "site_id": site.id
            }
        }
        
        # 6. æ£€æŸ¥æ¡ä»¶è¯·æ±‚
        from .utils import should_return_304, get_last_modified, generate_etag_with_cache
        
        # è·å–æœ€åä¿®æ”¹æ—¶é—´
        last_modified = get_last_modified(article)
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"article_detail:{site.id}:{article.slug}"
        
        # ç”ŸæˆETagï¼ˆä¼˜å…ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        etag = generate_etag_with_cache(cache_key, response_data, last_modified, 120)
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿”å›304
        if should_return_304(request, etag):
            response = Response(status=status.HTTP_304_NOT_MODIFIED)
            response["ETag"] = f'"{etag}"'
            return response
        
        # 7. ç”Ÿæˆç¼“å­˜ç›¸å…³å¤´éƒ¨
        response = Response(response_data)
        
        # Cache-Control
        response["Cache-Control"] = "public, s-maxage=120, stale-while-revalidate=60"
        
        # ETag
        response["ETag"] = f'"{etag}"'
        
        # Last-Modified
        if last_modified:
            response["Last-Modified"] = f"{last_modified.strftime('%a, %d %b %Y %H:%M:%S GMT')}"
        
        # Surrogate-Key
        surrogate_keys = generate_surrogate_keys(site, [article])
        response["Surrogate-Key"] = " ".join(surrogate_keys)
        
        return response
        
    except Exception as e:
        return Response(
            {"error": f"Internal server error: {str(e)}"}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )


@api_view(["GET"])
@CHANNELS_RATE_LIMIT
@monitor_cache_performance("channels_list")
def channels_list(request):
    """
    è·å–é¢‘é“åˆ—è¡¨
    
    æ”¯æŒå‚æ•°ï¼š
    - site: ç«™ç‚¹æ ‡è¯†ï¼ˆä¸»æœºåæˆ–site_idï¼‰
    - fields: å­—æ®µç™½åå•é€‰æ‹©
    """
    try:
        # 1. éªŒè¯ç«™ç‚¹å‚æ•°
        site = validate_site_parameter(request)
        if not site:
            return Response(
                {"error": "Invalid or missing site parameter"}, 
                status=status.HTTP_400_BAD_REQUEST
            )
        
        # 2. è·å–æŸ¥è¯¢å‚æ•°
        fields = request.query_params.get("fields", "").split(",") if request.query_params.get("fields") else []
        
        # 3. æŸ¥è¯¢é¢‘é“ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
        channels = Channel.objects.filter(sites=site).select_related().order_by('order', 'name')
        
        # 4. åºåˆ—åŒ–æ•°æ® - æ‰¹é‡å¤„ç†
        serialized_channels = [
            {
                "id": channel.id,
                "slug": channel.slug,
                "name": channel.name,
                "order": getattr(channel, 'order', 0),
                # ğŸ†• é¦–é¡µæ˜¾ç¤ºé…ç½®å­—æ®µ
                "show_in_homepage": getattr(channel, 'show_in_homepage', True),
                "homepage_order": getattr(channel, 'homepage_order', 0),
                # ğŸ¨ æ¨¡æ¿ä¿¡æ¯
                "template": {
                    "id": channel.template.id if channel.template else None,
                    "name": channel.template.name if channel.template else None,
                    "slug": channel.template.slug if channel.template else None,
                    "file_name": channel.template.file_name if channel.template else None,
                } if channel.template else None,
            }
            for channel in channels
        ]
        
        # åº”ç”¨å­—æ®µè¿‡æ»¤
        if fields:
            serialized_channels = [
                apply_field_filtering(channel_data, fields) 
                for channel_data in serialized_channels
            ]
        
        # 5. æ„å»ºå“åº”
        response_data = {
            "channels": serialized_channels,
            "meta": {
                "site": site.hostname,
                "site_id": site.id,
                "total": len(serialized_channels)
            }
        }
        
        # 6. ç”Ÿæˆç¼“å­˜ç›¸å…³å¤´éƒ¨
        response = Response(response_data)
        
        # Cache-Control
        response["Cache-Control"] = "public, s-maxage=300, stale-while-revalidate=60"
        
        # ETag
        etag = generate_etag(response_data)
        response["ETag"] = f'"{etag}"'
        
        # Surrogate-Key
        surrogate_keys = [f"site:{site.hostname}", "channels:all"]
        response["Surrogate-Key"] = " ".join(surrogate_keys)
        
        return response
        
    except Exception as e:
        return Response(
            {"error": f"Internal server error: {str(e)}"}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )


@api_view(["GET"])
def regions_list(request):
    """
    è·å–åœ°åŒºåˆ—è¡¨
    
    æ”¯æŒå‚æ•°ï¼š
    - site: ç«™ç‚¹æ ‡è¯†ï¼ˆä¸»æœºåæˆ–site_idï¼‰
    - fields: å­—æ®µç™½åå•é€‰æ‹©
    """
    try:
        # 1. éªŒè¯ç«™ç‚¹å‚æ•°
        site = validate_site_parameter(request)
        if not site:
            return Response(
                {"error": "Invalid or missing site parameter"}, 
                status=status.HTTP_400_BAD_REQUEST
            )
        
        # 2. è·å–æŸ¥è¯¢å‚æ•°
        fields = request.query_params.get("fields", "").split(",") if request.query_params.get("fields") else []
        
        # 3. æŸ¥è¯¢åœ°åŒº
        regions = Region.objects.filter(sites=site).order_by('order', 'name')
        
        # 4. åºåˆ—åŒ–æ•°æ®
        serialized_regions = []
        for region in regions:
            region_data = {
                "id": region.id,
                "slug": region.slug,
                "name": region.name,
                "order": getattr(region, 'order', 0)
            }
            
            # åº”ç”¨å­—æ®µè¿‡æ»¤
            if fields:
                region_data = apply_field_filtering(region_data, fields)
            
            serialized_regions.append(region_data)
        
        # 5. æ„å»ºå“åº”
        response_data = {
            "regions": serialized_regions,
            "meta": {
                "site": site.hostname,
                "site_id": site.id,
                "total": len(serialized_regions)
            }
        }
        
        # 6. ç”Ÿæˆç¼“å­˜ç›¸å…³å¤´éƒ¨
        response = Response(response_data)
        
        # Cache-Control
        response["Cache-Control"] = "public, s-maxage=300, stale-while-revalidate=60"
        
        # ETag
        etag = generate_etag(response_data)
        response["ETag"] = f'"{etag}"'
        
        # Surrogate-Key
        surrogate_keys = [f"site:{site.hostname}", "regions:all"]
        response["Surrogate-Key"] = " ".join(surrogate_keys)
        
        return response
        
    except Exception as e:
        return Response(
            {"error": f"Internal server error: {str(e)}"}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )


@api_view(["GET"])
def portal_articles(request):
    """
    é—¨æˆ·èšåˆæ–‡ç« æ¥å£
    
    åªè¿”å›æ‘˜è¦ï¼Œä¸è¿”å›æ­£æ–‡ï¼Œç”¨äºé—¨æˆ·èšåˆå±•ç¤º
    æ”¯æŒå‚æ•°ï¼š
    - allow_aggregate: æ˜¯å¦å…è®¸èšåˆï¼ˆé»˜è®¤trueï¼‰
    - fields: å­—æ®µç™½åå•é€‰æ‹©
    - channel: é¢‘é“è¿‡æ»¤
    - region: åœ°åŒºè¿‡æ»¤
    - q: æœç´¢å…³é”®è¯
    - is_featured: æ˜¯å¦ç½®é¡¶
    - since: æ—¶é—´è¿‡æ»¤
    - order: æ’åº
    - page: åˆ†é¡µ
    - size: æ¯é¡µå¤§å°
    """
    try:
        # 1. å‚æ•°
        allow_aggregate = request.query_params.get("allow_aggregate", "true").lower() == "true"
        fields = request.query_params.get("fields", "").split(",") if request.query_params.get("fields") else []
        channel = request.query_params.get("channel")
        categories = request.query_params.get("categories")  # é€—å·åˆ†éš”
        page = max(1, int(request.query_params.get("page", 1)))
        size = min(int(request.query_params.get("size", 20)), 100)
        start_from = (page - 1) * size

        # 2. ç«™ç‚¹ä¸ç´¢å¼•
        site = get_site_from_request(request)
        client = get_client()
        index = get_index_name(site)  # ğŸ¯ ä½¿ç”¨ç®€åŒ–ç´¢å¼•

        # 3. OpenSearch æŸ¥è¯¢æ„å»º
        must = []
        if channel:
            must.append({"term": {"primary_channel_slug.keyword": channel}})
        if categories:
            cats = [c.strip() for c in categories.split(',') if c.strip()]
            if cats:
                must.append({"terms": {"categories": cats}})
        body = {
            "query": {"bool": {"must": must or [{"match_all": {}}]}},
            "sort": [{"first_published_at": {"order": "desc"}}, {"article_id": {"order": "desc"}}],
            "from": start_from,
            "size": size,
            "track_total_hits": True,
        }

        # 4. æ‰§è¡ŒæŸ¥è¯¢
        res = client.search(index=index, body=body)
        hits = res.get("hits", {})
        total = hits.get("total", {}).get("value", 0)

        # 5. åºåˆ—åŒ– - å¢å¼ºå›¾ç‰‡æ•°æ®è·å–
        items = []
        article_ids = []
        
        # æ”¶é›†æ–‡ç« IDç”¨äºæ‰¹é‡æŸ¥è¯¢å›¾ç‰‡
        for h in hits.get("hits", []):
            s = h.get("_source", {})
            article_id = s.get("article_id") or h.get("_id")
            article_ids.append(article_id)
        
        # æ‰¹é‡æŸ¥è¯¢æ–‡ç« çš„å°é¢å›¾ç‰‡å’Œç»Ÿè®¡æ•°æ®
        article_data = {}
        if article_ids:
            from apps.news.models.article import ArticlePage
            articles_query = ArticlePage.objects.filter(
                id__in=article_ids
            ).select_related('cover').values(
                'id', 'cover__file', 'cover__title',
                'view_count', 'comment_count', 'like_count', 'favorite_count', 'reading_time',
                'author_name', 'is_featured', 'weight'
            )
            
            # ä½¿ç”¨å­˜å‚¨åç«¯ç»Ÿä¸€ç”Ÿæˆåª’ä½“URLï¼Œé¿å…ç¡¬ç¼–ç  /media å‰ç¼€
            from apps.core.storages import PublicMediaStorage
            storage = PublicMediaStorage()

            for article in articles_query:
                article_id = str(article['id'])
                data = {
                    'view_count': article['view_count'] or 0,
                    'comment_count': article['comment_count'] or 0,
                    'like_count': article['like_count'] or 0,
                    'favorite_count': article['favorite_count'] or 0,
                    'reading_time': article['reading_time'] or 1,
                    'author_name': article['author_name'] or '',
                    'is_featured': article['is_featured'] or False,
                    'weight': article['weight'] or 0,
                    'cover_url': '',
                    'cover_title': ''
                }
                
                # å¤„ç†å°é¢å›¾ç‰‡
                if article['cover__file']:
                    data['cover_url'] = storage.url(article['cover__file'])
                    data['cover_title'] = article['cover__title'] or ''
                
                article_data[article_id] = data
        
        # æ„å»ºå“åº”é¡¹ç›®
        for h in hits.get("hits", []):
            s = h.get("_source", {})
            article_id = s.get("article_id") or h.get("_id")
            
            # è·å–æ–‡ç« æ•°æ®ï¼ˆå›¾ç‰‡å’Œç»Ÿè®¡ä¿¡æ¯ï¼‰
            data = article_data.get(str(article_id), {})
            cover_url = data.get('cover_url', '')
            
            item = {
                "id": article_id,
                "title": s.get("title"),
                "slug": s.get("slug"),
                "excerpt": s.get("summary") or "",
                "cover_url": cover_url,  # ä»æ•°æ®åº“è·å–å®é™…å›¾ç‰‡
                "image_url": cover_url,  # å…¼å®¹æ€§å­—æ®µ
                "publish_at": s.get("first_published_at") or s.get("publish_time"),
                "channel_slug": s.get("primary_channel_slug") or s.get("channel"),
                "region": s.get("region"),
                "source_site": site,
                "source_url": s.get("url") or "",
                "canonical_url": s.get("url") or "",
                "author": data.get('author_name') or s.get("author"),  # ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ä¸­çš„ä½œè€…
                "has_video": s.get("has_video", False),  # æ·»åŠ è§†é¢‘æ ‡è¯†
                "is_featured": data.get('is_featured', False),  # æ·»åŠ æ¨èæ ‡è¯†
                "weight": data.get('weight', 0),  # æ·»åŠ æƒé‡å­—æ®µ
                # ç»Ÿè®¡æ•°æ®
                "view_count": data.get('view_count', 0),
                "comment_count": data.get('comment_count', 0), 
                "like_count": data.get('like_count', 0),
                "favorite_count": data.get('favorite_count', 0),
                "reading_time": data.get('reading_time', 1),
            }
            if fields:
                item = apply_field_filtering(item, fields)
            items.append(item)

        # 6. å“åº”
        response_data = {
            "items": items,
            "pagination": {
                "page": page,
                "size": size,
                "total": total,
                "has_next": (page * size) < total,
                "has_prev": page > 1,
            },
            "meta": {"type": "portal_aggregation", "allow_aggregate": allow_aggregate, "site": site},
        }

        response = Response(response_data)
        response["Cache-Control"] = "public, s-maxage=120, stale-while-revalidate=60"
        from .utils import generate_etag
        etag = generate_etag(response_data)
        response["ETag"] = f'"{etag}"'
        response["Surrogate-Key"] = "portal:aggregation articles:all"
        return response

    except Exception as e:
        return Response({"error": f"Internal server error: {str(e)}"}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(["GET"])
def hero_articles(request):
    """
    Hero è½®æ’­æ¨èæ–‡ç« æ¥å£
    
    ä¸“é—¨ä¸ºé¦–é¡µ Hero Carousel ä¼˜åŒ–çš„æ¥å£
    è¿”å›é«˜æƒé‡ã€ç²¾é€‰çš„æ–‡ç« ï¼Œå¿…é¡»æœ‰å›¾ç‰‡
    
    æ”¯æŒå‚æ•°ï¼š
    - site: ç«™ç‚¹è¿‡æ»¤ (å¿…éœ€)
    - limit: è¿”å›æ•°é‡ (é»˜è®¤5)
    """
    try:
        # è·å–ç«™ç‚¹æ ‡è¯†
        site_identifier = get_site_from_request(request)
        limit = min(int(request.GET.get("limit", 5)), 10)  # æœ€å¤š10æ¡
        
        # æ„å»ºæŸ¥è¯¢ï¼šé«˜æƒé‡ + ç²¾é€‰æ–‡ç« 
        from apps.news.models.article import ArticlePage
        
        # ç§‘å­¦çš„Heroé€‰æ‹©ç­–ç•¥ï¼šå¤šæ ·æ€§ + æƒé‡ + æ—¶æ•ˆæ€§
        from django.utils import timezone
        import datetime
        
        # 1. Heroæ ‡è®°çš„æ–‡ç« ï¼ˆå¿…é¡»æœ‰å°é¢ï¼‰
        high_weight_articles = ArticlePage.objects.filter(
            live=True,
            is_hero=True,
            cover__isnull=False,
            weight__gte=0
        ).select_related('cover', 'channel').order_by(
            '-weight', '-first_published_at'
        )[:limit]
        
        # ç®€åŒ–é€»è¾‘ï¼šå¦‚æœé«˜æƒé‡æ–‡ç« ä¸è¶³ï¼Œç›´æ¥è¡¥å……å…¶ä»–ç²¾é€‰æ–‡ç« 
        hero_articles = list(high_weight_articles)
        # ä¸å†å›é€€åˆ°æ— å°é¢æˆ–é»˜è®¤é™æ€å›¾ï¼Œä¿æŒä¸¥æ ¼è´¨é‡
        
        # æ„å»ºå“åº”æ•°æ®
        items = []
        from apps.core.signals_media import NEWS_IMAGE_RENDITIONS
        for article in hero_articles:
            # ä¸¥æ ¼è¦æ±‚å°é¢å­˜åœ¨
            if not article.cover:
                continue
            # ä½¿ç”¨Heroè§„æ ¼æ¸²æŸ“å›¾ï¼Œä¼˜å…ˆæ¡Œé¢ç«¯
            try:
                hero_spec = NEWS_IMAGE_RENDITIONS.get('hero_desktop', 'fill-1200x600|jpegquality-85')
                rendition = article.cover.get_rendition(hero_spec)
                image_url = rendition.url
            except Exception:
                try:
                    mobile_spec = NEWS_IMAGE_RENDITIONS.get('hero_mobile', 'fill-800x400|jpegquality-85')
                    rendition = article.cover.get_rendition(mobile_spec)
                    image_url = rendition.url
                except Exception:
                    # æ— æ³•ç”Ÿæˆæ¸²æŸ“å›¾åˆ™è·³è¿‡è¯¥æ–‡ç« 
                    continue
                
            item = {
                "id": str(article.id),
                "title": article.title,
                "slug": article.slug,
                "excerpt": getattr(article, 'excerpt', '') or "",
                "image_url": image_url,
                "cover_url": image_url,
                "publish_at": article.first_published_at.isoformat() if article.first_published_at else None,
                "publish_time": article.first_published_at.isoformat() if article.first_published_at else None,
                "author": getattr(article, 'author_name', '') or "",
                "source": site_identifier or "å®˜æ–¹",
                "channel": {
                    "id": getattr(article.channel, 'slug', '') or "news",
                    "name": getattr(article.channel, 'name', '') or "æ–°é—»",
                    "slug": getattr(article.channel, 'slug', '') or "news"
                } if article.channel else None,
                "tags": [],  # å¯ä»¥åç»­æ‰©å±•
                "is_featured": True,  # æ‰€æœ‰éƒ½æ˜¯ç²¾é€‰æ–‡ç« 
                "is_breaking": (getattr(article, 'weight', 0) or 0) >= 90,  # æƒé‡>=90è§†ä¸ºçªå‘æ–°é—»ï¼ˆçº¦top 10%ï¼‰
                "is_live": False,  # å¯ä»¥åç»­æ‰©å±•
                "is_event_mode": (getattr(article, 'weight', 0) or 0) >= 95,  # æƒé‡>=95è§†ä¸ºé‡å¤§äº‹ä»¶ï¼ˆçº¦top 5%ï¼‰
                "media_type": "image",
                "weight": getattr(article, 'weight', 0) or 0,
                # ç»Ÿè®¡æ•°æ®
                "view_count": getattr(article, 'view_count', 0) or 0,
                "comment_count": getattr(article, 'comment_count', 0) or 0,
                "like_count": getattr(article, 'like_count', 0) or 0,
                "favorite_count": getattr(article, 'favorite_count', 0) or 0,
                "reading_time": getattr(article, 'reading_time', 1) or 1,  # ç®€åŒ–ï¼šä¸è°ƒç”¨calculateæ–¹æ³•
            }
            items.append(item)
        
        return Response({
            "success": True,
            "items": items,
            "total": len(items),
            "debug": {
                "site": site_identifier,
                "strategy": "multi_tier_selection",
                "limit": limit,
                "actual_returned": len(items),
                "selection_breakdown": {
                    "high_weight_articles": len([item for item in items if item["weight"] >= 70]),
                    "recent_articles": len([item for item in items if item["weight"] < 70]),
                    "breaking_news": len([item for item in items if item["is_breaking"]]),
                    "major_events": len([item for item in items if item["is_event_mode"]]),
                },
                "channel_diversity": len(set(item["channel"]["slug"] if item["channel"] else "none" for item in items)),
                "weight_range": {
                    "min": min([item["weight"] for item in items]) if items else 0,
                    "max": max([item["weight"] for item in items]) if items else 0,
                    "avg": sum([item["weight"] for item in items]) / len(items) if items else 0
                }
            }
        }, status=status.HTTP_200_OK)
        
    except Exception as e:
        import logging
        logger = logging.getLogger(__name__)
        logger.error(f"Hero articles API error: {str(e)}", exc_info=True)
        return Response({
            "success": False,
            "error": "è·å–Heroæ–‡ç« å¤±è´¥",
            "message": str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(["GET"])
@SITE_SETTINGS_RATE_LIMIT
@monitor_cache_performance("site_settings")
def site_settings(request):
    """
    è·å–ç«™ç‚¹é…ç½®
    
    æ”¯æŒå‚æ•°ï¼š
    - site: ç«™ç‚¹æ ‡è¯†ï¼ˆä¸»æœºåæˆ–site_idï¼‰
    """
    try:
        # 1. éªŒè¯ç«™ç‚¹å‚æ•°
        site = validate_site_parameter(request)
        if not site:
            return Response(
                {"error": "Invalid or missing site parameter"}, 
                status=status.HTTP_400_BAD_REQUEST
            )
        
        # 2. è·å–ç«™ç‚¹é…ç½®
        try:
            settings = SiteSettings.get_for_site(site)
        except SiteSettings.DoesNotExist:
            # å¦‚æœç«™ç‚¹é…ç½®ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤é…ç½®
            settings = SiteSettings.objects.create(
                site=site,
                brand_name=site.site_name,
                theme_key="localsite-default",
                layout_key="layout-localsite-grid",
                brand_tokens={
                    "primary": "#3B82F6",
                    "secondary": "#6B7280",
                    "font": "Inter, sans-serif",
                    "radius": "0.5rem",
                    "shadow": "0 1px 3px rgba(0,0,0,0.1)"
                },
                modules={
                    "home": ["hero", "top-news", "channels"],
                    "sidebar": ["rank", "ad"]
                }
            )
        
        # 3. æ„å»ºå‰ç«¯è§„åˆ’éœ€è¦çš„æ•°æ®ç»“æ„
        site_settings_data = {
            "site_id": site.id,
            "site_name": site.site_name,
            "hostname": site.hostname,
            "port": site.port,
            "is_default_site": site.is_default_site,
            "root_page_id": site.root_page_id,
            
            # å‰ç«¯å¸ƒå±€é…ç½®ï¼ˆæ ¸å¿ƒï¼‰
            "theme_key": settings.theme_key,
            "layout_key": settings.layout_key,
            "brand_tokens": settings.brand_tokens or {
                "primary": settings.primary_color,
                "secondary": settings.secondary_color,
                "font": settings.font_family,
                "radius": "0.5rem",
                "shadow": "0 1px 3px rgba(0,0,0,0.1)"
            },
            "modules": settings.modules or {
                "home": ["hero", "top-news", "channels"],
                "sidebar": ["rank", "ad"]
            },
            
            # å“ç‰Œé…ç½®
            "brand": {
                "name": settings.brand_name or site.site_name,
                "logo_url": settings.brand_logo or settings.logo_url or "",
                "description": settings.brand_description or ""
            },
            
            # SEOé…ç½®  
            "seo": {
                # ç«™ç‚¹çº§SEO
                "site_title": settings.site_title or site.site_name,
                "site_description": settings.site_description or "",
                "site_keywords": getattr(settings, 'site_keywords', ''),
                
                # é¡µé¢çº§SEOæ¨¡æ¿
                "page_title_template": getattr(settings, 'page_title_template', '{title} - {site_name}'),
                "page_description_template": getattr(settings, 'page_description_template', ''),
                "auto_seo_enabled": getattr(settings, 'auto_seo_enabled', True)
            },
            
            # åˆ†æé…ç½®
            "analytics": {
                "google_analytics_id": settings.google_analytics_id or "",
                "track_user_behavior": getattr(settings, 'track_user_behavior', True)
            },
            
            # åŠŸèƒ½å¼€å…³
            "features": {
                "recommendation": settings.recommendation,
                "search_enabled": settings.search_enabled,
                "comments_enabled": settings.comments_enabled,
                "user_registration": settings.user_registration,
                "social_login": settings.social_login,
                "content_moderation": settings.content_moderation,
                "api_access": settings.api_access,
                "rss_feed": settings.rss_feed,
                "sitemap": settings.sitemap
            },
            
            # é¡µè„šé…ç½®
            "footer": {
                "links": [],  # TODO: å®ç°é¡µè„šé“¾æ¥
                "copyright": f"Â© 2024 {settings.brand_name or site.site_name}. All rights reserved."
            }
        }
        
        # 4. æ„å»ºå“åº”
        response_data = {
            "settings": site_settings_data,
            "meta": {
                "site": site.hostname,
                "site_id": site.id,
                "theme_key": settings.theme_key,
                "layout_key": settings.layout_key
            }
        }
        
        # 5. ç”Ÿæˆç¼“å­˜ç›¸å…³å¤´éƒ¨
        response = Response(response_data)
        
        # Cache-Control
        response["Cache-Control"] = "public, s-maxage=600, stale-while-revalidate=300"
        
        # ETag
        etag = generate_etag(response_data)
        response["ETag"] = f'"{etag}"'
        
        # Surrogate-Key - åŒ…å«å‰ç«¯å¸ƒå±€ç›¸å…³çš„æ ‡ç­¾
        surrogate_keys = [
            f"site:{site.hostname}", 
            "settings:all",
            f"theme:{settings.theme_key}",
            f"layout:{settings.layout_key}"
        ]
        response["Surrogate-Key"] = " ".join(surrogate_keys)
        
        return response
        
    except Exception as e:
        return Response(
            {"error": f"Internal server error: {str(e)}"}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )
