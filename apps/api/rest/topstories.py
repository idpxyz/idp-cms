"""
TopStories API - ä¸“ç”¨çš„å¤´æ¡æ–°é—»æ•°æ®ç«¯ç‚¹
åŒ…å«å¤æ‚çš„æ¨èç®—æ³•ã€èšç±»å»é‡ã€å¤šæ ·æ€§æ§åˆ¶ç­‰
"""

import math, re, time, hashlib, json, base64
from datetime import datetime, timezone as dt_timezone, timedelta
from difflib import SequenceMatcher
from rest_framework.decorators import api_view, throttle_classes
from rest_framework.response import Response
from django.core.cache import cache
from django.conf import settings
from apps.searchapp.client import get_client, index_name_for
from apps.searchapp.queries import build_query
from apps.core.site_utils import get_site_from_request
from apps.news.models.article import ArticlePage
from wagtail.models import Site
from ..utils.rate_limit import FEED_RATE_LIMIT
from apps.core.flags import flag
from ..utils.modern_cache import (
    ModernCacheStrategy, ModernCacheManager, SmartCacheKey, CacheHeaders,
    BreakingNewsDetector, ContentType, CacheLayer,
    get_cache_time, generate_cache_key, should_cache
)


def _normalize_text(text: str) -> str:
    """æ–‡æœ¬æ ‡å‡†åŒ–"""
    if not text:
        return ""
    t = text.lower()
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"[^\w\u4e00-\u9fff ]+", "", t)
    return t.strip()


def _slugify(text: str) -> str:
    """ç”Ÿæˆslug"""
    base = _normalize_text(text)
    base = re.sub(r"\s+", "-", base)
    h = hashlib.md5(base.encode("utf-8")).hexdigest()[:6]
    return (base[:48] + ("-" if base else "") + h) or h


def _compute_topstory_score(item: dict) -> float:
    """è®¡ç®—TopStoryè¯„åˆ† - å¢å¼ºç‰ˆï¼šä¼˜å…ˆæ˜¾ç¤ºæ–°å†…å®¹"""
    pop_1h = float(item.get("pop_1h", 0.0))
    pop_24h = float(item.get("pop_24h", 0.0))
    quality = float(item.get("quality_score", 1.0))
    weight = float(item.get("weight", 0.0))  # ç¼–è¾‘æƒé‡
    
    # æ–°é²œåº¦ï¼šç¼©çŸ­åŠè¡°æœŸï¼Œè®©æ–°æ–‡ç« ä¼˜åŠ¿æ›´æ˜æ˜¾ï¼ˆ6håŠè¡°æœŸï¼‰
    recency = 1.0
    hours_ago = 0
    try:
        pt = item.get("publish_at") or item.get("publish_time") or item.get("first_published_at")
        if isinstance(pt, str) and len(pt) >= 10:
            ts = pt.replace("Z", "+00:00")
            dt = datetime.fromisoformat(ts)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=dt_timezone.utc)
            now = datetime.now(dt_timezone.utc)
            hours_ago = (now - dt).total_seconds() / 3600
            recency = math.exp(-hours_ago * math.log(2) / 6)  # ğŸ¯ ç¼©çŸ­åˆ°6håŠè¡°æœŸ
    except Exception:
        recency = 0.5
        hours_ago = 24  # é»˜è®¤è®¤ä¸º24å°æ—¶å‰
    
    # çƒ­åº¦è¯„åˆ†ï¼šåŸºäºç”¨æˆ·è¡Œä¸ºæ•°æ®
    popularity_score = pop_1h * 0.4 + pop_24h * 0.3
    
    # ç¼–è¾‘æƒé‡è¯„åˆ†ï¼šå½’ä¸€åŒ–åˆ°0-1ï¼Œæƒé‡200+ä¸ºæ»¡åˆ†
    editorial_score = min(weight / 200.0, 1.0)
    
    # ğŸ¯ æ–°æ–‡ç« é¢å¤–åŠ æˆï¼š24å°æ—¶å†…çš„æ–‡ç« è·å¾—é¢å¤–æƒé‡
    freshness_bonus = 1.0
    if hours_ago <= 24:  # 24å°æ—¶å†…
        if hours_ago <= 6:   # 6å°æ—¶å†…ï¼Œæœ€é«˜åŠ æˆ
            freshness_bonus = 1.5
        elif hours_ago <= 12:  # 12å°æ—¶å†…ï¼Œä¸­ç­‰åŠ æˆ
            freshness_bonus = 1.3
        else:  # 24å°æ—¶å†…ï¼Œè½»å¾®åŠ æˆ
            freshness_bonus = 1.2
    
    # ç»¼åˆè¯„åˆ†ï¼šæé«˜æ–°é²œåº¦æƒé‡ï¼Œç¡®ä¿æ–°æ–‡ç« ä¼˜å…ˆæ˜¾ç¤º
    # åŸºç¡€åˆ† + æ–°é²œåº¦åŠ æˆï¼Œç„¶åä¹˜ä»¥è´¨é‡ç³»æ•°
    base_score = popularity_score + editorial_score * 0.5
    score = base_score * quality * recency * freshness_bonus
    
    return max(0.0, score)


def _enrich_with_images(items: list, site) -> list:
    """
    ä¸ºæ–‡ç« åˆ—è¡¨è¡¥å……å›¾ç‰‡URL
    ä¼˜å…ˆä½¿ç”¨å°é¢å›¾ç‰‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»æ­£æ–‡æå–ç¬¬ä¸€å¼ å›¾ç‰‡
    """
    if not items:
        return items
    
    # æ”¶é›†éœ€è¦è¡¥å……å›¾ç‰‡çš„æ–‡ç«  slug
    slugs_need_cover = []
    for item in items:
        if not item.get("image_url") and item.get("slug"):
            slugs_need_cover.append(item.get("slug"))
    
    # å¦‚æœæ‰€æœ‰æ–‡ç« éƒ½å·²æœ‰å›¾ç‰‡ï¼Œç›´æ¥è¿”å›
    if not slugs_need_cover:
        return items
    
    # æ‰¹é‡æŸ¥è¯¢æ•°æ®åº“è¡¥å……å›¾ç‰‡
    from apps.news.models import ArticlePage
    from wagtail.images import get_image_model
    import re
    
    slug_to_cover = {}
    try:
        Image = get_image_model()
        
        # æŸ¥è¯¢æ–‡ç« çš„å°é¢ ID å’Œæ­£æ–‡
        qs = ArticlePage.objects.filter(
            slug__in=slugs_need_cover
        ).values('slug', 'cover_id', 'body')
        
        # è·å–æ‰€æœ‰å°é¢å›¾ç‰‡ ID
        cover_ids = [row.get('cover_id') for row in qs if row.get('cover_id')]
        
        # æ‰¹é‡è·å–å›¾ç‰‡ URL
        id_to_url = {}
        if cover_ids:
            for img in Image.objects.filter(id__in=cover_ids).only("id", "file"):
                try:
                    # ä½¿ç”¨åª’ä½“ä»£ç†URL
                    from apps.core.url_config import URLConfig
                    file_path = str(img.file.name)
                    id_to_url[img.id] = URLConfig.build_media_proxy_url(file_path, for_internal=False)
                except Exception:
                    pass
        
        # æ„å»º slug -> image_url æ˜ å°„
        for row in qs:
            url = id_to_url.get(row.get("cover_id"), "")
            if not url:
                # ä»æ­£æ–‡æå–ç¬¬ä¸€å¼ å›¾ç‰‡
                body_html = str(row.get("body") or "")
                m = re.search(r'embedtype="image"[^>]*id="(\d+)"', body_html, re.I)
                if m:
                    # ä» Wagtail embed æ ‡ç­¾æå–å›¾ç‰‡ ID
                    try:
                        img_id = int(m.group(1))
                        img = Image.objects.filter(id=img_id).only("id", "file").first()
                        if img:
                            file_path = str(img.file.name)
                            url = URLConfig.build_media_proxy_url(file_path, for_internal=False)
                    except Exception:
                        pass
                
                if not url:
                    # å°è¯•æå–æ™®é€š img æ ‡ç­¾
                    m = re.search(r'<img[^>]*src=["\']([^"\']+)["\']', body_html, re.I)
                    if m:
                        url = m.group(1)
            
            if url:
                slug_to_cover[row["slug"]] = url
    except Exception as e:
        import logging
        logging.getLogger(__name__).warning(f"è¡¥å……å›¾ç‰‡URLå¤±è´¥: {e}")
    
    # å°†å›¾ç‰‡URLæ·»åŠ åˆ°æ–‡ç« é¡¹ä¸­
    for item in items:
        if not item.get("image_url"):
            slug = item.get("slug", "")
            cover_url = slug_to_cover.get(slug, "")
            item["image_url"] = cover_url if cover_url else "/images/default-covers/default.svg"
    
    return items


def _cluster_items(items: list, similarity: float = 0.92) -> list:
    """èšç±»å»é‡ç®—æ³•"""
    clusters = []  # æ¯é¡¹ï¼š{"rep": item, "items": [item,...], "key": str}
    for it in items:
        key = it.get("canonical_url") or it.get("url") or None
        if key:
            key = key.strip().lower()
        norm_title = _normalize_text(it.get("title", ""))

        placed = False
        # 1) URL çº§èšç±»
        if key:
            for cl in clusters:
                if cl.get("key") and cl["key"] == key:
                    cl["items"].append(it)
                    # æ›´æ–°ä»£è¡¨ä¸ºæ›´é«˜åˆ†è€…
                    if it.get("topstory_score", 0) > cl["rep"].get("topstory_score", 0):
                        cl["rep"] = it
                    placed = True
                    break
            if placed:
                continue

        # 2) æ ‡é¢˜ç›¸ä¼¼åº¦èšç±»
        for cl in clusters:
            rep_title = _normalize_text(cl["rep"].get("title", ""))
            try:
                sim = SequenceMatcher(None, norm_title, rep_title).ratio()
            except Exception:
                sim = 0.0
            if sim >= similarity:
                cl["items"].append(it)
                if it.get("topstory_score", 0) > cl["rep"].get("topstory_score", 0):
                    cl["rep"] = it
                placed = True
                break

        if not placed:
            clusters.append({"rep": it, "items": [it], "key": key})

    # è¿”å›å„ç°‡ä»£è¡¨ï¼Œå¹¶é™„ä¸Šæ¥æºæ•°é‡
    out = []
    for cl in clusters:
        rep = dict(cl["rep"])  # å¤åˆ¶ï¼Œé¿å…æ±¡æŸ“
        rep["more_sources"] = max(0, len(cl["items"]) - 1)
        rep["cluster_slug"] = _slugify(rep.get("title", "topic"))
        out.append(rep)
    return out


@api_view(["GET"])
@throttle_classes([])  # ä½¿ç”¨è‡ªå®šä¹‰é™æµ
@FEED_RATE_LIMIT
def topstories(request):
    """
    TopStories API - å¤æ‚çš„å¤´æ¡æ–°é—»æ¨è
    
    å‚æ•°:
    - size: è¿”å›æ•°é‡ï¼Œé»˜è®¤9ï¼Œæœ€å¤§30
    - hours: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤24
    - diversity: å¤šæ ·æ€§çº§åˆ« (high/med/low)ï¼Œé»˜è®¤high
    - exclude_cluster_ids: æ’é™¤çš„èšç±»IDåˆ—è¡¨
    - site: ç«™ç‚¹åŸŸå
    
    è¿”å›:
    - items: TopStoriesé¡¹ç›®åˆ—è¡¨
    - debug: è°ƒè¯•ä¿¡æ¯
    """
    site = get_site_from_request(request)
    size = max(1, min(int(request.query_params.get("size", 9)), 30))
    hours = int(request.query_params.get("hours", 24))
    diversity = request.query_params.get("diversity", "high")
    exclude_clusters = request.query_params.getlist("exclude_cluster_ids") or []
    
    # è·å–ç«™ç‚¹åç§°ï¼ˆå¤„ç†å­—ç¬¦ä¸²å’Œå¯¹è±¡ä¸¤ç§æƒ…å†µï¼‰
    site_name = site.hostname if hasattr(site, 'hostname') else str(site)
    
    # ä¼šè¯çº§å»é‡
    session_id = request.headers.get("X-Session-ID") or request.headers.get("X-AB-Session") or "anon"
    seen_cache_key = f"topstories:seen:{site_name}:{session_id}"
    cached_seen = cache.get(seen_cache_key, [])
    cached_seen = [str(x) for x in (cached_seen or [])]
    combined_seen = list(dict.fromkeys(cached_seen))
    
    # æ„å»ºç¼“å­˜key
    cache_params = f"{size}:{hours}:{diversity}:{len(exclude_clusters)}:{len(combined_seen)}"
    cache_key = f"topstories:{site_name}:{hashlib.md5(cache_params.encode()).hexdigest()[:8]}"
    
    # å°è¯•ä»ç¼“å­˜è·å–ï¼ˆå¼€å‘ç¯å¢ƒä¹Ÿå¯ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤OpenSearchæŸ¥è¯¢ï¼‰
    cached_data = cache.get(cache_key)
    if cached_data:  # âœ… å¼€å‘ç¯å¢ƒä¹Ÿä½¿ç”¨ç¼“å­˜ï¼Œæå‡ LCP æ€§èƒ½
        return Response({
            **cached_data,
            'cache_info': {
                'hit': True,
                'ttl': get_cache_time('hot', 'backend'),
                'type': 'topstories_complex',
                'debug_mode': settings.DEBUG
            }
        })
    
    # å¬å›å€™é€‰ï¼šä½¿ç”¨OpenSearch
    import time as _t
    t0 = _t.time()
    client = get_client()
    index = index_name_for(site_name)
    
    # æé«˜ESå€™é€‰é‡ï¼Œç¼“è§£å¤šæ ·æ€§ä¸å»é‡åçš„ç©ºé›†é£é™©
    elastic_size = max(size * 40, 400)
    
    # ğŸ¯ ä½¿ç”¨ä¸“é—¨çš„TopStoriesæŸ¥è¯¢æ¨¡æ¿
    query_template = "topstories_default"
    
    # TopStoriesæ¨¡å¼ï¼šä¸é™åˆ¶é¢‘é“ï¼Œè·å–å…¨ç«™æœ€ä½³å†…å®¹
    query_channels = []
    
    # TopStoriesåªè·å–éHeroå†…å®¹ (is_hero=false)
    non_hero_filter = {"term": {"is_hero": False}}
    
    body = build_query(
        query_template,
        site=site,
        channels=query_channels,
        hours=hours,
        seen_ids=combined_seen,
        size=elastic_size,
        extra_filters=[non_hero_filter]  # åªè¦éHeroå†…å®¹
    )
    
    candidates = []
    total_hits = 0
    returned_hits = 0
    
    try:
        resp = client.search(index=index, body=body, request_timeout=8)
        total_hits = resp.get("hits", {}).get("total", {}).get("value", 0)
        
        # é¦–å…ˆæ”¶é›†æ‰€æœ‰å€™é€‰é¡¹ï¼Œç„¶åæ™ºèƒ½å¤„ç†seenåˆ—è¡¨
        all_items = []
        for h in resp.get("hits", {}).get("hits", []):
            returned_hits += 1
            src = h.get("_source", {})
                
            item = {"id": h.get("_id"), **src}
            
            # å…œåº• publish_at
            if not item.get("publish_at") and item.get("publish_time"):
                item["publish_at"] = item["publish_time"]
            
            # è®¡ç®—TopStoryè¯„åˆ†
            item["topstory_score"] = _compute_topstory_score(item)
            
            # æ ‡è®°æ˜¯å¦å·²seen
            item["_is_seen"] = str(h.get("_id")) in combined_seen
            all_items.append(item)
        
        # ä¼˜å…ˆé€‰æ‹©æœªseençš„ï¼Œä½†å¦‚æœæœªseençš„ä¸å¤Ÿï¼Œåˆ™åŒ…å«ä¸€äº›seençš„
        unseen_items = [item for item in all_items if not item["_is_seen"]]
        seen_items = [item for item in all_items if item["_is_seen"]]
        
        # ç¡®ä¿è‡³å°‘æœ‰size/2ä¸ªå€™é€‰é¡¹ï¼ˆå³ä½¿åŒ…å«seençš„ï¼‰
        min_candidates = max(size // 2, 3)
        if len(unseen_items) >= min_candidates:
            candidates = unseen_items
        else:
            # éœ€è¦è¡¥å……ä¸€äº›seençš„é¡¹ç›®
            needed = min_candidates - len(unseen_items)
            candidates = unseen_items + seen_items[:needed]
        
        # ç§»é™¤ä¸´æ—¶æ ‡è®°
        for item in candidates:
            item.pop("_is_seen", None)
        
        t1 = _t.time()
        
        # æŒ‰è¯„åˆ†æ’åº
        candidates.sort(key=lambda x: x.get("topstory_score", 0), reverse=True)
        
        # èšç±»å»é‡
        similarity_threshold = {"high": 0.95, "med": 0.92, "low": 0.88}.get(diversity, 0.92)
        clustered = _cluster_items(candidates, similarity_threshold)
        
        # æ’é™¤æŒ‡å®šçš„èšç±»
        if exclude_clusters:
            clustered = [item for item in clustered if item.get("cluster_slug") not in exclude_clusters]
        
        # å¤šæ ·æ€§é‡‡æ ·
        if diversity == "high":
            # é«˜å¤šæ ·æ€§ï¼šç¡®ä¿é¢‘é“ã€ä¸»é¢˜åˆ†æ•£
            final_items = []
            used_channels = set()
            used_topics = set()
            
            # ğŸ¯ ä¼˜å…ˆå¤„ç†é«˜æƒé‡æ–‡ç« ï¼ˆæƒé‡>100çš„é‡è¦æ–‡ç« ï¼‰
            high_priority_items = [item for item in clustered if item.get("weight", 0) > 100]
            regular_items = [item for item in clustered if item.get("weight", 0) <= 100]
            
            # å…ˆæ·»åŠ é«˜æƒé‡æ–‡ç« ï¼Œä¸å—å¤šæ ·æ€§é™åˆ¶
            for item in high_priority_items[:3]:  # æœ€å¤š3ç¯‡é«˜æƒé‡æ–‡ç« 
                final_items.append(item)
                if len(final_items) >= size:
                    break
            
            # ç„¶åæŒ‰å¤šæ ·æ€§è§„åˆ™æ·»åŠ å¸¸è§„æ–‡ç« 
            for item in regular_items:
                if len(final_items) >= size:
                    break
                    
                # å¤„ç†channelå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸çš„æƒ…å†µ
                channel_obj = item.get("channel", {})
                if isinstance(channel_obj, dict):
                    channel = channel_obj.get("slug", "")
                else:
                    channel = str(channel_obj) if channel_obj else ""
                
                # å¤„ç†topicå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸çš„æƒ…å†µ  
                topic_obj = item.get("topic", {})
                if isinstance(topic_obj, dict):
                    topic = topic_obj.get("slug", "")
                else:
                    topic = str(topic_obj) if topic_obj else ""
                
                # é™åˆ¶åŒé¢‘é“ã€åŒä¸»é¢˜çš„æ•°é‡ï¼ˆä½†ä¸å½±å“å·²æ·»åŠ çš„é«˜æƒé‡æ–‡ç« ï¼‰
                def get_channel_slug(x):
                    ch = x.get("channel", {})
                    return ch.get("slug", "") if isinstance(ch, dict) else str(ch) if ch else ""

                def get_topic_slug(x):
                    tp = x.get("topic", {})
                    return tp.get("slug", "") if isinstance(tp, dict) else str(tp) if tp else ""

                # åªå¯¹å¸¸è§„æƒé‡æ–‡ç« ç»Ÿè®¡å¤šæ ·æ€§
                regular_final_items = [x for x in final_items if x.get("weight", 0) <= 100]
                channel_count = sum(1 for x in regular_final_items if get_channel_slug(x) == channel)
                topic_count = sum(1 for x in regular_final_items if get_topic_slug(x) == topic)
                
                if channel_count < 2 and topic_count < 2:
                    final_items.append(item)
                elif len(final_items) < size * 0.8:  # 80%å¡«æ»¡åæ”¾å®½é™åˆ¶
                    final_items.append(item)
                    
        else:
            # ä¸­ä½å¤šæ ·æ€§ï¼šä¸»è¦æŒ‰è¯„åˆ†æ’åº
            final_items = clustered[:size]
        
        t2 = _t.time()
        
        # ğŸ–¼ï¸ è¡¥å……å›¾ç‰‡URLï¼ˆç±»ä¼¼ Portal API çš„é€»è¾‘ï¼‰
        final_items = _enrich_with_images(final_items, site)
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            'items': final_items,
            'debug': {
                'site': site_name,
                'hours': hours,
                'diversity': diversity,
                'total_hits': total_hits,
                'returned_hits': returned_hits,
                'candidates': len(candidates),
                'clusters': len(clustered),
                'final_count': len(final_items),
                'exclude_clusters_count': len(exclude_clusters),
                'seen_count': len(combined_seen),
                'timing': {
                    'opensearch_ms': round((t1 - t0) * 1000, 2),
                    'processing_ms': round((t2 - t1) * 1000, 2),
                    'total_ms': round((t2 - t0) * 1000, 2)
                },
                'api_version': 'topstories_v1'
            }
        }
        
        # ç¼“å­˜ç»“æœï¼ˆä½¿ç”¨çƒ­ç‚¹æ–°é—»çš„ç¼“å­˜ç­–ç•¥ï¼‰
        cache_ttl = get_cache_time('hot', 'backend')
        cache.set(cache_key, response_data, cache_ttl)
        
        # æ›´æ–°å·²çœ‹è¿‡çš„å†…å®¹
        new_seen = [str(item.get("id")) for item in final_items]
        updated_seen = list(dict.fromkeys(combined_seen + new_seen))[-200:]  # ä¿ç•™æœ€è¿‘200ä¸ª
        cache.set(seen_cache_key, updated_seen, 3600)  # 1å°æ—¶
        
        # æ·»åŠ ç¼“å­˜ä¿¡æ¯
        response_data['cache_info'] = {
            'hit': False,
            'ttl': cache_ttl,
            'type': 'topstories_complex',
            'key': cache_key
        }
        
        return Response(response_data)
        
    except Exception as e:
        # é”™è¯¯å¤„ç†
        error_response = {
            'items': [],
            'error': {
                'message': 'Failed to fetch topstories',
                'type': 'opensearch_error',
                'debug': str(e) if settings.DEBUG else None
            },
            'debug': {
                'site': site_name,
                'hours': hours,
                'diversity': diversity,
                'api_version': 'topstories_v1'
            }
        }
        
        return Response(error_response, status=500)
