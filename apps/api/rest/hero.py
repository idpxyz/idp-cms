"""
Hero API - ä¸“ç”¨çš„Heroè½®æ’­æ•°æ®ç«¯ç‚¹
ç®€å•ã€å¿«é€Ÿçš„Heroå†…å®¹è·å–ï¼Œæ— éœ€å¤æ‚ç®—æ³•
"""

from datetime import datetime, timedelta
from rest_framework.decorators import api_view, throttle_classes
from rest_framework.response import Response
from django.core.cache import cache
from django.conf import settings
from apps.core.site_utils import get_site_from_request
from apps.news.models.article import ArticlePage
from wagtail.models import Site
from ..utils.rate_limit import FEED_RATE_LIMIT
from apps.core.flags import flag


@api_view(["GET"])
@throttle_classes([])  # ä½¿ç”¨è‡ªå®šä¹‰é™æµ
@FEED_RATE_LIMIT
def hero_items(request):
    """
    Heroè½®æ’­API - ç®€å•ç›´æ¥çš„Heroå†…å®¹è·å–
    
    å‚æ•°:
    - size: è¿”å›æ•°é‡ï¼Œé»˜è®¤5ï¼Œæœ€å¤§10
    - site: ç«™ç‚¹åŸŸå
    
    ğŸ¯ Heroå†…å®¹æ— æ—¶é—´é™åˆ¶ï¼šåªè¦æ ‡è®°ä¸ºis_hero=Trueçš„æ´»è·ƒæ–‡ç« éƒ½ä¼šæ˜¾ç¤º
    
    è¿”å›:
    - items: Heroé¡¹ç›®åˆ—è¡¨
    - total: æ€»æ•°é‡
    - cache_info: ç¼“å­˜ä¿¡æ¯
    """
    site = get_site_from_request(request)
    size = max(1, min(int(request.query_params.get("size", 5)), 10))
    # ğŸ¯ Heroä¸åº”è¯¥å—æ—¶é—´é™åˆ¶ - ç§»é™¤hourså‚æ•°
    # hours = int(request.query_params.get("hours", 168))  # å·²ç§»é™¤
    
    # è·å–ç«™ç‚¹åç§°ï¼ˆå¤„ç†å­—ç¬¦ä¸²å’Œå¯¹è±¡ä¸¤ç§æƒ…å†µï¼‰
    site_name = site.hostname if hasattr(site, 'hostname') else str(site)
    
    # æ„å»ºç¼“å­˜keyï¼ˆç§»é™¤hourså‚æ•°ï¼‰
    cache_key = f"hero_items:{site_name}:{size}"
    
    # å°è¯•ä»ç¼“å­˜è·å–ï¼ˆå¼€å‘ç¯å¢ƒä¹Ÿå¯ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤ç”Ÿæˆå›¾ç‰‡ï¼‰
    cached_data = cache.get(cache_key)
    if cached_data:  # âœ… å¼€å‘ç¯å¢ƒä¹Ÿä½¿ç”¨ç¼“å­˜ï¼Œæå‡ LCP æ€§èƒ½
        return Response({
            **cached_data,
            'cache_info': {
                'hit': True,
                'ttl': 300,
                'type': 'hero_simple',
                'key': cache_key,
                'debug_mode': settings.DEBUG
            }
        })
    
    # ğŸ¯ Heroä¸å—æ—¶é—´é™åˆ¶ï¼Œåªè¦æ˜¯æ ‡è®°ä¸ºHeroçš„æ´»è·ƒæ–‡ç« å°±æ˜¾ç¤º
    # cutoff_time = datetime.now() - timedelta(hours=hours)  # å·²ç§»é™¤
    
    try:
        # ğŸ¯ ç®€å•çš„æ•°æ®åº“æŸ¥è¯¢ï¼Œæ— éœ€OpenSearchï¼Œæ— æ—¶é—´é™åˆ¶
        hero_articles = ArticlePage.objects.filter(
            is_hero=True,
            live=True
            # first_published_at__gte=cutoff_time  # å·²ç§»é™¤æ—¶é—´é™åˆ¶
        ).select_related(
            'channel', 'cover'
        ).prefetch_related(
            'tags', 'topics'
        ).order_by('-first_published_at')[:size]
        
        items = []
        for article in hero_articles:
            # ç¡®ä¿æœ‰å°é¢å›¾ç‰‡
            image_url = None
            if article.cover:
                try:
                    # ğŸš€ LCP ä¼˜åŒ–ï¼šå¹³è¡¡æ€§èƒ½å’Œè´¨é‡
                    # å“åº”å¼å°ºå¯¸ï¼š900x450 WebP @ 82% quality (~120-180KBï¼Œæ€§èƒ½ä¸è´¨é‡çš„æœ€ä½³å¹³è¡¡ï¼‰
                    # è¶³å¤Ÿæ¸…æ™°ï¼ŒåŒæ—¶ä¿æŒå¿«é€ŸåŠ è½½
                    image_url = article.cover.get_rendition('fill-900x450|format-webp|webpquality-82').url
                except:
                    # å¦‚æœWebPæ¸²æŸ“å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ—§çš„è§„æ ¼ä½œä¸ºå¤‡ç”¨
                    try:
                        image_url = article.cover.get_rendition('width-800').url
                    except:
                        # æœ€åå¤‡ç”¨ï¼šä½¿ç”¨åŸå›¾
                        image_url = article.cover.file.url if article.cover.file else None
            
            # è·³è¿‡æ²¡æœ‰å°é¢å›¾çš„æ–‡ç« 
            if not image_url:
                continue
                
            # æ„å»ºHeroé¡¹ç›®æ•°æ®
            item = {
                'id': str(article.id),
                'article_id': str(article.id),
                'title': article.title,
                'excerpt': article.search_description or article.excerpt or '',
                'image_url': image_url,
                'publish_time': article.first_published_at.isoformat() if article.first_published_at else '',
                'publish_at': article.first_published_at.isoformat() if article.first_published_at else '',
                'slug': article.slug,
                'author': getattr(article, 'author_name', '') or '',
                'source': getattr(article, 'source', '') or 'æœ¬ç«™',
                'is_breaking': getattr(article, 'is_breaking', False),
                'is_live': getattr(article, 'is_live', False),
                'is_event_mode': getattr(article, 'is_event_mode', False),
                'has_video': getattr(article, 'has_video', False),
                'tags': [tag.name for tag in article.tags.all()] if hasattr(article, 'tags') else [],
            }
            
            # æ·»åŠ é¢‘é“ä¿¡æ¯
            if article.channel:
                item['channel'] = {
                    'id': article.channel.slug,
                    'name': article.channel.name,
                    'slug': article.channel.slug
                }
            
            # æ·»åŠ ä¸»é¢˜ä¿¡æ¯ï¼ˆtopicsæ˜¯å¤šå¯¹å¤šå…³ç³»ï¼‰
            if hasattr(article, 'topics') and article.topics.exists():
                first_topic = article.topics.first()
                if first_topic:
                    item['topic'] = {
                        'id': first_topic.slug if hasattr(first_topic, 'slug') else str(first_topic.id),
                        'name': first_topic.title if hasattr(first_topic, 'title') else first_topic.name,
                        'slug': first_topic.slug if hasattr(first_topic, 'slug') else str(first_topic.id)
                    }
            # å¦‚æœæ²¡æœ‰ä¸“é¢˜ï¼Œå¯ä»¥ä»æ ‡ç­¾ä¸­æ¨æ–­ä¸»é¢˜
            elif hasattr(article, 'tags') and article.tags.exists():
                first_tag = article.tags.first()
                if first_tag:
                    item['topic'] = {
                        'id': first_tag.slug if hasattr(first_tag, 'slug') else str(first_tag.id),
                        'name': first_tag.name,
                        'slug': first_tag.slug if hasattr(first_tag, 'slug') else str(first_tag.id)
                    }
            
            items.append(item)
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            'items': items,
            'total': len(items),
            'debug': {
                'site': site_name,
                'no_time_limit': True,  # æ ‡è¯†Heroæ— æ—¶é—´é™åˆ¶
                'requested_size': size,
                'returned_size': len(items),
                'query_type': 'database_direct',
                'api_version': 'hero_v2'  # ç‰ˆæœ¬å·æ›´æ–°
            }
        }
        
        # ç¼“å­˜ç»“æœï¼ˆ5åˆ†é’Ÿï¼‰
        cache.set(cache_key, response_data, 300)
        
        # æ·»åŠ ç¼“å­˜ä¿¡æ¯
        response_data['cache_info'] = {
            'hit': False,
            'ttl': 300,
            'type': 'hero_simple',
            'key': cache_key
        }
        
        return Response(response_data)
        
    except Exception as e:
        # é”™è¯¯å¤„ç†
        error_response = {
            'items': [],
            'total': 0,
            'error': {
                'message': 'Failed to fetch hero items',
                'type': 'database_error',
                'debug': str(e) if settings.DEBUG else None
            },
            'debug': {
                'site': site_name,
                'no_time_limit': True,  # æ ‡è¯†Heroæ— æ—¶é—´é™åˆ¶
                'requested_size': size,
                'api_version': 'hero_v2'  # ç‰ˆæœ¬å·æ›´æ–°
            }
        }
        
        return Response(error_response, status=500)
