import math, re, time, hashlib, json, base64
from datetime import datetime, timezone as dt_timezone, timedelta
from difflib import SequenceMatcher
from rest_framework.decorators import api_view, throttle_classes
from rest_framework.response import Response
from django.core.cache import cache
from django.conf import settings
from apps.searchapp.client import get_client, index_name_for
from apps.searchapp.queries import build_query
from apps.core.site_utils import get_site_from_request
from apps.news.models.article import ArticlePage
from wagtail.models import Site
from ..utils.rate_limit import FEED_RATE_LIMIT
from apps.core.flags import flag
from ..utils.modern_cache import (
    ModernCacheStrategy, ModernCacheManager, SmartCacheKey, CacheHeaders,
    BreakingNewsDetector, ContentType, CacheLayer,
    get_cache_time, generate_cache_key, should_cache
)


def _normalize_text(text: str) -> str:
    if not text:
        return ""
    t = text.lower()
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"[^\w\u4e00-\u9fff ]+", "", t)
    return t.strip()
def _slugify(text: str) -> str:
    base = _normalize_text(text)
    base = re.sub(r"\s+", "-", base)
    h = hashlib.md5(base.encode("utf-8")).hexdigest()[:6]
    return (base[:48] + ("-" if base else "") + h) or h



def _compute_headline_score(item: dict) -> float:
    pop_1h = float(item.get("pop_1h", 0.0))
    pop_24h = float(item.get("pop_24h", 0.0))
    quality = float(item.get("quality_score", 1.0))
    # æ–°é²œåº¦ï¼šæŒ‰å‘å¸ƒæ—¶é—´æŒ‡æ•°è¡°å‡ï¼ˆ12håŠè¡°æœŸï¼‰
    recency = 1.0
    try:
        pt = item.get("publish_at") or item.get("publish_time")
        if isinstance(pt, str) and len(pt) >= 10:
            ts = pt.replace("Z", "+00:00")
            dt = datetime.fromisoformat(ts)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=dt_timezone.utc)
            now = datetime.now(dt_timezone.utc)
            age_hours = max(0.0, (now - dt).total_seconds() / 3600.0)
            # åŠè¡°æœŸ 12h çš„æŒ‡æ•°è¡°å‡
            recency = math.pow(0.5, age_hours / 12.0)
        elif isinstance(pt, (int, float)):
            now = time.time()
            age_hours = max(0.0, (now - float(pt)) / 3600.0)
            recency = math.pow(0.5, age_hours / 12.0)
    except Exception:
        recency = 1.0

    # çˆ†å‘åº¦ï¼šçŸ­çª—å¢é‡ - é•¿çª—åŸºçº¿çš„ä¸€éƒ¨åˆ†
    burst = max(0.0, pop_1h - 0.25 * pop_24h)
    
    # å¤´æ¡ç®—æ³•ï¼šä¼˜å…ˆè€ƒè™‘å†…å®¹è´¨é‡å’Œå¤šæ ·æ€§ï¼Œé™ä½æ—¶é—´æ•æ„Ÿåº¦
    if pop_1h == 0.0 and pop_24h == 0.0:
        import hashlib
        
        # é¢‘é“å¤šæ ·æ€§å› å­ï¼šç¡®ä¿ä¸åŒé¢‘é“æœ‰ç›¸å¯¹å‡ç­‰çš„æœºä¼š
        channel = item.get("channel", "default")
        # ğŸ”§ å¤„ç†channelå¯èƒ½æ˜¯å­—å…¸çš„æƒ…å†µ
        if isinstance(channel, dict):
            channel_slug = channel.get("slug", "default")
        else:
            channel_slug = str(channel) if channel else "default"
        channel_hash = int(hashlib.md5(channel_slug.encode()).hexdigest(), 16) % 10000
        diversity_boost = 0.8 + (channel_hash / 10000) * 0.4  # 0.8-1.2èŒƒå›´ï¼Œæ›´å¤§å˜åŒ–
        
        # æ—¶é—´è¡°å‡æ›´æ¸©å’Œï¼š7å¤©å†…çš„æ–‡ç« éƒ½æœ‰æœºä¼šæˆä¸ºå¤´æ¡
        age_penalty = 1.0
        try:
            if isinstance(item.get("publish_at") or item.get("publish_time"), str):
                from datetime import datetime, timezone as dt_timezone
                pt_str = item.get("publish_at") or item.get("publish_time")
                pt = datetime.fromisoformat(pt_str.replace("Z", "+00:00"))
                if pt.tzinfo is None:
                    pt = pt.replace(tzinfo=dt_timezone.utc)
                age_days = (datetime.now(dt_timezone.utc) - pt).total_seconds() / 86400.0
                # 7å¤©å†…çº¿æ€§è¡°å‡ï¼Œ7å¤©åä¿æŒ0.3çš„åŸºç¡€åˆ†
                age_penalty = max(0.3, 1.0 - (age_days / 7.0) * 0.7)
        except Exception:
            age_penalty = 1.0
        
        # é‡æ–°å¹³è¡¡æƒé‡ï¼šè´¨é‡50% + å¤šæ ·æ€§40% + æ—¶é—´10%
        score = (0.5 * quality * diversity_boost + 0.4 * diversity_boost + 0.1 * age_penalty)
    else:
        # æœ‰æµè¡Œåº¦æ•°æ®æ—¶ä½¿ç”¨åŸæœ‰ç®—æ³•
        score = 0.55 * pop_1h + 0.25 * burst + 0.15 * quality + 0.05 * recency
    
    return score


def _cluster_items(items: list, similarity: float = 0.92) -> list:
    clusters = []  # æ¯é¡¹ï¼š{"rep": item, "items": [item,...], "key": str}
    for it in items:
        key = it.get("canonical_url") or it.get("url") or None
        if key:
            key = key.strip().lower()
        norm_title = _normalize_text(it.get("title", ""))

        placed = False
        # 1) URL çº§èšç±»
        if key:
            for cl in clusters:
                if cl.get("key") and cl["key"] == key:
                    cl["items"].append(it)
                    # æ›´æ–°ä»£è¡¨ä¸ºæ›´é«˜åˆ†è€…
                    if it.get("headline_score", 0) > cl["rep"].get("headline_score", 0):
                        cl["rep"] = it
                    placed = True
                    break
            if placed:
                continue

        # 2) æ ‡é¢˜ç›¸ä¼¼åº¦èšç±»
        for cl in clusters:
            rep_title = _normalize_text(cl["rep"].get("title", ""))
            try:
                sim = SequenceMatcher(None, norm_title, rep_title).ratio()
            except Exception:
                sim = 0.0
            if sim >= similarity:
                cl["items"].append(it)
                if it.get("headline_score", 0) > cl["rep"].get("headline_score", 0):
                    cl["rep"] = it
                placed = True
                break

        if not placed:
            clusters.append({"rep": it, "items": [it], "key": key})

    # è¿”å›å„ç°‡ä»£è¡¨ï¼Œå¹¶é™„ä¸Šæ¥æºæ•°é‡
    out = []
    for cl in clusters:
        rep = dict(cl["rep"])  # å¤åˆ¶ï¼Œé¿å…æ±¡æŸ“
        rep["more_sources"] = max(0, len(cl["items"]) - 1)
        rep["cluster_slug"] = _slugify(rep.get("title", "topic"))
        out.append(rep)
    return out


@api_view(["GET"])
@throttle_classes([])  # å¤ç”¨è‡ªå®šä¹‰é™æµ
@FEED_RATE_LIMIT
def headlines(request):
    site = get_site_from_request(request)
    size = max(1, min(int(request.query_params.get("size", 8)), 30))
    hours = int(request.query_params.get("hours", 24))  # é»˜è®¤24å°æ—¶çª—å£
    region = request.query_params.get("region")
    lang = request.query_params.get("lang")
    diversity = request.query_params.get("diversity", "med")
    req_channels = request.query_params.getlist("channel") or []
    req_channels = [ch for ch in req_channels if ch]
    exclude_clusters = request.query_params.getlist("exclude_cluster_ids") or []
    cursor_token = request.query_params.get("cursor")
    start_offset = 0
    if cursor_token:
        try:
            payload = json.loads(base64.urlsafe_b64decode(cursor_token.encode()).decode())
            start_offset = max(0, int(payload.get("offset", 0)))
        except Exception:
            start_offset = 0

    # ä¼šè¯çº§å»é‡ï¼ˆè·¨æ¨¡å—ï¼‰
    session_id = request.headers.get("X-Session-ID") or request.headers.get("X-AB-Session") or "anon"
    seen_cache_key = f"feed:seen:{site}:{session_id}"
    cached_seen = cache.get(seen_cache_key, [])
    cached_seen = [str(x) for x in (cached_seen or [])]
    combined_seen = list(dict.fromkeys(cached_seen))

    # å¬å›å€™é€‰ï¼šä¼˜å…ˆ hot/trending æ¸ é“ï¼Œæ”¾å¤§ä¸Šé™ï¼Œåç»­èšç±»å‹ç¼©
    import time as _t
    t0 = _t.time()
    client = get_client()
    index = index_name_for(site)
    # æé«˜ ES å€™é€‰é‡ï¼Œç¼“è§£å¤šæ ·æ€§ä¸å»é‡åçš„ç©ºé›†é£é™©
    elastic_size = max(size * 40, 400)
    
    # ğŸ¯ æ ¹æ®æ¨¡å¼é€‰æ‹©æŸ¥è¯¢æ¨¡æ¿
    mode = request.query_params.get("mode", "").lower()
    if mode == "topstories":
        query_template = "topstories_default"
    elif mode == "hero":
        query_template = "hero_default"
    else:
        query_template = "recommend_default"
    
    # ğŸ¯ æ ¹æ®æ¨¡å¼è®¾ç½®é¢‘é“ç­›é€‰ç­–ç•¥
    if mode == "topstories":
        # TopStoriesæ¨¡å¼ï¼šå¦‚æœæ²¡æœ‰æŒ‡å®šé¢‘é“ï¼Œåˆ™ä¸é™åˆ¶é¢‘é“ï¼ˆä¼ é€’Noneæˆ–ç©ºåˆ—è¡¨è®©æŸ¥è¯¢æ¨¡æ¿å¤„ç†ï¼‰
        query_channels = req_channels if req_channels else []
    elif mode == "hero":
        # Heroæ¨¡å¼ï¼šä¸é™åˆ¶é¢‘é“ï¼Œè·å–æ‰€æœ‰æ ‡è®°ä¸ºHeroçš„å†…å®¹
        query_channels = []
    else:
        # æ™®é€šæ¨¡å¼ï¼šä½¿ç”¨é»˜è®¤çš„çƒ­é—¨é¢‘é“
        query_channels = req_channels if req_channels else ["hot", "trending"]
    
    body = build_query(
        query_template,
        site=site,
        channels=query_channels,
        hours=hours,
        seen_ids=cached_seen,
        size=elastic_size,
    )

    candidates = []
    total_hits = 0
    returned_hits = 0
    try:
        resp = client.search(index=index, body=body, request_timeout=5)
        total_hits = resp.get("hits", {}).get("total", {}).get("value", 0)
        for h in resp.get("hits", {}).get("hits", []):
            returned_hits += 1
            src = h.get("_source", {})
            if str(h.get("_id")) in combined_seen:
                continue
            item = {"id": h.get("_id"), **src}
            # å…œåº• publish_at
            if not item.get("publish_at") and item.get("publish_time"):
                item["publish_at"] = item["publish_time"]
            
            # æ·»åŠ å°é¢å›¾ç‰‡ä¿¡æ¯ï¼ˆä»æ•°æ®åº“è·å–ï¼‰
            cover_url = None
            try:
                article_id = item.get("article_id") or item.get("id")
                if article_id:
                    # ä»æ•°æ®åº“è·å–æ–‡ç« å°é¢
                    try:
                        article = ArticlePage.objects.get(id=int(article_id))
                        if article.cover:
                            cover_url = f"http://192.168.8.195:8000/api/media/proxy/{article.cover.file.name}"
                    except (ArticlePage.DoesNotExist, ValueError):
                        pass
            except Exception:
                pass
            
            # æ·»åŠ å›¾ç‰‡å­—æ®µ
            item["image_url"] = cover_url
            item["cover"] = {"url": cover_url} if cover_url else None
            
            candidates.append(item)
        # è‹¥ESæ— ç»“æœï¼Œè¿›å…¥DBå›é€€
        if total_hits == 0 or not candidates:
            raise RuntimeError("Empty ES hits for headlines")
    except Exception:
        # DBå›é€€ï¼šå–æœ€è¿‘ hours å†…çš„æ–‡ç« 
        try:
            site_obj = Site.objects.get(hostname=site)
            qs = ArticlePage.objects.live().descendant_of(site_obj.root_page)
        except Exception:
            qs = ArticlePage.objects.live()
        try:
            since = datetime.now(dt_timezone.utc) - timedelta(hours=hours)
            qs = qs.filter(first_published_at__gte=since)
        except Exception:
            pass
        
        # DB å›é€€æ—¶æŒ‰ channel è¿‡æ»¤ï¼ˆè‹¥ä¼ å…¥ï¼‰
        if req_channels:
            try:
                qs = qs.filter(channel__slug__in=req_channels)
            except Exception:
                pass
        
        pages = list(qs.order_by('-first_published_at')[:elastic_size])
        if not pages:
            # æ‰©å¤§æ—¶é—´çª—è‡³7å¤©ç¡®ä¿æœ‰å†…å®¹
            try:
                since7 = datetime.now(dt_timezone.utc) - timedelta(hours=24*7)
                qs7 = qs.filter(first_published_at__gte=since7)
                pages = list(qs7.order_by('-first_published_at')[:elastic_size])
            except Exception:
                pages = []
        if not pages:
            # å›é€€åˆ°å…¨ç«™ï¼Œå¹¶åº”ç”¨ç›¸åŒæ—¶é—´çª—ç­–ç•¥
            qsg = ArticlePage.objects.live()
            try:
                since = datetime.now(dt_timezone.utc) - timedelta(hours=hours)
                qsg = qsg.filter(first_published_at__gte=since)
            except Exception:
                pass
            pages = list(qsg.order_by('-first_published_at')[:elastic_size])
            if not pages:
                try:
                    since7 = datetime.now(dt_timezone.utc) - timedelta(hours=24*7)
                    qsg7 = ArticlePage.objects.live().filter(first_published_at__gte=since7)
                    pages = list(qsg7.order_by('-first_published_at')[:elastic_size])
                except Exception:
                    pages = list(ArticlePage.objects.live().order_by('-first_published_at')[:elastic_size])
        for p in pages:
            # è·å–å°é¢å›¾ç‰‡URL
            cover_url = None
            if p.cover:
                try:
                    # ä½¿ç”¨åª’ä½“ä»£ç†URL
                    cover_url = f"http://192.168.8.195:8000/api/media/proxy/{p.cover.file.name}"
                except Exception:
                    pass
            
            item = {
                "id": str(p.id),
                "article_id": str(p.id),
                "title": p.title,
                "slug": p.slug,
                "publish_time": p.first_published_at.isoformat() if getattr(p, 'first_published_at', None) else None,
                "publish_at": p.first_published_at.isoformat() if getattr(p, 'first_published_at', None) else None,
                "channel": {
                    "id": str(p.channel.id) if p.channel else 'recommend',
                    "name": p.channel.name if p.channel else 'é¦–é¡µ',
                    "slug": p.channel.slug if p.channel else 'recommend'
                } if p.channel else {
                    "id": 'recommend',
                    "name": 'é¦–é¡µ', 
                    "slug": 'recommend'
                },
                "topic": getattr(p, 'topic_slug', ''),
                "author": getattr(p, 'author_name', ''),
                "image_url": cover_url,  # æ·»åŠ å›¾ç‰‡URL
                "cover": {"url": cover_url} if cover_url else None,  # æ·»åŠ coverå¯¹è±¡
                "quality_score": 1.0,
                "ctr_1h": 0.0,
                "pop_1h": 0.0,
                "pop_24h": 0.0,
            }
            if item["id"] in combined_seen:
                continue
            candidates.append(item)

    # åŒºåŸŸ/è¯­è¨€è½»é‡è¿‡æ»¤ï¼ˆè‹¥å­—æ®µå­˜åœ¨ï¼‰
    def _match_region(x: dict) -> bool:
        if not region:
            return True
        val = str(x.get("region") or x.get("locale") or "").lower()
        return (val.startswith(region.lower()) or val == region.lower()) if val else True

    def _match_lang(x: dict) -> bool:
        if not lang:
            return True
        val = str(x.get("lang") or x.get("language") or x.get("locale") or "").lower()
        return (val.startswith(lang.lower()) or val == lang.lower()) if val else True

    candidates = [c for c in candidates if _match_region(c) and _match_lang(c)]

    # æ‰“åˆ†ï¼ˆçˆ†å‘+æ–°é²œ+è´¨é‡ï¼‰
    for c in candidates:
        c["headline_score"] = _compute_headline_score(c)

    candidates.sort(key=lambda x: x.get("headline_score", 0), reverse=True)

    # èšç±»ï¼ˆæ•…äº‹çº§ï¼‰
    clustered = _cluster_items(candidates, similarity=0.88)

    # åŸºäºå¤šæ ·æ€§ä¸æ’é™¤åˆ—è¡¨è¿›è¡Œç­›é€‰
    # åŒç°‡å”¯ä¸€å·²ç”±èšç±»ä¿è¯ï¼›æ­¤å¤„é™åˆ¶æ¥æº(source)/é¢‘é“(channel)é‡å¤ï¼ŒåŒæ—¶åŠ å…¥ä¸»é¢˜/ç±»åˆ«é…é¢
    def _apply_diversity(items: list) -> list:
        if not items:
            return items
        seen_source = set()
        seen_channel = set()
        seen_topic = set()
        topic_quota = 1 if diversity == "high" else 2  # æ¯ä¸»é¢˜æœ€å¤šæ¡æ•°
        topic_count = {}
        out = []
        for it in items:
            src = (it.get("source") or "").strip().lower()
            # ğŸ”§ å¤„ç†channelå¯èƒ½æ˜¯å­—å…¸çš„æƒ…å†µ
            channel = it.get("channel") or ""
            if isinstance(channel, dict):
                ch = (channel.get("slug") or "").strip().lower()
            else:
                ch = str(channel).strip().lower()
            topic = (it.get("topic") or it.get("cluster_slug") or "").strip().lower()
            allow = True
            if diversity in ("med", "high") and src and src in seen_source:
                allow = False
            if diversity == "high" and ch and ch in seen_channel:
                allow = False
            if topic:
                cnt = topic_count.get(topic, 0)
                if cnt >= topic_quota:
                    allow = False
            if allow:
                out.append(it)
                if src:
                    seen_source.add(src)
                if ch:
                    seen_channel.add(ch)
                if topic:
                    topic_count[topic] = topic_count.get(topic, 0) + 1
            if len(out) >= (start_offset + size):
                break
        return out

    filtered = [x for x in clustered if x.get("cluster_slug") not in set(exclude_clusters)]
    # å…ˆæŒ‰çˆ†å‘åº¦æ’åºå†åš MMR å¤šæ ·æ€§é€‰å–
    def _burst_score(it: dict) -> float:
        p1 = float(it.get("pop_1h", 0.0))
        p24 = float(it.get("pop_24h", 0.0))
        quality = float(it.get("quality_score", 1.0))
        burst = max(0.0, p1 - 0.25 * p24)
        return 0.6 * p1 + 0.25 * burst + 0.15 * quality
    base_sorted = sorted(filtered, key=_burst_score, reverse=True)

    # ç®€æ˜“ MMRï¼šåœ¨å·²é€‰é›†åˆä¸Šæƒ©ç½šä¸å·²é€‰çš„è¯­ä¹‰/ç±»åˆ«ç›¸ä¼¼ï¼ˆè¿™é‡Œç”¨é¢‘é“ä¸ä¸»é¢˜è¿‘ä¼¼ï¼‰
    def _similarity(a: dict, b: dict) -> float:
        # ğŸ”§ å¤„ç†channelå¯èƒ½æ˜¯å­—å…¸çš„æƒ…å†µ
        def get_channel_slug(item):
            channel = item.get("channel") or ""
            if isinstance(channel, dict):
                return channel.get("slug") or ""
            return str(channel)
        
        ch_a = get_channel_slug(a).lower()
        ch_b = get_channel_slug(b).lower()
        sa = (ch_a + "|" + (a.get("topic") or a.get("cluster_slug") or "")).lower()
        sb = (ch_b + "|" + (b.get("topic") or b.get("cluster_slug") or "")).lower()
        return 1.0 if sa == sb else (0.5 if ch_a == ch_b else 0.0)

    lam = 0.7 if diversity == "high" else 0.5  # å¤šæ ·æ€§æƒé‡
    selected = []
    for cand in base_sorted:
        # è®¡ç®— MMR åˆ†æ•°ï¼šåŸºåˆ† - Î»*ä¸å·²é€‰æœ€é«˜ç›¸ä¼¼
        if not selected:
            selected.append(cand)
            continue
        sim_penalty = max((_similarity(cand, s) for s in selected), default=0.0)
        score = _burst_score(cand) - lam * sim_penalty
        cand["_mmr_score"] = score
        # æ’å…¥ä¿æŒæŒ‰ mmr æ’åºï¼Œç®€å•å¤„ç†
        selected.append(cand)
        selected.sort(key=lambda x: x.get("_mmr_score", _burst_score(x)), reverse=True)
        # æˆªæ–­
        if len(selected) > (start_offset + size) * 3:
            selected = selected[: (start_offset + size) * 3]

    filtered = _apply_diversity(selected)
    # è‹¥å¤šæ ·æ€§è¿‡æ»¤åæ•°é‡ä¸è¶³ï¼Œæ”¾å®½è§„åˆ™ä»¥ä¿è¯æœ€å°å¯ç”¨
    if len(filtered) < size:
        # å…ˆæ”¾å®½åˆ°æŒ‰é¢‘é“æœ€å¤š2æ¡
        max_per_channel = 2 if diversity == "high" else 3
        by_channel = {}
        relaxed = []
        for it in [x for x in clustered if x.get("cluster_slug") not in set(exclude_clusters)]:
            # ğŸ”§ å¤„ç†channelå¯èƒ½æ˜¯å­—å…¸çš„æƒ…å†µ
            channel = it.get("channel") or ""
            if isinstance(channel, dict):
                ch = (channel.get("slug") or "").strip().lower()
            else:
                ch = str(channel).strip().lower()
            cnt = by_channel.get(ch, 0)
            if cnt < max_per_channel:
                relaxed.append(it)
                by_channel[ch] = cnt + 1
            if len(relaxed) >= (start_offset + size):
                break
        if relaxed:
            filtered = relaxed
    # è‹¥ä»ä¸è¶³ï¼Œç›´æ¥ä½¿ç”¨æœªé™åˆ¶çš„èšç±»ç»“æœï¼ˆä»éµå®ˆæ’é™¤åˆ—è¡¨ï¼‰
    if len(filtered) < size:
        filtered = [x for x in clustered if x.get("cluster_slug") not in set(exclude_clusters)]

    # ç°ä»£åŒ–ç¼“å­˜ç­–ç•¥
    cache_key = None
    content_type = ContentType.NORMAL
    
    if start_offset == 0 and not exclude_clusters:
        # 1. æ™ºèƒ½æ£€æµ‹å†…å®¹ç±»å‹
        request_data = {
            'diversity': diversity,
            'hours': hours,
            'channels': req_channels,
            'region': region,
            'lang': lang
        }
        content_type = ModernCacheStrategy.detect_content_type(request_data)
        
        # 2. ç”Ÿæˆç°ä»£ç¼“å­˜key
        cache_params = {
            'hours': hours,
            'diversity': diversity, 
            'size': size,
            'region': region,
            'lang': lang,
            'channels': req_channels
        }
        
        session_id = request.headers.get('X-Session-ID')
        user_id = session_id if session_id and session_id.startswith('user_') else None
        
        cache_key = generate_cache_key(
            content_type, CacheLayer.BACKEND, site, cache_params, user_id
        )
        
        # 3. æ£€æŸ¥ç¼“å­˜
        if should_cache(content_type, CacheLayer.BACKEND):
            cached = ModernCacheManager.get(cache_key)
            if cached:
                response = Response(cached)
                # è®¾ç½®ç°ä»£å“åº”å¤´
                for key, value in CacheHeaders.generate_response_headers(content_type).items():
                    response[key] = value
                return response

    # æ¸¸æ ‡åˆ†é¡µ
    end = start_offset + size
    top = filtered[start_offset:end]

    t1 = _t.time()
    debug = {
        "site": site,
        "hours": hours,
        "total_hits": total_hits,
        "returned_hits": returned_hits,
        "candidates": len(candidates),
        "clusters": len(clustered),
        "perf_ms": int((t1 - t0) * 1000)
    }

    # æ›´æ–°ä¼šè¯çº§ seenï¼šåˆå¹¶å¹¶æˆªæ–­
    new_seen = combined_seen + [str(t.get("id") or t.get("article_id")) for t in top]
    # å»é‡å¹¶ä¿ç•™æœ€è¿‘500
    new_seen = list(dict.fromkeys(new_seen))[-500:]
    cache.set(seen_cache_key, new_seen, timeout=6*3600)

    # è®¡ç®—è¶‹åŠ¿ï¼šåŸºäº 1h/24h ä¸æ–°é²œåº¦
    def _trend(it: dict) -> str:
        p1 = float(it.get("pop_1h", 0.0))
        p24 = float(it.get("pop_24h", 0.0))
        if p24 <= 1e-6 and p1 > 0:
            return "up"
        ratio = p1 / max(1e-6, p24)
        return "up" if ratio >= 1.2 else ("down" if ratio <= 0.6 and p1 < p24 else "stable")

    for item in top:
        item["trend"] = _trend(item)

    next_cursor = None
    if end < len(filtered):
        token = base64.urlsafe_b64encode(json.dumps({"offset": end}).encode()).decode()
        next_cursor = token

    # åŠ¨æ€æ£€æµ‹çªå‘æ–°é—»
    if BreakingNewsDetector.detect(top):
        content_type = ContentType.BREAKING
    
    # è·å–ç¼“å­˜æ—¶é—´
    cache_time = get_cache_time(content_type, CacheLayer.BACKEND)
    
    payload = {
        "items": top, 
        "next_cursor": next_cursor,
        "content_type": content_type.value,
        "cache_strategy": {
            "type": content_type.value,
            "backend_ttl": cache_time,
            "gateway_ttl": get_cache_time(content_type, CacheLayer.GATEWAY),
            "cdn_ttl": get_cache_time(content_type, CacheLayer.CDN)
        }
    }
    
    if flag("features.debug_enabled", False):
        payload["debug"] = debug
    
    # ç°ä»£åŒ–ç¼“å­˜è®¾ç½®
    if cache_key and should_cache(content_type, CacheLayer.BACKEND):
        ModernCacheManager.set(cache_key, payload, cache_time, content_type)
    
    # æ„å»ºå“åº”
    response = Response(payload)
    
    # è®¾ç½®ç°ä»£å“åº”å¤´
    for key, value in CacheHeaders.generate_response_headers(content_type).items():
        response[key] = value
    
    return response


