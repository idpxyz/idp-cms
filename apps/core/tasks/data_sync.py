"""
æ ¸å¿ƒæ•°æ®åŒæ­¥å’Œè¡Œä¸ºåˆ†æç›¸å…³çš„Celeryä»»åŠ¡
"""

from celery import shared_task
from django.conf import settings
from django.utils import timezone
import logging

logger = logging.getLogger(__name__)


@shared_task(bind=True, autoretry_for=(Exception,), retry_backoff=True, max_retries=3)
def batch_sync_article_weights(self, article_ids=None, limit=1000):
    """
    æ‰¹é‡æ›´æ–°æ–‡ç« æƒé‡ï¼ˆé€‚åˆå®šæ—¶ä»»åŠ¡ï¼‰
    
    Args:
        article_ids: æŒ‡å®šæ–‡ç« IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ›´æ–°æ‰€æœ‰
        limit: æ‰¹é‡å¤„ç†é™åˆ¶
    
    Returns:
        dict: å¤„ç†ç»“æœç»Ÿè®¡
    """
    try:
        from apps.core.services.data_sync_service import data_sync_service
        from apps.news.models.article import ArticlePage
        
        # æ„å»ºæŸ¥è¯¢
        if article_ids:
            articles = ArticlePage.objects.filter(id__in=article_ids[:limit])
        else:
            # ä¼˜å…ˆæ›´æ–°æœ€è¿‘ä¿®æ”¹çš„æ–‡ç« 
            articles = ArticlePage.objects.filter(
                live=True
            ).order_by('-updated_at')[:limit]
        
        updated_count = 0
        failed_count = 0
        
        for article in articles:
            try:
                # ğŸ¯ åœ¨æƒé‡åŒæ­¥å‰ï¼Œåˆ·æ–°æ–‡ç« å®ä¾‹ä»¥è·å–æœ€æ–°çŠ¶æ€
                article.refresh_from_db()
                
                if data_sync_service.sync_article_weight(article):
                    updated_count += 1
                else:
                    failed_count += 1
            except Exception as e:
                logger.error(f"æ›´æ–°æ–‡ç«  {article.id} æƒé‡å¤±è´¥: {e}")
                failed_count += 1
        
        result = {
            'success': True,
            'updated_count': updated_count,
            'failed_count': failed_count,
            'total_processed': articles.count(),
            'timestamp': timezone.now().isoformat()
        }
        
        logger.info(f"æ‰¹é‡æƒé‡æ›´æ–°å®Œæˆ: {result}")
        return result
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æƒé‡æ›´æ–°ä»»åŠ¡å¤±è´¥: {e}")
        # Celeryè‡ªåŠ¨é‡è¯•
        raise


@shared_task(bind=True, autoretry_for=(Exception,), retry_backoff=True, max_retries=2)
def sync_articles_to_opensearch_batch(self, article_ids=None, limit=100):
    """
    æ‰¹é‡åŒæ­¥æ–‡ç« åˆ°OpenSearchï¼ˆè¡¥å……ç°æœ‰çš„å•ç¯‡åŒæ­¥ï¼‰
    
    Args:
        article_ids: æŒ‡å®šæ–‡ç« IDåˆ—è¡¨
        limit: æ‰¹é‡å¤„ç†é™åˆ¶
    
    Returns:
        dict: åŒæ­¥ç»“æœç»Ÿè®¡
    """
    try:
        from apps.core.services.data_sync_service import data_sync_service
        
        result = data_sync_service.batch_sync_articles(article_ids, limit)
        
        logger.info(f"æ‰¹é‡OpenSearchåŒæ­¥å®Œæˆ: {result}")
        return result
        
    except Exception as e:
        logger.error(f"æ‰¹é‡OpenSearchåŒæ­¥å¤±è´¥: {e}")
        raise


@shared_task
def update_trending_articles_cache():
    """
    æ›´æ–°çƒ­é—¨æ–‡ç« ç¼“å­˜
    
    å®šæœŸè®¡ç®—å’Œç¼“å­˜ä¸åŒæ—¶é—´æ®µçš„çƒ­é—¨æ–‡ç« ï¼Œæå‡APIå“åº”é€Ÿåº¦
    """
    try:
        from django.core.cache import cache
        from apps.core.services.behavior_analytics import behavior_analytics
        
        # æ›´æ–°ä¸åŒæ—¶é—´æ®µçš„çƒ­é—¨æ–‡ç« 
        time_ranges = [1, 6, 24, 168]  # 1h, 6h, 24h, 1week
        updated_ranges = []
        
        for hours in time_ranges:
            try:
                trending = behavior_analytics.get_trending_articles(hours=hours, limit=20)
                cache_key = f'trending_articles_{hours}h'
                cache.set(cache_key, trending, timeout=3600)  # ç¼“å­˜1å°æ—¶
                updated_ranges.append(hours)
                logger.debug(f"æ›´æ–° {hours}h çƒ­é—¨æ–‡ç« ç¼“å­˜ï¼Œ{len(trending)} ç¯‡æ–‡ç« ")
            except Exception as e:
                logger.warning(f"æ›´æ–° {hours}h çƒ­é—¨æ–‡ç« ç¼“å­˜å¤±è´¥: {e}")
        
        result = {
            'success': True,
            'updated_ranges': updated_ranges,
            'timestamp': timezone.now().isoformat()
        }
        
        logger.info(f"çƒ­é—¨æ–‡ç« ç¼“å­˜æ›´æ–°å®Œæˆ: {result}")
        return result
        
    except Exception as e:
        logger.error(f"æ›´æ–°çƒ­é—¨æ–‡ç« ç¼“å­˜å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}


@shared_task
def comprehensive_data_consistency_check():
    """
    ç»¼åˆæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
    
    æ£€æŸ¥PostgreSQLã€OpenSearchã€ClickHouseä¹‹é—´çš„æ•°æ®ä¸€è‡´æ€§
    """
    try:
        from apps.core.services.data_sync_service import data_sync_service
        
        # æ£€æŸ¥ä¸»è¦ç«™ç‚¹çš„æ•°æ®ä¸€è‡´æ€§
        sites_to_check = [settings.SITE_HOSTNAME, 'localhost']
        results = {}
        
        for site in sites_to_check:
            try:
                result = data_sync_service.check_data_consistency(site)
                results[site] = result
                
                # å¦‚æœå‘ç°ä¸¥é‡ä¸ä¸€è‡´ï¼Œè§¦å‘è‡ªåŠ¨ä¿®å¤
                consistency = result.get('consistency', {})
                if not consistency.get('is_consistent', True):
                    logger.warning(f"ç«™ç‚¹ {site} æ•°æ®ä¸ä¸€è‡´ï¼Œå»¶è¿Ÿè§¦å‘è‡ªåŠ¨ä¿®å¤")
                    # ğŸ¯ å»¶è¿Ÿè§¦å‘ä¿®å¤ä»»åŠ¡ï¼Œé¿å…æ•°æ®ç«äº‰ï¼ˆå»¶è¿Ÿ5åˆ†é’Ÿï¼‰
                    sync_articles_to_opensearch_batch.apply_async(
                        kwargs={'limit': 50}, 
                        countdown=300  # 5åˆ†é’Ÿåæ‰§è¡Œ
                    )
                    
            except Exception as e:
                logger.error(f"æ£€æŸ¥ç«™ç‚¹ {site} ä¸€è‡´æ€§å¤±è´¥: {e}")
                results[site] = {'error': str(e)}
        
        overall_result = {
            'success': True,
            'sites_checked': sites_to_check,
            'results': results,
            'timestamp': timezone.now().isoformat()
        }
        
        logger.info(f"ç»¼åˆæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆ: {len(sites_to_check)} ä¸ªç«™ç‚¹")
        return overall_result
        
    except Exception as e:
        logger.error(f"ç»¼åˆæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}


@shared_task
def cleanup_old_behavior_data(days_to_keep=90):
    """
    æ¸…ç†è¿‡æœŸçš„è¡Œä¸ºæ•°æ®
    
    Args:
        days_to_keep: ä¿ç•™çš„å¤©æ•°ï¼Œé»˜è®¤90å¤©
    """
    try:
        from apps.core.services.behavior_analytics import behavior_analytics
        
        client = behavior_analytics.get_clickhouse_client()
        if not client:
            return {'success': False, 'error': 'ClickHouseè¿æ¥å¤±è´¥'}
        
        # åˆ é™¤æŒ‡å®šå¤©æ•°å‰çš„æ•°æ®
        query = f"""
            ALTER TABLE events DELETE WHERE ts < subtractDays(now(), {days_to_keep})
        """
        
        result = client.execute(query)
        
        logger.info(f"æ¸…ç† {days_to_keep} å¤©å‰çš„è¡Œä¸ºæ•°æ®å®Œæˆ")
        return {
            'success': True,
            'days_kept': days_to_keep,
            'timestamp': timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"æ¸…ç†è¿‡æœŸè¡Œä¸ºæ•°æ®å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}


@shared_task
def generate_user_behavior_insights():
    """
    ç”Ÿæˆç”¨æˆ·è¡Œä¸ºæ´å¯ŸæŠ¥å‘Š
    
    å®šæœŸåˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼ï¼Œç”Ÿæˆæ´å¯ŸæŠ¥å‘Š
    """
    try:
        from apps.core.services.behavior_analytics import behavior_analytics
        from django.core.cache import cache
        
        insights = {}
        
        # 1. çƒ­é—¨æ–‡ç« åˆ†æ
        trending_24h = behavior_analytics.get_trending_articles(hours=24, limit=10)
        insights['trending_articles'] = trending_24h
        
        # 2. ç¼“å­˜æ´å¯ŸæŠ¥å‘Š
        cache.set('behavior_insights', insights, timeout=3600 * 6)  # ç¼“å­˜6å°æ—¶
        
        logger.info(f"ç”¨æˆ·è¡Œä¸ºæ´å¯ŸæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œçƒ­é—¨æ–‡ç« : {len(trending_24h)} ç¯‡")
        return {
            'success': True,
            'insights_generated': list(insights.keys()),
            'timestamp': timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆç”¨æˆ·è¡Œä¸ºæ´å¯Ÿå¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}
