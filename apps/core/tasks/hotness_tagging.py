"""
çƒ­åº¦æ ‡è®°ä»»åŠ¡ - å®šæœŸé‡æ–°è®¡ç®—æ–‡ç« çƒ­åº¦å¹¶æ›´æ–°ESç´¢å¼•

è¿™ä¸ªä»»åŠ¡ä¼šï¼š
1. è·å–æœ€è¿‘å‘å¸ƒçš„æ–‡ç« 
2. è®¡ç®—å…¶çƒ­åº¦è¯„åˆ†
3. åŠ¨æ€åˆ†ç±»ä¸º hot/trending/normal
4. æ›´æ–°ElasticSearchç´¢å¼•ä¸­çš„è™šæ‹Ÿé¢‘é“æ ‡ç­¾
"""

import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from django.conf import settings
from django.utils import timezone
from celery import shared_task
from apps.news.models.article import ArticlePage
from apps.core.services.hotness_calculator import HotnessCalculator
from apps.searchapp.client import get_client
from apps.searchapp.alias import write_alias
from apps.core.utils.circuit_breaker import get_breaker

logger = logging.getLogger(__name__)


@shared_task(bind=True, autoretry_for=(Exception,), retry_backoff=True, max_retries=3)
def update_article_hotness_tags(self, site: str = None, hours_back: int = 72, batch_size: int = 100):
    """
    æ›´æ–°æ–‡ç« çƒ­åº¦æ ‡è®°
    
    Args:
        site: ç«™ç‚¹æ ‡è¯†ç¬¦
        hours_back: å›æº¯å°æ—¶æ•°ï¼Œåªå¤„ç†è¿™ä¸ªæ—¶é—´å†…çš„æ–‡ç« 
        batch_size: æ‰¹å¤„ç†å¤§å°
    """
    site = site or getattr(settings, 'SITE_HOSTNAME', 'localhost')
    
    try:
        logger.info(f"å¼€å§‹æ›´æ–°æ–‡ç« çƒ­åº¦æ ‡è®°: site={site}, hours_back={hours_back}")
        
        # è·å–éœ€è¦æ›´æ–°çš„æ–‡ç« 
        since = timezone.now() - timedelta(hours=hours_back)
        
        # æŸ¥è¯¢æœ€è¿‘å‘å¸ƒçš„æ–‡ç« 
        articles_qs = ArticlePage.objects.live().filter(
            first_published_at__gte=since
        ).order_by('-first_published_at')
        
        # æŒ‰ç«™ç‚¹è¿‡æ»¤ï¼ˆå¦‚æœé…ç½®äº†å¤šç«™ç‚¹ï¼‰
        try:
            from wagtail.models import Site
            site_obj = Site.objects.get(hostname=site)
            articles_qs = articles_qs.descendant_of(site_obj.root_page)
        except:
            # å•ç«™ç‚¹æ¨¡å¼æˆ–ç«™ç‚¹ä¸å­˜åœ¨æ—¶ä¸è¿‡æ»¤
            pass
        
        total_articles = articles_qs.count()
        logger.info(f"æ‰¾åˆ° {total_articles} ç¯‡æ–‡ç« éœ€è¦æ›´æ–°çƒ­åº¦æ ‡è®°")
        
        if total_articles == 0:
            logger.info("æ²¡æœ‰éœ€è¦æ›´æ–°çš„æ–‡ç« ")
            return {"processed": 0, "updated": 0, "errors": 0}
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        processed = 0
        updated = 0
        errors = 0
        
        # åˆ†æ‰¹å¤„ç†
        calculator = HotnessCalculator()
        es_client = get_client()
        write_index = write_alias(site)
        
        for batch_start in range(0, total_articles, batch_size):
            batch_articles = list(articles_qs[batch_start:batch_start + batch_size])
            
            try:
                # æ‰¹é‡è®¡ç®—çƒ­åº¦
                article_data = []
                for article in batch_articles:
                    article_data.append({
                        'id': article.id,
                        'publish_time': article.first_published_at,
                        'quality_score': getattr(article, 'quality_score', 1.0),
                    })
                
                # è®¡ç®—çƒ­åº¦åˆ†ç±»
                classified = calculator.batch_classify_articles(article_data, site)
                
                # æ‰¹é‡æ›´æ–°ESç´¢å¼•
                bulk_body = []
                for item in classified:
                    article_id = str(item['id'])
                    category = item['hotness_category']
                    
                    # å‡†å¤‡æ›´æ–°æ–‡æ¡£
                    update_doc = {
                        'hotness_score': item['hotness_score'],
                        'hotness_category': category,
                        'ctr_1h': item['ctr_1h'],
                        'pop_1h': item['pop_1h'],
                    }
                    
                    # ğŸ¯ å…³é”®ï¼šæ ¹æ®åˆ†ç±»åŠ¨æ€è®¾ç½®channelå­—æ®µ
                    if category == 'hot':
                        update_doc['channel'] = 'hot'
                        update_doc['original_channel'] = item.get('primary_channel_slug', 'recommend')
                    elif category == 'trending':
                        update_doc['channel'] = 'trending'
                        update_doc['original_channel'] = item.get('primary_channel_slug', 'recommend')
                    else:
                        # normalåˆ†ç±»ï¼šæ¢å¤åŸå§‹é¢‘é“
                        original_channel = _get_article_original_channel(batch_articles, article_id)
                        update_doc['channel'] = original_channel
                        if 'original_channel' in update_doc:
                            del update_doc['original_channel']
                    
                    # æ·»åŠ åˆ°æ‰¹é‡æ“ä½œ
                    bulk_body.extend([
                        {"update": {"_index": write_index, "_id": article_id}},
                        {"doc": update_doc, "doc_as_upsert": True}
                    ])
                
                # æ‰§è¡Œæ‰¹é‡æ›´æ–°
                if bulk_body:
                    response = es_client.bulk(body=bulk_body, refresh=True)
                    
                    # æ£€æŸ¥æ›´æ–°ç»“æœ
                    batch_updated = 0
                    batch_errors = 0
                    
                    for item in response.get('items', []):
                        if 'update' in item:
                            if item['update'].get('status') in [200, 201]:
                                batch_updated += 1
                            else:
                                batch_errors += 1
                                logger.warning(f"ESæ›´æ–°å¤±è´¥: {item}")
                    
                    updated += batch_updated
                    errors += batch_errors
                    
                    logger.info(f"æ‰¹æ¬¡å¤„ç†å®Œæˆ: {len(classified)}ç¯‡æ–‡ç« , æ›´æ–°æˆåŠŸ{batch_updated}, å¤±è´¥{batch_errors}")
                
                processed += len(classified)
                
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥ {batch_start}-{batch_start + len(batch_articles)}: {e}")
                errors += len(batch_articles)
        
        result = {
            "processed": processed,
            "updated": updated,
            "errors": errors,
            "total_articles": total_articles
        }
        
        logger.info(f"çƒ­åº¦æ ‡è®°æ›´æ–°å®Œæˆ: {result}")
        return result
        
    except Exception as e:
        logger.error(f"çƒ­åº¦æ ‡è®°ä»»åŠ¡å¤±è´¥: {e}")
        raise


def _get_article_original_channel(articles: List[ArticlePage], article_id: str) -> str:
    """è·å–æ–‡ç« çš„åŸå§‹é¢‘é“slug"""
    try:
        article = next((a for a in articles if str(a.id) == article_id), None)
        if article and hasattr(article, 'channel') and article.channel:
            return article.channel.slug
    except:
        pass
    return 'recommend'


@shared_task(bind=True, autoretry_for=(Exception,), retry_backoff=True, max_retries=2)
def refresh_hot_trending_articles(self, site: str = None):
    """
    å¿«é€Ÿåˆ·æ–°hot/trendingæ–‡ç« æ ‡è®°
    åªå¤„ç†æœ€è¿‘1å°æ—¶çš„æ–‡ç« ï¼Œç”¨äºå®æ—¶å“åº”çƒ­ç‚¹
    """
    return update_article_hotness_tags.apply_async(
        kwargs={'site': site, 'hours_back': 1, 'batch_size': 50}
    ).get()


@shared_task(bind=True, autoretry_for=(Exception,), retry_backoff=True, max_retries=2)
def daily_hotness_cleanup(self, site: str = None):
    """
    æ¯æ—¥çƒ­åº¦æ¸…ç†ä»»åŠ¡
    æ¸…ç†è¿‡æœŸçš„çƒ­åº¦æ ‡è®°ï¼Œæ¢å¤æ–‡ç« çš„åŸå§‹é¢‘é“åˆ†ç±»
    """
    site = site or getattr(settings, 'SITE_HOSTNAME', 'localhost')
    
    try:
        logger.info(f"å¼€å§‹æ¯æ—¥çƒ­åº¦æ¸…ç†: site={site}")
        
        # è·å–7å¤©å‰çš„æ—¶é—´ç‚¹
        cutoff_time = timezone.now() - timedelta(days=7)
        
        # æŸ¥è¯¢è¿‡æœŸçš„hot/trendingæ–‡ç« 
        old_articles = ArticlePage.objects.live().filter(
            first_published_at__lt=cutoff_time
        )
        
        # æŒ‰ç«™ç‚¹è¿‡æ»¤
        try:
            from wagtail.models import Site
            site_obj = Site.objects.get(hostname=site)
            old_articles = old_articles.descendant_of(site_obj.root_page)
        except:
            pass
        
        total_old = old_articles.count()
        logger.info(f"æ‰¾åˆ° {total_old} ç¯‡è¿‡æœŸæ–‡ç« éœ€è¦æ¸…ç†çƒ­åº¦æ ‡è®°")
        
        if total_old == 0:
            return {"cleaned": 0}
        
        # æ‰¹é‡æ¢å¤åŸå§‹é¢‘é“
        es_client = get_client()
        write_index = write_alias(site)
        cleaned = 0
        
        for batch_start in range(0, total_old, 100):
            batch_articles = list(old_articles[batch_start:batch_start + 100])
            bulk_body = []
            
            for article in batch_articles:
                original_channel = 'recommend'
                if hasattr(article, 'channel') and article.channel:
                    original_channel = article.channel.slug
                
                bulk_body.extend([
                    {"update": {"_index": write_index, "_id": str(article.id)}},
                    {
                        "doc": {
                            "channel": original_channel,
                            "hotness_category": "normal",
                            "hotness_score": 0.0
                        },
                        "doc_as_upsert": True
                    }
                ])
            
            if bulk_body:
                try:
                    es_client.bulk(body=bulk_body, refresh=True)
                    cleaned += len(batch_articles)
                    logger.info(f"æ¸…ç†æ‰¹æ¬¡å®Œæˆ: {len(batch_articles)}ç¯‡æ–‡ç« ")
                except Exception as e:
                    logger.error(f"æ¸…ç†æ‰¹æ¬¡å¤±è´¥: {e}")
        
        result = {"cleaned": cleaned, "total_old": total_old}
        logger.info(f"æ¯æ—¥çƒ­åº¦æ¸…ç†å®Œæˆ: {result}")
        return result
        
    except Exception as e:
        logger.error(f"æ¯æ—¥çƒ­åº¦æ¸…ç†å¤±è´¥: {e}")
        raise


# ä¾¿æ·å‡½æ•°
def trigger_hotness_update(site: str = None, immediate: bool = False):
    """
    è§¦å‘çƒ­åº¦æ›´æ–°
    
    Args:
        site: ç«™ç‚¹æ ‡è¯†
        immediate: æ˜¯å¦ç«‹å³æ‰§è¡Œï¼ˆåŒæ­¥ï¼‰ï¼Œå¦åˆ™å¼‚æ­¥æ‰§è¡Œ
    """
    if immediate:
        return update_article_hotness_tags(site=site)
    else:
        return update_article_hotness_tags.delay(site=site)


def trigger_fast_refresh(site: str = None):
    """è§¦å‘å¿«é€Ÿåˆ·æ–°ï¼ˆ1å°æ—¶å†…æ–‡ç« ï¼‰"""
    return refresh_hot_trending_articles.delay(site=site)
