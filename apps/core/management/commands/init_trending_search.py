import logging
import random
from datetime import datetime, timedelta
from collections import Counter
from django.core.management.base import BaseCommand
from django.conf import settings
from django.utils import timezone
from clickhouse_driver import Client
from wagtail.models import Site
from apps.news.models import ArticlePage
import re
import jieba
import jieba.analyse

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    help = 'åŸºäºç°æœ‰æ–‡ç« æ•°æ®ç”Ÿæˆåˆå§‹åŒ–çƒ­æœæ¦œ'

    def add_arguments(self, parser):
        parser.add_argument(
            '--site',
            type=str,
            default='localhost',
            help='ç«™ç‚¹æ ‡è¯† (é»˜è®¤: localhost)'
        )
        parser.add_argument(
            '--days',
            type=int,
            default=30,
            help='åˆ†ææœ€è¿‘Nå¤©çš„æ–‡ç«  (é»˜è®¤: 30)'
        )
        parser.add_argument(
            '--limit',
            type=int,
            default=50,
            help='ç”Ÿæˆçš„çƒ­æœè¯æ•°é‡ (é»˜è®¤: 50)'
        )
        parser.add_argument(
            '--min-length',
            type=int,
            default=2,
            help='å…³é”®è¯æœ€å°é•¿åº¦ (é»˜è®¤: 2)'
        )
        parser.add_argument(
            '--dry-run',
            action='store_true',
            help='åªæ˜¾ç¤ºç»“æœï¼Œä¸å†™å…¥ClickHouse'
        )

    def get_clickhouse_client(self):
        """è·å–ClickHouseå®¢æˆ·ç«¯"""
        try:
            # ä½¿ç”¨å’Œtrack APIç›¸åŒçš„è¿æ¥æ–¹å¼
            return Client.from_url(settings.CLICKHOUSE_URL)
        except Exception as e:
            self.stdout.write(
                self.style.ERROR(f'ClickHouseè¿æ¥å¤±è´¥: {e}')
            )
            return None

    def extract_keywords_from_text(self, text, topK=10):
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        if not text:
            return []
        
        # ä½¿ç”¨jiebaçš„TF-IDFç®—æ³•æå–å…³é”®è¯
        keywords = jieba.analyse.extract_tags(
            text, 
            topK=topK * 2,  # æå–æ›´å¤šå…³é”®è¯ç”¨äºç­›é€‰
            withWeight=True,
            allowPOS=('n', 'nz', 'v', 'vd', 'vn', 'l', 'a', 'd')
        )
        
        # è¿‡æ»¤å¹¶ç»™é•¿è¯åŠ æƒ
        filtered_keywords = []
        for word, weight in keywords:
            if len(word) >= self.min_length:
                # ç»™é•¿è¯æ›´é«˜çš„æƒé‡
                length_boost = 1.0 + (len(word) - 2) * 0.3  # æ¯å¢åŠ ä¸€ä¸ªå­—ç¬¦ï¼Œæƒé‡å¢åŠ 30%
                boosted_weight = weight * length_boost
                filtered_keywords.append((word, boosted_weight))
        
        # æŒ‰æƒé‡æ’åºå¹¶è¿”å›å‰topKä¸ª
        filtered_keywords.sort(key=lambda x: x[1], reverse=True)
        return filtered_keywords[:topK]

    def clean_keyword(self, keyword):
        """æ¸…ç†å…³é”®è¯"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—
        keyword = re.sub(r'[^\u4e00-\u9fa5a-zA-Z]', '', keyword)
        # è¿‡æ»¤æ‰å¸¸è§åœç”¨è¯å’Œé€šç”¨è¯æ±‡
        stop_words = {
            # æ–°é—»ç±»é€šç”¨è¯ï¼ˆä¿ç•™ä¸€äº›å¯èƒ½æœ‰æ„ä¹‰çš„è¯ï¼‰
            'æ–°é—»', 'èµ„è®¯', 'æŠ¥é“', 'æ¶ˆæ¯', 'é€šçŸ¥', 'å…¬å‘Š', 
            'è®¿è°ˆ', 'ä¸“è®¿', 'å®å½•', 'ç›´å‡»', 'è§‚å¯Ÿ', 'åˆ†æ', 'è§£æ',
            'ä¸“é¢˜', 'ç‹¬å®¶', 'è¿½è¸ª', 'è¦é—»', 'çœ‹ç‚¹', 'æŠ¥å‘Š',
            # æ—¶é—´è¯
            'ä»Šæ—¥', 'æ˜¨æ—¥', 'ä»Šå¤©', 'æ˜¨å¤©', 'æœ€æ–°', 'çƒ­é—¨',
            'æœ¬å‘¨', 'æœ¬æœˆ', 'ä»Šå¹´', 'å»å¹´', 'è¿‘æ—¥', 'æ—¥å‰', 'ç›®å‰',
            # é€šç”¨åŠ¨è¯
            'ç›¸å…³', 'æœ‰å…³', 'å…³äº', 'è¿›è¡Œ', 'å¼€å±•', 'å®æ–½',
            'å‘ç”Ÿ', 'å‡ºç°', 'æ˜¾ç¤º', 'è¡¨ç¤º', 'è®¤ä¸º', 'æŒ‡å‡º', 'æåˆ°',
            'è¿æ¥', 'é¢ä¸´', 'é‡åˆ°', 'è¾¾åˆ°', 'å®ç°', 'å®Œæˆ', 'å¼€å§‹',
            # ä»£è¯å’Œé‡è¯
            'ä¸€ä¸ª', 'ä¸€äº›', 'è¿™ä¸ª', 'é‚£ä¸ª', 'æˆ‘ä»¬', 'ä»–ä»¬', 'è‡ªå·±',
            'å¤šä¸ª', 'å‡ ä¸ª', 'æ‰€æœ‰', 'å…¨éƒ¨', 'éƒ¨åˆ†', 'å¤§é‡', 'å°‘é‡',
            # è¿‡äºé€šç”¨çš„è¯
            'é—®é¢˜', 'æƒ…å†µ', 'æ–¹é¢', 'å†…å®¹', 'ç»“æœ', 'æ•ˆæœ', 'å½±å“',
            'ä½œç”¨', 'æ„ä¹‰', 'ä»·å€¼', 'æ°´å¹³', 'èƒ½åŠ›', 'æ¡ä»¶', 'ç¯å¢ƒ'
        }
        return keyword if keyword not in stop_words and len(keyword) >= self.min_length else None

    def get_site_articles(self, site_hostname, days):
        """è·å–æŒ‡å®šç«™ç‚¹çš„æ–‡ç« """
        try:
            # è·å–ç«™ç‚¹
            if site_hostname == 'localhost':
                site = Site.objects.get(is_default_site=True)
            else:
                site = Site.objects.get(hostname=site_hostname)
            
            # è®¡ç®—æ—¶é—´èŒƒå›´
            since_date = timezone.now() - timedelta(days=days)
            
            # æŸ¥è¯¢æ–‡ç« 
            articles = ArticlePage.objects.live().filter(
                first_published_at__gte=since_date
            ).filter(
                # é€šè¿‡é¡µé¢æ ‘å…³ç³»è·å–ç«™ç‚¹ä¸‹çš„æ–‡ç« 
                depth__gte=3  # é€šå¸¸æ–‡ç« åœ¨ç«™ç‚¹æ ¹é¡µé¢çš„å­é¡µé¢ä¸­
            ).order_by('-first_published_at')[:1000]  # é™åˆ¶æŸ¥è¯¢æ•°é‡
            
            self.stdout.write(f'æ‰¾åˆ° {articles.count()} ç¯‡æ–‡ç« ')
            return articles
            
        except Site.DoesNotExist:
            self.stdout.write(
                self.style.ERROR(f'ç«™ç‚¹ {site_hostname} ä¸å­˜åœ¨')
            )
            return ArticlePage.objects.none()

    def analyze_articles(self, articles):
        """åˆ†ææ–‡ç« æå–å…³é”®è¯"""
        keyword_weights = Counter()
        keyword_articles = {}  # è®°å½•å…³é”®è¯å‡ºç°çš„æ–‡ç« æ•°
        
        total_articles = len(articles)
        self.stdout.write(f'å¼€å§‹åˆ†æ {total_articles} ç¯‡æ–‡ç« ...')
        
        for i, article in enumerate(articles):
            if i % 100 == 0:
                self.stdout.write(f'å¤„ç†è¿›åº¦: {i}/{total_articles}')
            
            # åˆå¹¶æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œåˆ†æ
            text_content = []
            if hasattr(article, 'title') and article.title:
                text_content.append(article.title)
            if hasattr(article, 'search_description') and article.search_description:
                text_content.append(article.search_description)
            if hasattr(article, 'introduction') and article.introduction:
                text_content.append(article.introduction)
            
            combined_text = ' '.join(text_content)
            
            # æå–å…³é”®è¯
            keywords = self.extract_keywords_from_text(combined_text, topK=15)
            
            for keyword, weight in keywords:
                cleaned_keyword = self.clean_keyword(keyword)
                if cleaned_keyword:
                    # æ ‡é¢˜ä¸­çš„å…³é”®è¯æƒé‡æ›´é«˜
                    title_boost = 2.0 if hasattr(article, 'title') and cleaned_keyword in article.title else 1.0
                    
                    # æœ€è¿‘æ–‡ç« æƒé‡æ›´é«˜
                    if hasattr(article, 'first_published_at') and article.first_published_at:
                        days_ago = (timezone.now() - article.first_published_at).days
                        time_boost = max(0.5, 1.0 - (days_ago / 30.0))  # 30å¤©å†…çº¿æ€§è¡°å‡
                    else:
                        time_boost = 0.5
                    
                    final_weight = weight * title_boost * time_boost
                    keyword_weights[cleaned_keyword] += final_weight
                    
                    # è®°å½•æ–‡ç« æ•°
                    if cleaned_keyword not in keyword_articles:
                        keyword_articles[cleaned_keyword] = set()
                    keyword_articles[cleaned_keyword].add(article.id)
        
        return keyword_weights, keyword_articles

    def generate_trending_data(self, keyword_weights, keyword_articles, limit):
        """ç”Ÿæˆçƒ­æœæ¦œæ•°æ®"""
        # æŒ‰æƒé‡æ’åº
        top_keywords = keyword_weights.most_common(limit)
        
        trending_data = []
        for rank, (keyword, weight) in enumerate(top_keywords, 1):
            # è®¡ç®—æœç´¢æ¬¡æ•°ï¼ˆåŸºäºæƒé‡å’Œæ–‡ç« æ•°ï¼‰
            article_count = len(keyword_articles.get(keyword, set()))
            search_count = max(10, int(weight * article_count * random.uniform(0.8, 1.2)))
            
            # éšæœºç”Ÿæˆè¶‹åŠ¿å˜åŒ–
            changes = ['hot', 'up', 'stable', 'down', 'new']
            weights = [0.1, 0.3, 0.4, 0.15, 0.05]  # æƒé‡åˆ†å¸ƒ
            change = random.choices(changes, weights=weights)[0]
            
            trending_data.append({
                'text': keyword,
                'rank': rank,
                'change': change,
                'score': int(weight * 100),
                'count': search_count,
                'article_count': article_count
            })
        
        return trending_data

    def insert_to_clickhouse(self, trending_data, site):
        """å°†çƒ­æœæ•°æ®æ’å…¥ClickHouseä½œä¸ºæœç´¢äº‹ä»¶"""
        client = self.get_clickhouse_client()
        if not client:
            return False
        
        try:
            # ç”Ÿæˆæœç´¢äº‹ä»¶æ•°æ®
            events = []
            base_time = timezone.now()
            
            for item in trending_data:
                # ä¸ºæ¯ä¸ªçƒ­æœè¯ç”Ÿæˆå¤šä¸ªæœç´¢äº‹ä»¶
                search_count = item['count']
                keyword = item['text']
                
                # åœ¨è¿‡å»å‡ å¤©å†…åˆ†å¸ƒæœç´¢äº‹ä»¶
                for i in range(min(search_count, 200)):  # é™åˆ¶å•ä¸ªè¯çš„äº‹ä»¶æ•°é‡
                    # éšæœºåˆ†å¸ƒåœ¨è¿‡å»7å¤©å†…
                    random_hours = random.randint(0, 168)  # 7å¤© * 24å°æ—¶
                    event_time = base_time - timedelta(hours=random_hours)
                    
                    events.append((
                        event_time,
                        f'user_{random.randint(1000, 9999)}',  # éšæœºç”¨æˆ·ID
                        f'device_{random.randint(1000, 9999)}',  # éšæœºè®¾å¤‡ID
                        f'session_{random.randint(1000, 9999)}',  # éšæœºä¼šè¯ID
                        'search',
                        f'article_{random.randint(1, 1000)}',  # éšæœºæ–‡ç« ID
                        'recommend',
                        site,
                        0,  # dwell_ms
                        keyword  # search_query
                    ))
            
            # æ‰¹é‡æ’å…¥
            if events:
                client.execute(
                    """
                    INSERT INTO events (ts,user_id,device_id,session_id,event,article_id,channel,site,dwell_ms,search_query)
                    VALUES
                    """,
                    events
                )
                
                self.stdout.write(
                    self.style.SUCCESS(f'æˆåŠŸæ’å…¥ {len(events)} æ¡æœç´¢äº‹ä»¶')
                )
                return True
                
        except Exception as e:
            self.stdout.write(
                self.style.ERROR(f'æ’å…¥ClickHouseå¤±è´¥: {e}')
            )
            return False
        
        return False

    def handle(self, *args, **options):
        self.min_length = options['min_length']
        site = options['site']
        days = options['days']
        limit = options['limit']
        dry_run = options['dry_run']
        
        self.stdout.write(
            self.style.SUCCESS(f'å¼€å§‹ç”Ÿæˆçƒ­æœæ¦œæ•°æ®...')
        )
        self.stdout.write(f'ç«™ç‚¹: {site}')
        self.stdout.write(f'åˆ†æå¤©æ•°: {days}')
        self.stdout.write(f'ç”Ÿæˆæ•°é‡: {limit}')
        self.stdout.write(f'æœ€å°é•¿åº¦: {self.min_length}')
        
        # 1. è·å–æ–‡ç« 
        articles = self.get_site_articles(site, days)
        if not articles:
            self.stdout.write(
                self.style.ERROR('æ²¡æœ‰æ‰¾åˆ°æ–‡ç« æ•°æ®')
            )
            return
        
        # 2. åˆ†æå…³é”®è¯
        keyword_weights, keyword_articles = self.analyze_articles(articles)
        
        if not keyword_weights:
            self.stdout.write(
                self.style.ERROR('æ²¡æœ‰æå–åˆ°æœ‰æ•ˆå…³é”®è¯')
            )
            return
        
        # 3. ç”Ÿæˆçƒ­æœæ•°æ®
        trending_data = self.generate_trending_data(keyword_weights, keyword_articles, limit)
        
        # 4. æ˜¾ç¤ºç»“æœ
        self.stdout.write('\n' + '='*60)
        self.stdout.write(self.style.SUCCESS('ç”Ÿæˆçš„çƒ­æœæ¦œæ•°æ®:'))
        self.stdout.write('='*60)
        
        for item in trending_data[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
            self.stdout.write(
                f"{item['rank']:2d}. {item['text']:10s} "
                f"(åˆ†æ•°: {item['score']:4d}, æœç´¢: {item['count']:3d}, "
                f"æ–‡ç« : {item['article_count']:2d}, è¶‹åŠ¿: {item['change']})"
            )
        
        if len(trending_data) > 20:
            self.stdout.write(f'... è¿˜æœ‰ {len(trending_data) - 20} ä¸ªçƒ­æœè¯')
        
        # 5. å†™å…¥ClickHouse
        if not dry_run:
            self.stdout.write('\nå¼€å§‹å†™å…¥ClickHouse...')
            success = self.insert_to_clickhouse(trending_data, site)
            if success:
                self.stdout.write(
                    self.style.SUCCESS('âœ… çƒ­æœæ¦œåˆå§‹åŒ–å®Œæˆï¼')
                )
            else:
                self.stdout.write(
                    self.style.ERROR('âŒ å†™å…¥ClickHouseå¤±è´¥')
                )
        else:
            self.stdout.write(
                self.style.WARNING('\nğŸ” è¿™æ˜¯é¢„è§ˆæ¨¡å¼ï¼Œæ²¡æœ‰å†™å…¥æ•°æ®åº“')
            )
            self.stdout.write('å¦‚éœ€å†™å…¥ï¼Œè¯·ç§»é™¤ --dry-run å‚æ•°')
