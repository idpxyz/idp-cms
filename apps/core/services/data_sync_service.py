"""
æ•°æ®åŒæ­¥æœåŠ¡ - ç»Ÿä¸€ç®¡ç†PostgreSQLã€OpenSearchã€ClickHouseä¹‹é—´çš„æ•°æ®åŒæ­¥

è´Ÿè´£ï¼š
1. PostgreSQL â†’ OpenSearch å†…å®¹åŒæ­¥
2. ç”¨æˆ·è¡Œä¸º â†’ ClickHouse åŸ‹ç‚¹
3. ç»Ÿè®¡æ•°æ® â†’ PostgreSQL æƒé‡æ›´æ–°
4. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å’Œä¿®å¤
"""

import logging
from typing import Dict, List, Optional, Any
from django.conf import settings
from django.db import transaction
from django.utils import timezone
from apps.news.models.article import ArticlePage
from apps.searchapp.indexer import ArticleIndexer
from apps.searchapp.client import get_client as get_opensearch_client
from clickhouse_driver import Client as ClickHouseClient

logger = logging.getLogger(__name__)


class DataSyncService:
    """æ•°æ®åŒæ­¥æœåŠ¡"""
    
    def __init__(self):
        self.opensearch_client = None
        self.clickhouse_client = None
        self.indexer = ArticleIndexer()
    
    def get_opensearch_client(self):
        """è·å–OpenSearchå®¢æˆ·ç«¯"""
        if not self.opensearch_client:
            try:
                self.opensearch_client = get_opensearch_client()
            except Exception as e:
                logger.error(f"OpenSearchè¿æ¥å¤±è´¥: {e}")
                return None
        return self.opensearch_client
    
    def get_clickhouse_client(self):
        """è·å–ClickHouseå®¢æˆ·ç«¯"""
        if not self.clickhouse_client:
            try:
                self.clickhouse_client = ClickHouseClient.from_url(settings.CLICKHOUSE_URL)
            except Exception as e:
                logger.error(f"ClickHouseè¿æ¥å¤±è´¥: {e}")
                return None
        return self.clickhouse_client
    
    def sync_article_to_opensearch(self, article: ArticlePage, force_update: bool = False) -> bool:
        """
        åŒæ­¥æ–‡ç« åˆ°OpenSearch
        
        Args:
            article: ArticlePageå®ä¾‹
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
        
        Returns:
            bool: åŒæ­¥æ˜¯å¦æˆåŠŸ
        """
        try:
            client = self.get_opensearch_client()
            if not client:
                return False
            
            # ç”ŸæˆOpenSearchæ–‡æ¡£
            doc = self.indexer.to_doc(article)
            
            # ä¼˜åŒ–ï¼šæ·»åŠ bodyå†…å®¹ï¼ˆæå–HTMLçº¯æ–‡æœ¬ï¼‰
            if hasattr(article, 'body') and article.body:
                import re
                # ç®€å•æå–HTMLçº¯æ–‡æœ¬
                body_text = re.sub(r'<[^>]+>', '', str(article.body))
                doc['body'] = body_text[:1000]  # é™åˆ¶é•¿åº¦
            
            # æ·»åŠ ç»Ÿè®¡æ•°æ®
            doc.update({
                'view_count': article.view_count or 0,
                'comment_count': article.comment_count or 0,
                'like_count': article.like_count or 0,
                'favorite_count': article.favorite_count or 0,
                'weight': article.weight or 0,
                'is_featured': article.is_featured,
                'is_hero': article.is_hero,  # ğŸ¯ æ·»åŠ ç¼ºå¤±çš„is_heroå­—æ®µ
                'updated_at': timezone.now().isoformat()
            })
            
            # ç¡®å®šç´¢å¼•å
            from apps.searchapp.utils import get_site_from_article
            from apps.searchapp.simple_index import get_index_name  # ğŸ¯ ä½¿ç”¨æ ‡å‡†å‡½æ•°
            site_identifier = get_site_from_article(article)
            index_name = get_index_name(site_identifier)
            
            # ç´¢å¼•æ–‡æ¡£
            response = client.index(
                index=index_name,
                id=str(article.id),
                body=doc,
                refresh=True if force_update else 'wait_for'
            )
            
            logger.info(f"æ–‡ç«  {article.id} åŒæ­¥åˆ°OpenSearchæˆåŠŸ: {response.get('result')}")
            return True
            
        except Exception as e:
            logger.error(f"æ–‡ç«  {article.id} åŒæ­¥åˆ°OpenSearchå¤±è´¥: {e}", exc_info=True)
            return False
    
    def track_user_behavior(self, event_data: Dict[str, Any]) -> bool:
        """
        è®°å½•ç”¨æˆ·è¡Œä¸ºåˆ°ClickHouse
        
        Args:
            event_data: äº‹ä»¶æ•°æ®å­—å…¸
        
        Returns:
            bool: è®°å½•æ˜¯å¦æˆåŠŸ
        """
        try:
            client = self.get_clickhouse_client()
            if not client:
                return False
            
            # æ ‡å‡†åŒ–äº‹ä»¶æ•°æ®
            normalized_data = {
                'ts': event_data.get('ts', timezone.now()),
                'user_id': event_data.get('user_id', ''),
                'device_id': event_data.get('device_id', ''),
                'session_id': event_data.get('session_id', ''),
                'event': event_data.get('event', ''),
                'article_id': str(event_data.get('article_id', '')),
                'channel': event_data.get('channel', ''),
                'site': event_data.get('site', ''),
                'dwell_ms': int(event_data.get('dwell_ms', 0)),
                'search_query': event_data.get('search_query', ''),
                'user_agent': event_data.get('user_agent', ''),
                'ip_address': event_data.get('ip_address', ''),
                'referrer': event_data.get('referrer', ''),
            }
            
            # æ’å…¥ClickHouse
            client.execute(
                """
                INSERT INTO events (
                    ts, user_id, device_id, session_id, event,
                    article_id, channel, site, dwell_ms, search_query,
                    user_agent, ip_address, referrer
                ) VALUES
                """,
                [normalized_data]
            )
            
            logger.debug(f"ç”¨æˆ·è¡Œä¸ºäº‹ä»¶è®°å½•æˆåŠŸ: {event_data.get('event')} - {event_data.get('article_id')}")
            return True
            
        except Exception as e:
            logger.error(f"è®°å½•ç”¨æˆ·è¡Œä¸ºå¤±è´¥: {e}", exc_info=True)
            return False
    
    def sync_article_weight(self, article: ArticlePage) -> bool:
        """
        åŒæ­¥æ–‡ç« æƒé‡ï¼ˆåŸºäºç»Ÿè®¡æ•°æ®é‡æ–°è®¡ç®—ï¼‰
        
        Args:
            article: ArticlePageå®ä¾‹
        
        Returns:
            bool: åŒæ­¥æ˜¯å¦æˆåŠŸ
        """
        try:
            with transaction.atomic():
                old_weight = article.weight
                
                # é‡æ–°è®¡ç®—åŠ¨æ€æƒé‡
                article.update_dynamic_weight()
                new_weight = article.weight
                
                # ä¿å­˜æƒé‡æ›´æ–°
                article.save(update_fields=['weight'])
                
                # åŒæ­¥åˆ°OpenSearch
                self.sync_article_to_opensearch(article, force_update=True)
                
                if old_weight != new_weight:
                    logger.info(f"æ–‡ç«  {article.id} æƒé‡æ›´æ–°: {old_weight} â†’ {new_weight}")
                
                return True
                
        except Exception as e:
            logger.error(f"æ–‡ç«  {article.id} æƒé‡åŒæ­¥å¤±è´¥: {e}", exc_info=True)
            return False
    
    def batch_sync_articles(self, article_ids: List[int] = None, limit: int = 100) -> Dict[str, int]:
        """
        æ‰¹é‡åŒæ­¥æ–‡ç« 
        
        Args:
            article_ids: æŒ‡å®šæ–‡ç« IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŒæ­¥æ‰€æœ‰
            limit: æ‰¹é‡å¤„ç†æ•°é‡é™åˆ¶
        
        Returns:
            Dict: åŒæ­¥ç»Ÿè®¡ç»“æœ
        """
        stats = {
            'total': 0,
            'opensearch_success': 0,
            'opensearch_failed': 0,
            'weight_updated': 0,
            'weight_failed': 0
        }
        
        try:
            # æ„å»ºæŸ¥è¯¢
            if article_ids:
                articles = ArticlePage.objects.filter(id__in=article_ids[:limit])
            else:
                articles = ArticlePage.objects.all()[:limit]
            
            stats['total'] = articles.count()
            
            for article in articles:
                # ğŸ¯ ç¡®ä¿è·å–æœ€æ–°çš„æ–‡ç« çŠ¶æ€ï¼Œç‰¹åˆ«æ˜¯is_heroå­—æ®µ
                article.refresh_from_db()
                
                # åŒæ­¥åˆ°OpenSearch
                if self.sync_article_to_opensearch(article):
                    stats['opensearch_success'] += 1
                else:
                    stats['opensearch_failed'] += 1
                
                # åŒæ­¥æƒé‡
                if self.sync_article_weight(article):
                    stats['weight_updated'] += 1
                else:
                    stats['weight_failed'] += 1
            
            logger.info(f"æ‰¹é‡åŒæ­¥å®Œæˆ: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åŒæ­¥å¤±è´¥: {e}", exc_info=True)
            return stats
    
    def check_data_consistency(self, site: str = None) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        
        Args:
            site: ç«™ç‚¹æ ‡è¯†ç¬¦
        
        Returns:
            Dict: ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
        """
        result = {
            'timestamp': timezone.now().isoformat(),
            'site': site,
            'postgresql': {},
            'opensearch': {},
            'clickhouse': {},
            'consistency': {},
            'recommendations': []
        }
        
        try:
            # PostgreSQLç»Ÿè®¡
            if site:
                from wagtail.models import Site
                try:
                    site_obj = Site.objects.get(hostname=site)
                    pg_articles = ArticlePage.objects.live().descendant_of(site_obj.root_page)
                except Site.DoesNotExist:
                    pg_articles = ArticlePage.objects.all()
            else:
                pg_articles = ArticlePage.objects.all()
            
            result['postgresql'] = {
                'total_articles': pg_articles.count(),
                'featured_articles': pg_articles.filter(is_featured=True).count(),
                'articles_with_weight': pg_articles.filter(weight__gt=0).count(),
                'articles_with_stats': pg_articles.filter(view_count__gt=0).count()
            }
            
            # OpenSearchç»Ÿè®¡
            os_client = self.get_opensearch_client()
            if os_client and site:
                try:
                    index_name = f"articles_{site.replace('.', '_')}"
                    os_response = os_client.count(index=index_name)
                    result['opensearch'] = {
                        'total_articles': os_response.get('count', 0),
                        'index_name': index_name
                    }
                except:
                    result['opensearch'] = {'error': 'Index not found or query failed'}
            
            # ClickHouseç»Ÿè®¡
            ch_client = self.get_clickhouse_client()
            if ch_client:
                try:
                    # ç»Ÿè®¡äº‹ä»¶æ€»æ•°
                    total_events = ch_client.execute("SELECT count() FROM events")[0][0]
                    
                    # ç»Ÿè®¡æœ€è¿‘24å°æ—¶äº‹ä»¶
                    recent_events = ch_client.execute("""
                        SELECT count() FROM events 
                        WHERE ts >= subtractHours(now(), 24)
                    """)[0][0]
                    
                    result['clickhouse'] = {
                        'total_events': total_events,
                        'recent_24h_events': recent_events
                    }
                except:
                    result['clickhouse'] = {'error': 'Query failed'}
            
            # ä¸€è‡´æ€§åˆ†æ
            pg_count = result['postgresql']['total_articles']
            os_count = result['opensearch'].get('total_articles', 0)
            
            if os_count > 0:
                consistency_ratio = min(pg_count, os_count) / max(pg_count, os_count)
                result['consistency'] = {
                    'pg_os_ratio': consistency_ratio,
                    'is_consistent': consistency_ratio >= 0.95
                }
                
                if consistency_ratio < 0.95:
                    result['recommendations'].append('PostgreSQLå’ŒOpenSearchæ•°æ®ä¸ä¸€è‡´ï¼Œå»ºè®®é‡æ–°ç´¢å¼•')
            
            # æƒé‡ä¸€è‡´æ€§æ£€æŸ¥
            weight_inconsistent = pg_articles.exclude(weight=0).count()
            if weight_inconsistent < pg_count * 0.1:  # å°‘äº10%æœ‰æƒé‡
                result['recommendations'].append('å¤§éƒ¨åˆ†æ–‡ç« æƒé‡ä¸º0ï¼Œå»ºè®®è¿è¡Œæƒé‡åŒæ­¥')
            
            return result
            
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}", exc_info=True)
            return result


# å…¨å±€å®ä¾‹
data_sync_service = DataSyncService()


# ä¾¿æ·å‡½æ•°
def sync_article(article_id: int) -> bool:
    """åŒæ­¥å•ç¯‡æ–‡ç« """
    try:
        article = ArticlePage.objects.get(id=article_id)
        return data_sync_service.sync_article_to_opensearch(article)
    except ArticlePage.DoesNotExist:
        return False


def track_behavior(event_type: str, article_id: int, **kwargs) -> bool:
    """è®°å½•ç”¨æˆ·è¡Œä¸º"""
    event_data = {
        'event': event_type,
        'article_id': article_id,
        'ts': timezone.now(),
        **kwargs
    }
    return data_sync_service.track_user_behavior(event_data)


def check_consistency(site: str = None) -> Dict[str, Any]:
    """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
    return data_sync_service.check_data_consistency(site)
