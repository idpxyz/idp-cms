# å¤šç«™ç‚¹ API æ–‡æ¡£

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº†å¤šç«™ç‚¹æ¶æ„ä¸‹çš„ API ä½¿ç”¨æ–¹æ³•ã€ç«¯ç‚¹è¯´æ˜å’Œé›†æˆæŒ‡å—ã€‚å¤šç«™ç‚¹ API æ”¯æŒé€šè¿‡ä¸åŒçš„æ–¹å¼è¯†åˆ«å’Œè®¿é—®å„ä¸ªç«™ç‚¹çš„æ•°æ®ã€‚

## API ç«¯ç‚¹

### Feed API

è·å–ç«™ç‚¹ç‰¹å®šçš„å†…å®¹æ¨èåˆ—è¡¨ã€‚

#### ç«¯ç‚¹

```
GET /api/feed
```

#### ç«™ç‚¹è¯†åˆ«æ–¹å¼

##### 1. Host Headerï¼ˆæ¨èï¼‰

```bash
curl -X GET "http://localhost:8000/api/feed" \
     -H "Host: site-a.local" \
     -H "Accept: application/json"
```

##### 2. URL å‚æ•°

```bash
curl -X GET "http://localhost:8000/api/feed?site=site-a.local" \
     -H "Accept: application/json"
```

##### 3. ä¼˜å…ˆçº§è§„åˆ™

1. **URL å‚æ•° `site`** - æœ€é«˜ä¼˜å…ˆçº§
2. **Host Header** - ä¸­ç­‰ä¼˜å…ˆçº§  
3. **é»˜è®¤é…ç½®** - æœ€ä½ä¼˜å…ˆçº§ (`localhost`)

#### è¯·æ±‚å‚æ•°

| å‚æ•° | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `site` | string | å¦ | - | æŒ‡å®šç«™ç‚¹æ ‡è¯†ç¬¦ |
| `size` | integer | å¦ | 20 | è¿”å›æ–‡ç« æ•°é‡ (1-100) |
| `cursor` | string | å¦ | - | åˆ†é¡µæ¸¸æ ‡ |
| `sort` | string | å¦ | `final_score` | æ’åºæ–¹å¼: `final_score`, `publish_time`, `ctr_1h` |
| `hours` | integer | å¦ | 72 | æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰ |

#### å“åº”æ ¼å¼

```json
{
  "items": [
    {
      "article_id": "2001",
      "site": "site-a.local",
      "title": "Site A News: AIæŠ€æœ¯çªç ´",
      "body": "Site Aç‹¬å®¶æŠ¥é“ï¼šäººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—é‡å¤§çªç ´...",
      "author": "Site Aç§‘æŠ€è®°è€…",
      "publish_time": "2025-08-25T14:30:00.000000",
      "topic": "ai",
      "region": "china", 
      "lang": "zh",
      "tenant": "site-a.local",
      "channel": "tech-news",
      "tags": ["AI", "æŠ€æœ¯çªç ´", "å›¾åƒè¯†åˆ«"],
      "ctr_1h": 5.2,
      "pop_1h": 85.0,
      "ctr_24h": 12.5,
      "pop_24h": 234.0,
      "quality_score": 8.5,
      "has_video": false,
      "final_score": 14.9159698
    }
  ],
  "next_cursor": "eyJzZWVuIjogWyIyMDAxIl0sICJ0cyI6IDE3NTYxMDM0MDB9",
  "debug": {
    "hours": 72,
    "template": "recommend_default",
    "sort_by": "final_score",
    "site": "site-a.local",
    "host": "site-a.local"
  }
}
```

#### å­—æ®µè¯´æ˜

##### æ ¸å¿ƒå­—æ®µ

- `article_id`: æ–‡ç« å”¯ä¸€æ ‡è¯†ç¬¦
- `site`: æ‰€å±ç«™ç‚¹
- `title`: æ–‡ç« æ ‡é¢˜
- `body`: æ–‡ç« æ­£æ–‡æ‘˜è¦
- `author`: ä½œè€…
- `publish_time`: å‘å¸ƒæ—¶é—´ (ISO 8601 æ ¼å¼)

##### åˆ†ç±»å­—æ®µ

- `topic`: ä¸»é¢˜åˆ†ç±» (`ai`, `finance`, `sports`, `technology`, `general`)
- `region`: åœ°ç†åŒºåŸŸ (`china`, `global`, `asia`, `europe`, `america`)
- `lang`: è¯­è¨€ä»£ç  (`zh`, `en`, `ja`, `ko`)
- `channel`: é¢‘é“åˆ†ç±» (`tech-news`, `finance`, `sports`, `headlines`)
- `tags`: æ ‡ç­¾æ•°ç»„

##### æŒ‡æ ‡å­—æ®µ

- `ctr_1h`: 1å°æ—¶ç‚¹å‡»ç‡ (%)
- `pop_1h`: 1å°æ—¶ç‚¹å‡»æ•°
- `ctr_24h`: 24å°æ—¶ç‚¹å‡»ç‡ (%)
- `pop_24h`: 24å°æ—¶ç‚¹å‡»æ•°
- `quality_score`: å†…å®¹è´¨é‡è¯„åˆ† (0-10)
- `final_score`: æœ€ç»ˆæ¨èåˆ†æ•°

##### å…ƒæ•°æ®

- `tenant`: ç§Ÿæˆ·æ ‡è¯†ï¼ˆé€šå¸¸ä¸ site ç›¸åŒï¼‰
- `has_video`: æ˜¯å¦åŒ…å«è§†é¢‘å†…å®¹

##### è°ƒè¯•ä¿¡æ¯ (debug)

- `hours`: æŸ¥è¯¢çš„æ—¶é—´çª—å£
- `template`: ä½¿ç”¨çš„æ¨èæ¨¡æ¿
- `sort_by`: å®é™…ä½¿ç”¨çš„æ’åºå­—æ®µ
- `site`: è¯†åˆ«å‡ºçš„ç«™ç‚¹
- `host`: è¯·æ±‚çš„ Host header

## ç«™ç‚¹ç‰¹å®šè¡Œä¸º

### ä¸åŒç«™ç‚¹çš„å†…å®¹å·®å¼‚

#### localhost (å¼€å‘ç¯å¢ƒ)
```bash
curl -H "Host: localhost" "http://localhost:8000/api/feed?size=2"
```
- è¿”å›é€šç”¨å¼€å‘æµ‹è¯•å†…å®¹
- æ”¯æŒæ‰€æœ‰åŠŸèƒ½çš„æµ‹è¯•
- æ•°æ®å¯èƒ½åŒ…å«å„ç§æµ‹è¯•åœºæ™¯

#### site-a.local (ç«™ç‚¹A)
```bash
curl -H "Host: site-a.local" "http://localhost:8000/api/feed?size=2"
```
- ä¸“æ³¨äºAIå’Œç§‘æŠ€å†…å®¹
- é«˜è´¨é‡çš„æŠ€æœ¯æ–‡ç« 
- é€‚åˆæŠ€æœ¯å¯¼å‘çš„ç”¨æˆ·ç¾¤ä½“

#### site-b.local (ç«™ç‚¹B)
```bash
curl -H "Host: site-b.local" "http://localhost:8000/api/feed?size=2"
```
- ä½“è‚²å’Œç§‘æŠ€å†…å®¹å¹¶é‡
- æ›´å¤šæ ·åŒ–çš„ä¸»é¢˜åˆ†å¸ƒ
- é€‚åˆç»¼åˆæ€§å†…å®¹æ¶ˆè´¹

#### portal.local (é—¨æˆ·ç«™ç‚¹)
```bash
curl -H "Host: portal.local" "http://localhost:8000/api/feed?size=2"
```
- èšåˆæ€§å†…å®¹ï¼Œæ±‡æ€»å„ç«™ç‚¹ç²¾å
- é«˜è´¨é‡è¯„åˆ†çš„æ–‡ç« ä¼˜å…ˆ
- é€‚åˆä½œä¸ºç»Ÿä¸€å…¥å£

## é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯å“åº”

#### 1. ç´¢å¼•ä¸å­˜åœ¨

```json
{
  "error": "NotFoundError",
  "message": "no such index [news_unknown_site_articles]",
  "status": 404
}
```

**è§£å†³æ–¹æ¡ˆ**: æ£€æŸ¥ç«™ç‚¹é…ç½®æˆ–åˆ›å»ºå¯¹åº”ç´¢å¼•

#### 2. å‚æ•°é”™è¯¯

```json
{
  "error": "ValidationError",
  "message": "size parameter must be between 1 and 100",
  "status": 400
}
```

#### 3. æœåŠ¡ä¸å¯ç”¨

```json
{
  "error": "ServiceUnavailable", 
  "message": "OpenSearch cluster is unavailable",
  "status": 503
}
```

## å®¢æˆ·ç«¯é›†æˆ

### JavaScript/TypeScript

```typescript
// TypeScript å®¢æˆ·ç«¯ç¤ºä¾‹
interface FeedItem {
  article_id: string;
  site: string;
  title: string;
  body: string;
  author: string;
  publish_time: string;
  topic: string;
  region: string;
  lang: string;
  ctr_1h: number;
  quality_score: number;
  final_score: number;
  tags: string[];
}

interface FeedResponse {
  items: FeedItem[];
  next_cursor?: string;
  debug: {
    site: string;
    host: string;
    hours: number;
    template: string;
    sort_by: string;
  };
}

class MultiSiteFeedClient {
  private baseUrl: string;
  
  constructor(baseUrl: string = 'http://localhost:8000') {
    this.baseUrl = baseUrl;
  }
  
  async getFeed(options: {
    site?: string;
    size?: number;
    cursor?: string;
    sort?: string;
    hours?: number;
  } = {}): Promise<FeedResponse> {
    const url = new URL('/api/feed', this.baseUrl);
    
    // è®¾ç½®æŸ¥è¯¢å‚æ•°
    Object.entries(options).forEach(([key, value]) => {
      if (value !== undefined) {
        url.searchParams.set(key, value.toString());
      }
    });
    
    const headers: Record<string, string> = {
      'Accept': 'application/json',
    };
    
    // å¦‚æœæ²¡æœ‰æŒ‡å®šç«™ç‚¹å‚æ•°ï¼Œä½¿ç”¨å½“å‰åŸŸåä½œä¸º Host header
    if (!options.site && typeof window !== 'undefined') {
      headers['X-Forwarded-Host'] = window.location.hostname;
    }
    
    const response = await fetch(url.toString(), { headers });
    
    if (!response.ok) {
      throw new Error(`Feed API error: ${response.status} ${response.statusText}`);
    }
    
    return response.json();
  }
  
  // è·å–ç‰¹å®šç«™ç‚¹çš„å†…å®¹
  async getSiteFeed(site: string, size: number = 20): Promise<FeedResponse> {
    return this.getFeed({ site, size });
  }
  
  // åˆ†é¡µè·å–å†…å®¹
  async getNextPage(cursor: string, site?: string): Promise<FeedResponse> {
    return this.getFeed({ cursor, site });
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const client = new MultiSiteFeedClient();

// è·å–é»˜è®¤ç«™ç‚¹å†…å®¹
const defaultFeed = await client.getFeed({ size: 10 });

// è·å–ç‰¹å®šç«™ç‚¹å†…å®¹
const siteAFeed = await client.getSiteFeed('site-a.local', 15);

// åˆ†é¡µè·å–
const nextPage = await client.getNextPage(defaultFeed.next_cursor);
```

### Python

```python
import requests
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

@dataclass
class FeedItem:
    article_id: str
    site: str
    title: str
    body: str
    author: str
    publish_time: str
    topic: str
    region: str
    lang: str
    ctr_1h: float
    quality_score: float
    final_score: float
    tags: List[str]

@dataclass 
class FeedResponse:
    items: List[FeedItem]
    next_cursor: Optional[str]
    debug: Dict[str, Any]

class MultiSiteFeedClient:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'Accept': 'application/json',
            'User-Agent': 'MultiSiteFeedClient/1.0'
        })
    
    def get_feed(self, 
                 site: Optional[str] = None,
                 size: int = 20,
                 cursor: Optional[str] = None,
                 sort: str = "final_score",
                 hours: int = 72) -> FeedResponse:
        """è·å–ç«™ç‚¹å†…å®¹æ¨è"""
        
        url = f"{self.base_url}/api/feed"
        params = {
            'size': size,
            'sort': sort,
            'hours': hours
        }
        
        if site:
            params['site'] = site
        if cursor:
            params['cursor'] = cursor
            
        # ç§»é™¤ None å€¼
        params = {k: v for k, v in params.items() if v is not None}
        
        headers = {}
        if site and not params.get('site'):
            headers['Host'] = site
            
        response = self.session.get(url, params=params, headers=headers)
        response.raise_for_status()
        
        data = response.json()
        
        # è½¬æ¢ä¸ºæ•°æ®ç±»
        items = [FeedItem(**item) for item in data['items']]
        
        return FeedResponse(
            items=items,
            next_cursor=data.get('next_cursor'),
            debug=data['debug']
        )
    
    def get_site_feed(self, site: str, size: int = 20) -> FeedResponse:
        """è·å–ç‰¹å®šç«™ç‚¹çš„å†…å®¹"""
        return self.get_feed(site=site, size=size)
    
    def get_all_sites_summary(self, size: int = 5) -> Dict[str, FeedResponse]:
        """è·å–æ‰€æœ‰ç«™ç‚¹çš„å†…å®¹æ‘˜è¦"""
        sites = ['localhost', 'site-a.local', 'site-b.local', 'portal.local']
        results = {}
        
        for site in sites:
            try:
                results[site] = self.get_site_feed(site, size)
            except Exception as e:
                print(f"Failed to get feed for {site}: {e}")
                
        return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    client = MultiSiteFeedClient()
    
    # è·å–ç‰¹å®šç«™ç‚¹å†…å®¹
    feed = client.get_site_feed('site-a.local', size=10)
    print(f"ç«™ç‚¹: {feed.debug['site']}")
    print(f"æ–‡ç« æ•°é‡: {len(feed.items)}")
    
    for item in feed.items:
        print(f"- {item.title} (è¯„åˆ†: {item.quality_score})")
    
    # è·å–æ‰€æœ‰ç«™ç‚¹æ‘˜è¦
    all_feeds = client.get_all_sites_summary(size=3)
    for site, feed in all_feeds.items():
        print(f"\n=== {site} ===")
        for item in feed.items:
            print(f"- {item.title}")
```

### cURL è„šæœ¬

```bash
#!/bin/bash
# multi_site_test.sh - å¤šç«™ç‚¹APIæµ‹è¯•è„šæœ¬

BASE_URL="http://localhost:8000"
SITES=("localhost" "site-a.local" "site-b.local" "portal.local")

echo "=== å¤šç«™ç‚¹ API æµ‹è¯• ==="

for site in "${SITES[@]}"; do
    echo ""
    echo "ğŸŒ æµ‹è¯•ç«™ç‚¹: $site"
    echo "----------------------------------------"
    
    # æµ‹è¯• Host header æ–¹å¼
    echo "ğŸ“¡ Host Header æ–¹å¼:"
    curl -s -X GET "$BASE_URL/api/feed?size=2" \
         -H "Host: $site" \
         -H "Accept: application/json" | \
    python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    print(f'  âœ… ç«™ç‚¹: {data[\"debug\"][\"site\"]}')
    print(f'  ğŸ“Š æ–‡ç« æ•°é‡: {len(data[\"items\"])}')
    for i, item in enumerate(data['items'][:2], 1):
        print(f'  {i}. {item[\"title\"]} (è¯„åˆ†: {item[\"quality_score\"]})')
except Exception as e:
    print(f'  âŒ é”™è¯¯: {e}')
"
    
    echo ""
    echo "ğŸ”— URL å‚æ•°æ–¹å¼:"
    curl -s -X GET "$BASE_URL/api/feed?site=$site&size=1" \
         -H "Accept: application/json" | \
    python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    print(f'  âœ… ç«™ç‚¹: {data[\"debug\"][\"site\"]}')
    print(f'  ğŸ“Š æ–‡ç« æ•°é‡: {len(data[\"items\"])}')
    if data['items']:
        item = data['items'][0]
        print(f'  ğŸ“° {item[\"title\"]}')
except Exception as e:
    print(f'  âŒ é”™è¯¯: {e}')
"
done

echo ""
echo "ğŸ¯ æµ‹è¯•å®Œæˆ!"
```

## æ€§èƒ½ä¼˜åŒ–

### 1. ç¼“å­˜ç­–ç•¥

```typescript
// å®¢æˆ·ç«¯ç¼“å­˜ç¤ºä¾‹
class CachedFeedClient {
  private cache = new Map<string, { data: FeedResponse; timestamp: number }>();
  private cacheTimeout = 5 * 60 * 1000; // 5åˆ†é’Ÿ
  
  async getFeed(options: any): Promise<FeedResponse> {
    const cacheKey = JSON.stringify(options);
    const cached = this.cache.get(cacheKey);
    
    if (cached && Date.now() - cached.timestamp < this.cacheTimeout) {
      return cached.data;
    }
    
    const data = await this.client.getFeed(options);
    this.cache.set(cacheKey, { data, timestamp: Date.now() });
    
    return data;
  }
}
```

### 2. æ‰¹é‡è¯·æ±‚

```python
async def get_multiple_sites_parallel(sites: List[str], size: int = 10):
    """å¹¶è¡Œè·å–å¤šä¸ªç«™ç‚¹çš„æ•°æ®"""
    import asyncio
    import aiohttp
    
    async def fetch_site(session, site):
        url = f"http://localhost:8000/api/feed?site={site}&size={size}"
        async with session.get(url) as response:
            return site, await response.json()
    
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_site(session, site) for site in sites]
        results = await asyncio.gather(*tasks)
        
    return dict(results)
```

### 3. è¯·æ±‚ä¼˜åŒ–

- **å‹ç¼©**: å¯ç”¨ gzip å‹ç¼©å‡å°‘ä¼ è¾“å¤§å°
- **Keep-Alive**: å¤ç”¨ HTTP è¿æ¥
- **åˆç†çš„ size**: é¿å…ä¸€æ¬¡è¯·æ±‚è¿‡å¤šæ•°æ®
- **æ¸¸æ ‡åˆ†é¡µ**: ä½¿ç”¨ cursor è€Œä¸æ˜¯ offset åˆ†é¡µ

## ç›‘æ§ä¸æ—¥å¿—

### API æŒ‡æ ‡ç›‘æ§

```python
# ç›‘æ§è„šæœ¬ç¤ºä¾‹
import time
import requests
from datetime import datetime

def monitor_api_health():
    sites = ['localhost', 'site-a.local', 'site-b.local', 'portal.local']
    
    for site in sites:
        try:
            start_time = time.time()
            response = requests.get(
                "http://localhost:8000/api/feed",
                headers={"Host": site},
                params={"size": 1},
                timeout=5
            )
            
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                print(f"{datetime.now()} âœ… {site}: {response_time:.3f}s, {len(data['items'])} items")
            else:
                print(f"{datetime.now()} âŒ {site}: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"{datetime.now()} ğŸ’¥ {site}: {e}")

if __name__ == "__main__":
    while True:
        monitor_api_health()
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
```

---

## æ€»ç»“

å¤šç«™ç‚¹ API æä¾›äº†å¼ºå¤§è€Œçµæ´»çš„å¤šç«™ç‚¹å†…å®¹è®¿é—®èƒ½åŠ›ï¼š

âœ… **å¤šç§è¯†åˆ«æ–¹å¼** - Host headerã€URL å‚æ•°ã€é»˜è®¤é…ç½®  
âœ… **å®Œæ•´çš„æ•°æ®éš”ç¦»** - æ¯ä¸ªç«™ç‚¹ç‹¬ç«‹çš„å†…å®¹å’ŒæŒ‡æ ‡  
âœ… **ä¸°å¯Œçš„å…ƒæ•°æ®** - æ”¯æŒåˆ†ç±»ã€æ ‡ç­¾ã€è´¨é‡è¯„åˆ†ç­‰  
âœ… **é«˜æ€§èƒ½è®¾è®¡** - æ”¯æŒåˆ†é¡µã€æ’åºã€æ—¶é—´çª—å£æŸ¥è¯¢  
âœ… **æ˜“äºé›†æˆ** - æä¾›å¤šè¯­è¨€å®¢æˆ·ç«¯ç¤ºä¾‹  

è¿™å¥— API ä¸ºæ„å»ºå¤šç«™ç‚¹åº”ç”¨æä¾›äº†å®Œæ•´çš„æ•°æ®è®¿é—®å±‚ï¼Œæ”¯æŒå„ç§å¤æ‚çš„ä¸šåŠ¡åœºæ™¯ã€‚
