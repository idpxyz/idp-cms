# å¤šç«™ç‚¹éƒ¨ç½²è¿ç»´æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£ä»‹ç»å¤šç«™ç‚¹æ¶æ„çš„éƒ¨ç½²æµç¨‹ã€è¿ç»´æœ€ä½³å®è·µã€ç›‘æ§ç­–ç•¥å’Œæ•…éšœå¤„ç†æ–¹æ¡ˆã€‚æ¶µç›–ä»å¼€å‘ç¯å¢ƒåˆ°ç”Ÿäº§ç¯å¢ƒçš„å®Œæ•´éƒ¨ç½²é“¾è·¯ã€‚

## ç›®å½•

- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [éƒ¨ç½²æ¶æ„](#éƒ¨ç½²æ¶æ„)
- [é…ç½®ç®¡ç†](#é…ç½®ç®¡ç†)
- [è‡ªåŠ¨åŒ–éƒ¨ç½²](#è‡ªåŠ¨åŒ–éƒ¨ç½²)
- [ç›‘æ§å‘Šè­¦](#ç›‘æ§å‘Šè­¦)
- [å¤‡ä»½æ¢å¤](#å¤‡ä»½æ¢å¤)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æ•…éšœå¤„ç†](#æ•…éšœå¤„ç†)
- [æ‰©å®¹æ–¹æ¡ˆ](#æ‰©å®¹æ–¹æ¡ˆ)

## ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚

#### æœ€å°é…ç½®
- **CPU**: 4 æ ¸å¿ƒ
- **å†…å­˜**: 8GB RAM
- **å­˜å‚¨**: 100GB SSD
- **ç½‘ç»œ**: 100Mbps

#### æ¨èé…ç½®
- **CPU**: 8 æ ¸å¿ƒ
- **å†…å­˜**: 16GB RAM  
- **å­˜å‚¨**: 500GB SSD
- **ç½‘ç»œ**: 1Gbps

#### ç”Ÿäº§é…ç½®
- **CPU**: 16+ æ ¸å¿ƒ
- **å†…å­˜**: 32GB+ RAM
- **å­˜å‚¨**: 1TB+ NVMe SSD
- **ç½‘ç»œ**: 10Gbps

### ä¾èµ–æœåŠ¡

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  # åº”ç”¨æœåŠ¡
  authoring:
    image: idp-cms/authoring:latest
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    environment:
      - DJANGO_SETTINGS_MODULE=authoring.settings.production
      - MULTI_SITE_ENABLED=true
      
  # å‰ç«¯æœåŠ¡
  portal:
    image: idp-cms/portal:latest
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1'
          memory: 2G
          
  # æ•°æ®åº“æœåŠ¡
  postgres:
    image: postgres:15
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    volumes:
      - postgres_data:/var/lib/postgresql/data
      
  # æœç´¢å¼•æ“
  opensearch:
    image: opensearchproject/opensearch:2.11.0
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
    environment:
      - cluster.name=multi-site-cluster
      - node.name=opensearch-node
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms4g -Xmx4g"
      
  # åˆ†ææ•°æ®åº“
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
          
  # ç¼“å­˜å’Œæ¶ˆæ¯é˜Ÿåˆ—
  redis:
    image: redis:7-alpine
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
          
  # è´Ÿè½½å‡è¡¡
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
```

## éƒ¨ç½²æ¶æ„

### å•èŠ‚ç‚¹éƒ¨ç½²

```mermaid
graph TB
    User[ç”¨æˆ·] --> LB[Nginx è´Ÿè½½å‡è¡¡]
    LB --> App1[Django App 1]
    LB --> App2[Django App 2]
    LB --> App3[Django App 3]
    
    App1 --> DB[(PostgreSQL)]
    App2 --> DB
    App3 --> DB
    
    App1 --> OS[(OpenSearch)]
    App2 --> OS
    App3 --> OS
    
    App1 --> CH[(ClickHouse)]
    App2 --> CH
    App3 --> CH
    
    App1 --> Redis[(Redis)]
    App2 --> Redis
    App3 --> Redis
```

### å¤šèŠ‚ç‚¹é›†ç¾¤éƒ¨ç½²

```mermaid
graph TB
    subgraph "è´Ÿè½½å‡è¡¡å±‚"
        LB1[Nginx 1]
        LB2[Nginx 2]
    end
    
    subgraph "åº”ç”¨å±‚"
        App1[Django Node 1]
        App2[Django Node 2]
        App3[Django Node 3]
    end
    
    subgraph "æ•°æ®å±‚"
        DB1[(PostgreSQL Master)]
        DB2[(PostgreSQL Replica)]
        OS1[(OpenSearch Node 1)]
        OS2[(OpenSearch Node 2)]
        OS3[(OpenSearch Node 3)]
        CH1[(ClickHouse Node 1)]
        CH2[(ClickHouse Node 2)]
    end
    
    LB1 --> App1
    LB1 --> App2
    LB2 --> App2
    LB2 --> App3
    
    App1 --> DB1
    App2 --> DB1
    App3 --> DB2
    
    App1 --> OS1
    App2 --> OS2
    App3 --> OS3
```

## é…ç½®ç®¡ç†

### ç¯å¢ƒå˜é‡é…ç½®

#### å¼€å‘ç¯å¢ƒ (.env.dev)
```bash
# åŸºç¡€é…ç½®
DJANGO_DEBUG=true
DJANGO_SECRET_KEY=dev-secret-key
DJANGO_ALLOWED_HOSTS=localhost,127.0.0.1,*.local

# å¤šç«™ç‚¹é…ç½®
MULTI_SITE_ENABLED=true
DEFAULT_SITE_IDENTIFIER=localhost
SITE_HOSTNAME=localhost

# æ•°æ®åº“é…ç½®
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=idp_cms_dev
POSTGRES_USER=dev_user
POSTGRES_PASSWORD=dev_password

# OpenSearch é…ç½®
OPENSEARCH_URL=http://localhost:9200
OPENSEARCH_USERNAME=admin
OPENSEARCH_PASSWORD=admin

# ClickHouse é…ç½®
CLICKHOUSE_URL=clickhouse://default:dev_password@localhost:9000/default

# Redis é…ç½®
REDIS_URL=redis://localhost:6379/1

# æ—¶åŒºé…ç½®
DJANGO_TIME_ZONE=Asia/Shanghai
```

#### æµ‹è¯•ç¯å¢ƒ (.env.test)
```bash
# åŸºç¡€é…ç½®
DJANGO_DEBUG=false
DJANGO_SECRET_KEY=test-secret-key-change-in-production
DJANGO_ALLOWED_HOSTS=test.example.com,test-a.example.com,test-b.example.com

# å¤šç«™ç‚¹é…ç½®
MULTI_SITE_ENABLED=true
DEFAULT_SITE_IDENTIFIER=test.example.com
SITE_HOSTNAME=test.example.com

# æ•°æ®åº“é…ç½®
POSTGRES_HOST=postgres-test
POSTGRES_PORT=5432
POSTGRES_DB=idp_cms_test
POSTGRES_USER=test_user
POSTGRES_PASSWORD=test_secure_password

# OpenSearch é…ç½®
OPENSEARCH_URL=https://opensearch-test:9200
OPENSEARCH_USERNAME=admin
OPENSEARCH_PASSWORD=test_opensearch_password

# ClickHouse é…ç½®
CLICKHOUSE_URL=clickhouse://default:test_password@clickhouse-test:9000/default

# Redis é…ç½®
REDIS_URL=redis://redis-test:6379/1

# SSLé…ç½®
DJANGO_SECURE_SSL_REDIRECT=true
DJANGO_SECURE_HSTS_SECONDS=31536000
```

#### ç”Ÿäº§ç¯å¢ƒ (.env.prod)
```bash
# åŸºç¡€é…ç½®
DJANGO_DEBUG=false
DJANGO_SECRET_KEY=${SECRET_KEY_FROM_VAULT}
DJANGO_ALLOWED_HOSTS=example.com,site-a.com,site-b.com,portal.com

# å¤šç«™ç‚¹é…ç½®
MULTI_SITE_ENABLED=true
DEFAULT_SITE_IDENTIFIER=example.com
SITE_HOSTNAME=example.com

# æ•°æ®åº“é…ç½® (ä½¿ç”¨å¯†é’¥ç®¡ç†)
POSTGRES_HOST=postgres-cluster.internal
POSTGRES_PORT=5432
POSTGRES_DB=idp_cms_prod
POSTGRES_USER=${DB_USER_FROM_VAULT}
POSTGRES_PASSWORD=${DB_PASSWORD_FROM_VAULT}

# OpenSearch é…ç½®
OPENSEARCH_URL=https://opensearch-cluster.internal:9200
OPENSEARCH_USERNAME=${OS_USER_FROM_VAULT}
OPENSEARCH_PASSWORD=${OS_PASSWORD_FROM_VAULT}

# ClickHouse é…ç½®
CLICKHOUSE_URL=clickhouse://${CH_USER}:${CH_PASSWORD}@clickhouse-cluster.internal:9000/production

# Redis é…ç½®
REDIS_URL=redis://redis-cluster.internal:6379/1

# å®‰å…¨é…ç½®
DJANGO_SECURE_SSL_REDIRECT=true
DJANGO_SECURE_HSTS_SECONDS=31536000
DJANGO_SECURE_HSTS_INCLUDE_SUBDOMAINS=true
DJANGO_SECURE_HSTS_PRELOAD=true
DJANGO_SECURE_CONTENT_TYPE_NOSNIFF=true
DJANGO_SECURE_BROWSER_XSS_FILTER=true
DJANGO_SECURE_REFERRER_POLICY=strict-origin-when-cross-origin

# ç›‘æ§é…ç½®
SENTRY_DSN=${SENTRY_DSN_FROM_VAULT}
PROMETHEUS_METRICS_ENABLED=true
```

### Nginx é…ç½®

#### å¤šç«™ç‚¹åå‘ä»£ç†é…ç½®

```nginx
# /etc/nginx/sites-available/multi-site
upstream django_backend {
    least_conn;
    server app1:8000 max_fails=3 fail_timeout=30s;
    server app2:8000 max_fails=3 fail_timeout=30s;
    server app3:8000 max_fails=3 fail_timeout=30s;
}

# æ—¥å¿—æ ¼å¼
log_format multi_site '$remote_addr - $remote_user [$time_local] '
                     '"$request" $status $body_bytes_sent '
                     '"$http_referer" "$http_user_agent" '
                     '"$http_host" $request_time';

# ç«™ç‚¹Aé…ç½®
server {
    listen 80;
    listen 443 ssl http2;
    server_name site-a.com www.site-a.com;
    
    # SSLé…ç½®
    ssl_certificate /etc/nginx/ssl/site-a.com.crt;
    ssl_certificate_key /etc/nginx/ssl/site-a.com.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
    
    # å®‰å…¨å¤´
    add_header X-Frame-Options DENY always;
    add_header X-Content-Type-Options nosniff always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    
    # æ—¥å¿—
    access_log /var/log/nginx/site-a-access.log multi_site;
    error_log /var/log/nginx/site-a-error.log;
    
    # é™æ€æ–‡ä»¶
    location /static/ {
        alias /app/static/;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
    
    location /media/ {
        alias /app/media/;
        expires 1y;
        add_header Cache-Control "public";
    }
    
    # APIè¯·æ±‚
    location /api/ {
        proxy_pass http://django_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $host;
        proxy_set_header X-Forwarded-Port $server_port;
        
        # ç¼“å­˜é…ç½®
        proxy_cache_bypass $http_pragma;
        proxy_cache_revalidate on;
        proxy_cache_min_uses 1;
        proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
        proxy_cache_background_update on;
        proxy_cache_lock on;
        
        # è¶…æ—¶é…ç½®
        proxy_connect_timeout 30s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
    
    # ç®¡ç†åå°
    location /admin/ {
        proxy_pass http://django_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # è®¿é—®æ§åˆ¶ (å¯é€‰)
        # allow 192.168.1.0/24;
        # deny all;
    }
    
    # å‰ç«¯åº”ç”¨
    location / {
        proxy_pass http://portal_frontend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}

# ç«™ç‚¹Bé…ç½®ï¼ˆç±»ä¼¼ç«™ç‚¹Aï¼‰
server {
    listen 80;
    listen 443 ssl http2;
    server_name site-b.com www.site-b.com;
    
    # ... ç±»ä¼¼é…ç½®
}

# é—¨æˆ·ç«™ç‚¹é…ç½®
server {
    listen 80;
    listen 443 ssl http2;
    server_name portal.com www.portal.com;
    
    # ... ç±»ä¼¼é…ç½®
}

# HTTPåˆ°HTTPSé‡å®šå‘
server {
    listen 80;
    server_name site-a.com site-b.com portal.com;
    return 301 https://$server_name$request_uri;
}
```

## è‡ªåŠ¨åŒ–éƒ¨ç½²

### Docker æ„å»ºè„šæœ¬

```bash
#!/bin/bash
# scripts/build.sh

set -e

VERSION=${1:-latest}
REGISTRY=${REGISTRY:-your-registry.com}

echo "ğŸš€ æ„å»ºå¤šç«™ç‚¹åº”ç”¨é•œåƒ v${VERSION}"

# æ„å»ºåç«¯é•œåƒ
echo "ğŸ“¦ æ„å»º Django åç«¯..."
docker build -t ${REGISTRY}/idp-cms/authoring:${VERSION} \
  -f docker/Dockerfile.authoring .

# æ„å»ºå‰ç«¯é•œåƒ
echo "ğŸ“¦ æ„å»º Next.js å‰ç«¯..."
docker build -t ${REGISTRY}/idp-cms/portal:${VERSION} \
  -f docker/Dockerfile.portal ./portal/next

# æ¨é€åˆ°é•œåƒä»“åº“
if [[ "${PUSH:-true}" == "true" ]]; then
  echo "ğŸ“¤ æ¨é€é•œåƒåˆ°ä»“åº“..."
  docker push ${REGISTRY}/idp-cms/authoring:${VERSION}
  docker push ${REGISTRY}/idp-cms/portal:${VERSION}
fi

echo "âœ… æ„å»ºå®Œæˆ!"
```

### éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# scripts/deploy.sh

set -e

ENVIRONMENT=${1:-staging}
VERSION=${2:-latest}

echo "ğŸš€ éƒ¨ç½²åˆ° ${ENVIRONMENT} ç¯å¢ƒ (ç‰ˆæœ¬: ${VERSION})"

# æ£€æŸ¥ç¯å¢ƒ
case $ENVIRONMENT in
  "staging"|"production")
    ;;
  *)
    echo "âŒ æ— æ•ˆçš„ç¯å¢ƒ: $ENVIRONMENT"
    exit 1
    ;;
esac

# è®¾ç½®ç¯å¢ƒå˜é‡
export COMPOSE_FILE="docker-compose.${ENVIRONMENT}.yml"
export IMAGE_TAG=${VERSION}

# å¤‡ä»½å½“å‰é…ç½®
echo "ğŸ’¾ å¤‡ä»½å½“å‰é…ç½®..."
docker compose ps > "backup/services-before-${VERSION}-$(date +%Y%m%d_%H%M%S).txt"

# æ‹‰å–æ–°é•œåƒ
echo "ğŸ“¥ æ‹‰å–æ–°é•œåƒ..."
docker compose pull

# æ•°æ®åº“è¿ç§»
echo "ğŸ—„ï¸  æ‰§è¡Œæ•°æ®åº“è¿ç§»..."
docker compose run --rm authoring python authoring/manage.py migrate

# æ›´æ–°é™æ€æ–‡ä»¶
echo "ğŸ“„ æ”¶é›†é™æ€æ–‡ä»¶..."
docker compose run --rm authoring python authoring/manage.py collectstatic --noinput

# æ›´æ–°æœç´¢ç´¢å¼•
echo "ğŸ” æ›´æ–°æœç´¢ç´¢å¼•..."
docker compose run --rm authoring python authoring/manage.py setup_sites

# æ»šåŠ¨æ›´æ–°
echo "ğŸ”„ æ‰§è¡Œæ»šåŠ¨æ›´æ–°..."
docker compose up -d --remove-orphans

# å¥åº·æ£€æŸ¥
echo "ğŸ¥ å¥åº·æ£€æŸ¥..."
./scripts/health_check.sh

echo "âœ… éƒ¨ç½²å®Œæˆ!"
```

### å¥åº·æ£€æŸ¥è„šæœ¬

```bash
#!/bin/bash
# scripts/health_check.sh

set -e

TIMEOUT=300  # 5åˆ†é’Ÿè¶…æ—¶
INTERVAL=10  # 10ç§’é—´éš”æ£€æŸ¥

sites=("localhost" "site-a.local" "site-b.local" "portal.local")
health_check_url="/api/health"

echo "ğŸ¥ æ‰§è¡Œå¥åº·æ£€æŸ¥..."

check_service() {
    local site=$1
    local url="http://localhost:8000${health_check_url}"
    
    echo "æ£€æŸ¥ ${site}..."
    
    response=$(curl -s -w "%{http_code}" \
                   -H "Host: ${site}" \
                   -H "Accept: application/json" \
                   -o /dev/null \
                   "${url}" || echo "000")
    
    if [[ "$response" == "200" ]]; then
        echo "âœ… ${site} å¥åº·"
        return 0
    else
        echo "âŒ ${site} ä¸å¥åº· (HTTP ${response})"
        return 1
    fi
}

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# æ£€æŸ¥æ‰€æœ‰ç«™ç‚¹
start_time=$(date +%s)
all_healthy=false

while [[ $all_healthy == false ]]; do
    current_time=$(date +%s)
    elapsed=$((current_time - start_time))
    
    if [[ $elapsed -gt $TIMEOUT ]]; then
        echo "âŒ å¥åº·æ£€æŸ¥è¶…æ—¶ (${TIMEOUT}ç§’)"
        exit 1
    fi
    
    echo "æ£€æŸ¥è½®æ¬¡ $((elapsed / INTERVAL + 1))..."
    
    healthy_count=0
    for site in "${sites[@]}"; do
        if check_service "$site"; then
            ((healthy_count++))
        fi
    done
    
    if [[ $healthy_count -eq ${#sites[@]} ]]; then
        all_healthy=true
        echo "ğŸ‰ æ‰€æœ‰ç«™ç‚¹å¥åº·æ£€æŸ¥é€šè¿‡!"
    else
        echo "â³ ç­‰å¾… ${INTERVAL} ç§’åé‡è¯•..."
        sleep $INTERVAL
    fi
done

# è¯¦ç»†çš„APIåŠŸèƒ½æµ‹è¯•
echo ""
echo "ğŸ§ª æ‰§è¡ŒåŠŸèƒ½æµ‹è¯•..."

for site in "${sites[@]}"; do
    echo "æµ‹è¯• ${site} çš„ Feed API..."
    
    response=$(curl -s \
                   -H "Host: ${site}" \
                   -H "Accept: application/json" \
                   "http://localhost:8000/api/feed?size=1")
    
    # æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«é¢„æœŸå­—æ®µ
    if echo "$response" | jq -e '.items | length' > /dev/null 2>&1; then
        item_count=$(echo "$response" | jq '.items | length')
        site_returned=$(echo "$response" | jq -r '.debug.site')
        echo "âœ… ${site}: è¿”å› ${item_count} ç¯‡æ–‡ç« ï¼Œè¯†åˆ«ç«™ç‚¹: ${site_returned}"
    else
        echo "âŒ ${site}: APIå“åº”æ ¼å¼é”™è¯¯"
        echo "å“åº”å†…å®¹: $response"
        exit 1
    fi
done

echo "ğŸ‰ æ‰€æœ‰åŠŸèƒ½æµ‹è¯•é€šè¿‡!"
```

### CI/CD é…ç½®

#### GitHub Actions

```yaml
# .github/workflows/deploy.yml
name: Multi-Site Deployment

on:
  push:
    branches:
      - main
      - staging
  pull_request:
    branches:
      - main

env:
  REGISTRY: ghcr.io
  IMAGE_BASE: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          
      opensearch:
        image: opensearchproject/opensearch:2.11.0
        env:
          discovery.type: single-node
          DISABLE_SECURITY_PLUGIN: true
        options: >-
          --health-cmd "curl -f http://localhost:9200/_cluster/health"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install dependencies
      run: |
        cd authoring
        pip install -r requirements.txt
        
    - name: Run tests
      env:
        DATABASE_URL: postgresql://postgres:test_password@localhost/test_db
        OPENSEARCH_URL: http://localhost:9200
      run: |
        cd authoring
        python manage.py test
        
    - name: Test multi-site functionality
      run: |
        cd authoring
        python manage.py check
        python manage.py setup_sites --dry-run

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/authoring
          ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/portal
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha
          
    - name: Build and push Django image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/Dockerfile.authoring
        push: true
        tags: ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/authoring:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        
    - name: Build and push Next.js image
      uses: docker/build-push-action@v5
      with:
        context: ./portal/next
        file: ./docker/Dockerfile.portal
        push: true
        tags: ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/portal:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy-staging:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/staging'
    environment: staging
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to staging
      run: |
        echo "ğŸš€ éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ..."
        # å®é™…éƒ¨ç½²é€»è¾‘
        ./scripts/deploy.sh staging ${{ github.sha }}
        
  deploy-production:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to production
      run: |
        echo "ğŸš€ éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ..."
        # å®é™…éƒ¨ç½²é€»è¾‘
        ./scripts/deploy.sh production ${{ github.sha }}
```

## ç›‘æ§å‘Šè­¦

### Prometheus ç›‘æ§é…ç½®

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # Django åº”ç”¨ç›‘æ§
  - job_name: 'django-multi-site'
    static_configs:
      - targets: ['app1:8000', 'app2:8000', 'app3:8000']
    metrics_path: '/metrics'
    scrape_interval: 30s
    
  # OpenSearch ç›‘æ§
  - job_name: 'opensearch'
    static_configs:
      - targets: ['opensearch:9200']
    metrics_path: '/_prometheus/metrics'
    
  # ClickHouse ç›‘æ§
  - job_name: 'clickhouse'
    static_configs:
      - targets: ['clickhouse:9363']
      
  # Redis ç›‘æ§
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
      
  # Nginx ç›‘æ§
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx-exporter:9113']
```

### å‘Šè­¦è§„åˆ™

```yaml
# monitoring/rules/multi-site.yml
groups:
- name: multi-site-alerts
  rules:
  
  # API å“åº”æ—¶é—´å‘Šè­¦
  - alert: HighAPIResponseTime
    expr: histogram_quantile(0.95, rate(django_request_duration_seconds_bucket[5m])) > 2
    for: 5m
    labels:
      severity: warning
      service: django
    annotations:
      summary: "APIå“åº”æ—¶é—´è¿‡é«˜"
      description: "{{ $labels.site }} ç«™ç‚¹çš„95%è¯·æ±‚å“åº”æ—¶é—´è¶…è¿‡2ç§’"
      
  # API é”™è¯¯ç‡å‘Šè­¦
  - alert: HighAPIErrorRate
    expr: rate(django_request_exceptions_total[5m]) / rate(django_requests_total[5m]) > 0.05
    for: 5m
    labels:
      severity: critical
      service: django
    annotations:
      summary: "APIé”™è¯¯ç‡è¿‡é«˜"
      description: "{{ $labels.site }} ç«™ç‚¹çš„APIé”™è¯¯ç‡è¶…è¿‡5%"
      
  # OpenSearch ç´¢å¼•å¥åº·å‘Šè­¦
  - alert: OpenSearchIndexDown
    expr: elasticsearch_cluster_health_status{color!="green"} > 0
    for: 2m
    labels:
      severity: critical
      service: opensearch
    annotations:
      summary: "OpenSearchç´¢å¼•çŠ¶æ€å¼‚å¸¸"
      description: "OpenSearché›†ç¾¤çŠ¶æ€ä¸º {{ $labels.color }}"
      
  # ç«™ç‚¹ç‰¹å®šçš„æµé‡å‘Šè­¦
  - alert: LowSiteTraffic
    expr: rate(django_requests_total[10m]) < 1
    for: 10m
    labels:
      severity: warning
      service: django
    annotations:
      summary: "ç«™ç‚¹æµé‡å¼‚å¸¸ä½"
      description: "{{ $labels.site }} ç«™ç‚¹åœ¨è¿‡å»10åˆ†é’Ÿå†…è¯·æ±‚é‡ä½äº1 req/sec"
      
  # å†…å­˜ä½¿ç”¨å‘Šè­¦
  - alert: HighMemoryUsage
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
    for: 5m
    labels:
      severity: warning
      service: system
    annotations:
      summary: "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
      description: "æœåŠ¡å™¨å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡85%: {{ $value }}%"
      
  # ç£ç›˜ç©ºé—´å‘Šè­¦
  - alert: LowDiskSpace
    expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 80
    for: 5m
    labels:
      severity: warning
      service: system
    annotations:
      summary: "ç£ç›˜ç©ºé—´ä¸è¶³"
      description: "{{ $labels.mountpoint }} ç£ç›˜ä½¿ç”¨ç‡è¶…è¿‡80%: {{ $value }}%"
```

### Grafana ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "å¤šç«™ç‚¹ç›‘æ§ä»ªè¡¨æ¿",
    "panels": [
      {
        "title": "ç«™ç‚¹QPS",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(django_requests_total[1m])",
            "legendFormat": "{{ site }} - {{ method }}"
          }
        ]
      },
      {
        "title": "ç«™ç‚¹å“åº”æ—¶é—´",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(django_request_duration_seconds_bucket[5m]))",
            "legendFormat": "{{ site }} - P50"
          },
          {
            "expr": "histogram_quantile(0.95, rate(django_request_duration_seconds_bucket[5m]))",
            "legendFormat": "{{ site }} - P95"
          }
        ]
      },
      {
        "title": "ç«™ç‚¹é”™è¯¯ç‡",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(django_request_exceptions_total[5m]) / rate(django_requests_total[5m]) * 100",
            "legendFormat": "{{ site }} é”™è¯¯ç‡ (%)"
          }
        ]
      },
      {
        "title": "OpenSearch ç´¢å¼•çŠ¶æ€",
        "type": "table",
        "targets": [
          {
            "expr": "elasticsearch_indices_docs{index=~\"news_.*_articles.*\"}",
            "format": "table"
          }
        ]
      }
    ]
  }
}
```

## å¤‡ä»½æ¢å¤

### æ•°æ®å¤‡ä»½ç­–ç•¥

#### PostgreSQL å¤‡ä»½

```bash
#!/bin/bash
# scripts/backup_postgres.sh

set -e

BACKUP_DIR="/backups/postgres"
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=30

echo "ğŸ—„ï¸  å¼€å§‹ PostgreSQL å¤‡ä»½..."

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# å…¨é‡å¤‡ä»½
docker exec postgres pg_dumpall -U postgres | \
  gzip > "$BACKUP_DIR/full_backup_$DATE.sql.gz"

# æŒ‰ç«™ç‚¹å¤‡ä»½ï¼ˆå¦‚æœéœ€è¦ï¼‰
sites=("localhost" "site_a_local" "site_b_local" "portal_local")
for site in "${sites[@]}"; do
  echo "å¤‡ä»½ç«™ç‚¹æ•°æ®: $site"
  docker exec postgres pg_dump -U postgres idp_cms_prod \
    --table="*$site*" | \
    gzip > "$BACKUP_DIR/site_${site}_$DATE.sql.gz"
done

# æ¸…ç†æ—§å¤‡ä»½
find $BACKUP_DIR -name "*.sql.gz" -mtime +$RETENTION_DAYS -delete

echo "âœ… PostgreSQL å¤‡ä»½å®Œæˆ"
```

#### OpenSearch å¤‡ä»½

```bash
#!/bin/bash
# scripts/backup_opensearch.sh

set -e

BACKUP_DIR="/backups/opensearch"
DATE=$(date +%Y%m%d_%H%M%S)
OS_URL="http://localhost:9200"
OS_USER="admin"
OS_PASS="admin"

echo "ğŸ” å¼€å§‹ OpenSearch å¤‡ä»½..."

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# è·å–æ‰€æœ‰å¤šç«™ç‚¹ç´¢å¼•
indices=$(curl -s -u $OS_USER:$OS_PASS "$OS_URL/_cat/indices?h=index" | \
          grep "news_.*_articles" | tr '\n' ',')

if [[ -n "$indices" ]]; then
  # åˆ›å»ºå¿«ç…§ä»“åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
  curl -s -u $OS_USER:$OS_PASS -X PUT "$OS_URL/_snapshot/backup_repo" \
    -H "Content-Type: application/json" \
    -d '{
      "type": "fs",
      "settings": {
        "location": "/backup",
        "compress": true
      }
    }'
  
  # åˆ›å»ºå¿«ç…§
  snapshot_name="multi_site_backup_$DATE"
  curl -s -u $OS_USER:$OS_PASS -X PUT \
    "$OS_URL/_snapshot/backup_repo/$snapshot_name" \
    -H "Content-Type: application/json" \
    -d "{
      \"indices\": \"${indices%,}\",
      \"include_global_state\": false
    }"
  
  echo "âœ… OpenSearch å¿«ç…§åˆ›å»ºå®Œæˆ: $snapshot_name"
else
  echo "âš ï¸  æœªæ‰¾åˆ°éœ€è¦å¤‡ä»½çš„ç´¢å¼•"
fi
```

### æ•°æ®æ¢å¤æµç¨‹

#### ç¾éš¾æ¢å¤è„šæœ¬

```bash
#!/bin/bash
# scripts/disaster_recovery.sh

set -e

BACKUP_DATE=${1:-latest}
RESTORE_SITE=${2:-all}

echo "ğŸš¨ å¼€å§‹ç¾éš¾æ¢å¤ (å¤‡ä»½æ—¥æœŸ: $BACKUP_DATE, ç«™ç‚¹: $RESTORE_SITE)"

# åœæ­¢åº”ç”¨æœåŠ¡
echo "â¸ï¸  åœæ­¢åº”ç”¨æœåŠ¡..."
docker compose stop authoring portal

# æ¢å¤ PostgreSQL
if [[ "$RESTORE_SITE" == "all" ]]; then
  echo "ğŸ—„ï¸  æ¢å¤ PostgreSQL å…¨é‡æ•°æ®..."
  if [[ "$BACKUP_DATE" == "latest" ]]; then
    backup_file=$(ls -t /backups/postgres/full_backup_*.sql.gz | head -1)
  else
    backup_file="/backups/postgres/full_backup_$BACKUP_DATE.sql.gz"
  fi
  
  if [[ -f "$backup_file" ]]; then
    gunzip -c "$backup_file" | docker exec -i postgres psql -U postgres
    echo "âœ… PostgreSQL æ¢å¤å®Œæˆ"
  else
    echo "âŒ å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $backup_file"
    exit 1
  fi
else
  echo "ğŸ—„ï¸  æ¢å¤ç«™ç‚¹æ•°æ®: $RESTORE_SITE"
  # ç«™ç‚¹ç‰¹å®šæ¢å¤é€»è¾‘
fi

# æ¢å¤ OpenSearch
echo "ğŸ” æ¢å¤ OpenSearch æ•°æ®..."
# OpenSearch æ¢å¤é€»è¾‘

# é‡æ–°å¯åŠ¨æœåŠ¡
echo "ğŸš€ é‡æ–°å¯åŠ¨æœåŠ¡..."
docker compose up -d

# å¥åº·æ£€æŸ¥
echo "ğŸ¥ æ‰§è¡Œå¥åº·æ£€æŸ¥..."
./scripts/health_check.sh

echo "âœ… ç¾éš¾æ¢å¤å®Œæˆ"
```

## æ€§èƒ½ä¼˜åŒ–

### æ•°æ®åº“ä¼˜åŒ–

#### PostgreSQL é…ç½®ä¼˜åŒ–

```sql
-- postgresql.conf ä¼˜åŒ–é…ç½®
shared_buffers = 4GB                    -- 25% of total RAM
effective_cache_size = 12GB              -- 75% of total RAM
maintenance_work_mem = 1GB
checkpoint_segments = 32
checkpoint_completion_target = 0.9
wal_buffers = 16MB
default_statistics_target = 100
random_page_cost = 1.1                  -- SSDä¼˜åŒ–
effective_io_concurrency = 200          -- SSDå¹¶å‘ä¼˜åŒ–

-- ç«™ç‚¹ç‰¹å®šç´¢å¼•ä¼˜åŒ–
CREATE INDEX CONCURRENTLY idx_articles_site_publish_time 
  ON articles (site, publish_time DESC);
  
CREATE INDEX CONCURRENTLY idx_articles_site_topic_score 
  ON articles (site, topic, quality_score DESC);
  
-- åˆ†åŒºè¡¨ä¼˜åŒ–ï¼ˆæŒ‰ç«™ç‚¹åˆ†åŒºï¼‰
CREATE TABLE articles_partitioned (
  id SERIAL,
  site VARCHAR(100),
  title TEXT,
  content TEXT,
  publish_time TIMESTAMP,
  -- å…¶ä»–å­—æ®µ...
) PARTITION BY LIST (site);

-- ä¸ºæ¯ä¸ªç«™ç‚¹åˆ›å»ºåˆ†åŒº
CREATE TABLE articles_localhost PARTITION OF articles_partitioned
  FOR VALUES IN ('localhost');
  
CREATE TABLE articles_site_a PARTITION OF articles_partitioned
  FOR VALUES IN ('site-a.local');
```

### OpenSearch ä¼˜åŒ–

#### ç´¢å¼•é…ç½®ä¼˜åŒ–

```json
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 1,
    "refresh_interval": "30s",
    "index": {
      "codec": "best_compression",
      "max_result_window": 10000,
      "mapping": {
        "total_fields": {
          "limit": 2000
        }
      },
      "search": {
        "slowlog": {
          "threshold": {
            "query": {
              "warn": "10s",
              "info": "5s"
            }
          }
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "site": {
        "type": "keyword"
      },
      "title": {
        "type": "text",
        "analyzer": "ik_max_word",
        "search_analyzer": "ik_smart"
      },
      "body": {
        "type": "text",
        "analyzer": "ik_max_word"
      },
      "publish_time": {
        "type": "date",
        "format": "strict_date_optional_time||epoch_millis"
      },
      "quality_score": {
        "type": "float"
      },
      "ctr_1h": {
        "type": "float"
      }
    }
  }
}
```

### åº”ç”¨å±‚ä¼˜åŒ–

#### Django é…ç½®ä¼˜åŒ–

```python
# authoring/authoring/settings/production.py

# æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
DATABASES['default'].update({
    'CONN_MAX_AGE': 600,
    'OPTIONS': {
        'MAX_CONNS': 20,
        'MIN_CONNS': 5,
    }
})

# ç¼“å­˜é…ç½®
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://redis-cluster:6379/1',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_KWARGS': {
                'max_connections': 50,
                'retry_on_timeout': True,
            },
            'COMPRESSOR': 'django_redis.compressors.zlib.ZlibCompressor',
            'SERIALIZER': 'django_redis.serializers.json.JSONSerializer',
        },
        'TIMEOUT': 300,
        'KEY_PREFIX': 'multisite',
        'VERSION': 1,
    }
}

# ä¼šè¯é…ç½®
SESSION_ENGINE = 'django.contrib.sessions.backends.cache'
SESSION_CACHE_ALIAS = 'default'
SESSION_COOKIE_AGE = 86400  # 24å°æ—¶

# ç«™ç‚¹ç‰¹å®šç¼“å­˜
SITE_CACHE_TIMEOUT = {
    'localhost': 300,      # 5åˆ†é’Ÿ
    'site-a.local': 600,   # 10åˆ†é’Ÿ
    'site-b.local': 600,   # 10åˆ†é’Ÿ
    'portal.local': 1800,  # 30åˆ†é’Ÿ
}
```

#### API ç¼“å­˜ç­–ç•¥

```python
# apps/api/cache.py
from django.core.cache import cache
from django.utils.encoding import force_str
import hashlib

class SiteBasedCacheMiddleware:
    def __init__(self, get_response):
        self.get_response = get_response
    
    def __call__(self, request):
        # ç”Ÿæˆç«™ç‚¹ç‰¹å®šçš„ç¼“å­˜é”®
        site = request.get_host()
        cache_key = self.make_cache_key(request, site)
        
        # æ£€æŸ¥ç¼“å­˜
        response = cache.get(cache_key)
        if response is not None:
            return response
        
        # å¤„ç†è¯·æ±‚
        response = self.get_response(request)
        
        # ç¼“å­˜å“åº”ï¼ˆä»…å¯¹GETè¯·æ±‚ï¼‰
        if request.method == 'GET' and response.status_code == 200:
            timeout = SITE_CACHE_TIMEOUT.get(site, 300)
            cache.set(cache_key, response, timeout)
        
        return response
    
    def make_cache_key(self, request, site):
        path = request.get_full_path()
        key_parts = [site, path]
        key = ':'.join(key_parts)
        return hashlib.md5(force_str(key).encode()).hexdigest()
```

## æ•…éšœå¤„ç†

### å¸¸è§æ•…éšœåœºæ™¯

#### 1. ç«™ç‚¹æ— æ³•è®¿é—®

**æ•…éšœç°è±¡**: ç‰¹å®šç«™ç‚¹è¿”å›502/503é”™è¯¯

**æ’æŸ¥æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker compose ps

# 2. æ£€æŸ¥å®¹å™¨æ—¥å¿—
docker compose logs -f authoring

# 3. æ£€æŸ¥ç«™ç‚¹é…ç½®
curl -I -H "Host: problem-site.com" http://localhost:8000/api/health

# 4. æ£€æŸ¥è´Ÿè½½å‡è¡¡é…ç½®
nginx -t
systemctl status nginx

# 5. æ£€æŸ¥æ•°æ®åº“è¿æ¥
docker compose exec authoring python authoring/manage.py dbshell
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é‡å¯ç‰¹å®šæœåŠ¡
docker compose restart authoring

# é‡æ–°åŠ è½½nginxé…ç½®
nginx -s reload

# å¦‚æœéœ€è¦ï¼Œå›æ»šåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬
./scripts/rollback.sh
```

#### 2. OpenSearch ç´¢å¼•å¼‚å¸¸

**æ•…éšœç°è±¡**: æœç´¢åŠŸèƒ½å¼‚å¸¸ï¼Œè¿”å›ç©ºç»“æœ

**æ’æŸ¥æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥ç´¢å¼•çŠ¶æ€
curl -u admin:password "http://localhost:9200/_cat/indices?v"

# 2. æ£€æŸ¥é›†ç¾¤å¥åº·
curl -u admin:password "http://localhost:9200/_cluster/health?pretty"

# 3. æ£€æŸ¥ç´¢å¼•æ˜ å°„
curl -u admin:password "http://localhost:9200/news_*/_mapping"

# 4. æµ‹è¯•æŸ¥è¯¢
curl -u admin:password -X GET "http://localhost:9200/news_localhost_articles/_search?q=*"
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é‡å»ºå¼‚å¸¸ç´¢å¼•
docker compose exec authoring python authoring/manage.py setup_sites --create-indices

# å¦‚æœéœ€è¦ï¼Œä»å¤‡ä»½æ¢å¤
./scripts/restore_opensearch.sh latest
```

#### 3. æ•°æ®åº“æ€§èƒ½é—®é¢˜

**æ•…éšœç°è±¡**: APIå“åº”ç¼“æ…¢ï¼Œæ•°æ®åº“CPUä½¿ç”¨ç‡é«˜

**æ’æŸ¥æ­¥éª¤**:
```sql
-- æ£€æŸ¥æ…¢æŸ¥è¯¢
SELECT query, mean_exec_time, calls
FROM pg_stat_statements
WHERE mean_exec_time > 1000
ORDER BY mean_exec_time DESC;

-- æ£€æŸ¥é”ç­‰å¾…
SELECT * FROM pg_stat_activity 
WHERE wait_event IS NOT NULL;

-- æ£€æŸ¥è¿æ¥æ•°
SELECT count(*) FROM pg_stat_activity;
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é‡å¯æ•°æ®åº“è¿æ¥æ± 
docker compose restart authoring

# åˆ†æå¹¶ä¼˜åŒ–æ…¢æŸ¥è¯¢
# æ·»åŠ å¿…è¦çš„ç´¢å¼•
# è°ƒæ•´æ•°æ®åº“é…ç½®å‚æ•°
```

### æ•…éšœè‡ªåŠ¨æ¢å¤

#### è‡ªåŠ¨é‡å¯è„šæœ¬

```bash
#!/bin/bash
# scripts/auto_recovery.sh

set -e

HEALTH_CHECK_URL="http://localhost:8000/api/health"
MAX_FAILURES=3
FAILURE_COUNT=0

while true; do
    sites=("localhost" "site-a.local" "site-b.local" "portal.local")
    
    for site in "${sites[@]}"; do
        echo "æ£€æŸ¥ç«™ç‚¹: $site"
        
        if ! curl -f -s -H "Host: $site" "$HEALTH_CHECK_URL" > /dev/null; then
            echo "âŒ ç«™ç‚¹ $site å¥åº·æ£€æŸ¥å¤±è´¥"
            ((FAILURE_COUNT++))
            
            if [[ $FAILURE_COUNT -ge $MAX_FAILURES ]]; then
                echo "ğŸš¨ è§¦å‘è‡ªåŠ¨æ¢å¤..."
                
                # å°è¯•é‡å¯æœåŠ¡
                docker compose restart authoring
                sleep 30
                
                # é‡æ–°æ£€æŸ¥
                if curl -f -s -H "Host: $site" "$HEALTH_CHECK_URL" > /dev/null; then
                    echo "âœ… è‡ªåŠ¨æ¢å¤æˆåŠŸ"
                    FAILURE_COUNT=0
                else
                    echo "âŒ è‡ªåŠ¨æ¢å¤å¤±è´¥ï¼Œå‘é€å‘Šè­¦"
                    # å‘é€å‘Šè­¦é€šçŸ¥
                    ./scripts/send_alert.sh "è‡ªåŠ¨æ¢å¤å¤±è´¥: $site"
                fi
            fi
        else
            echo "âœ… ç«™ç‚¹ $site æ­£å¸¸"
            FAILURE_COUNT=0
        fi
    done
    
    sleep 60  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
done
```

## æ‰©å®¹æ–¹æ¡ˆ

### æ°´å¹³æ‰©å®¹

#### åº”ç”¨å±‚æ‰©å®¹

```bash
# å¢åŠ Djangoåº”ç”¨å®ä¾‹
docker compose up -d --scale authoring=5

# æ›´æ–°è´Ÿè½½å‡è¡¡é…ç½®
# æ·»åŠ æ–°çš„upstreamæœåŠ¡å™¨åˆ°nginxé…ç½®
```

#### æ•°æ®åº“æ‰©å®¹

```yaml
# è¯»å†™åˆ†ç¦»é…ç½®
version: '3.8'
services:
  postgres-master:
    image: postgres:15
    environment:
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD: repl_password
    
  postgres-replica1:
    image: postgres:15
    environment:
      POSTGRES_MASTER_SERVICE: postgres-master
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD: repl_password
    
  postgres-replica2:
    image: postgres:15
    environment:
      POSTGRES_MASTER_SERVICE: postgres-master
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD: repl_password
```

#### OpenSearch é›†ç¾¤æ‰©å®¹

```yaml
# OpenSearch é›†ç¾¤é…ç½®
version: '3.8'
services:
  opensearch-master:
    image: opensearchproject/opensearch:2.11.0
    environment:
      - cluster.name=multi-site-cluster
      - node.name=opensearch-master
      - node.roles=master,data,ingest
      - discovery.seed_hosts=opensearch-data1,opensearch-data2
      - cluster.initial_master_nodes=opensearch-master
      
  opensearch-data1:
    image: opensearchproject/opensearch:2.11.0
    environment:
      - cluster.name=multi-site-cluster
      - node.name=opensearch-data1
      - node.roles=data,ingest
      - discovery.seed_hosts=opensearch-master,opensearch-data2
      
  opensearch-data2:
    image: opensearchproject/opensearch:2.11.0
    environment:
      - cluster.name=multi-site-cluster
      - node.name=opensearch-data2
      - node.roles=data,ingest
      - discovery.seed_hosts=opensearch-master,opensearch-data1
```

### è‡ªåŠ¨æ‰©å®¹

#### Kubernetes éƒ¨ç½²

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-site-authoring
spec:
  replicas: 3
  selector:
    matchLabels:
      app: authoring
  template:
    metadata:
      labels:
        app: authoring
    spec:
      containers:
      - name: authoring
        image: idp-cms/authoring:latest
        ports:
        - containerPort: 8000
        env:
        - name: MULTI_SITE_ENABLED
          value: "true"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
        livenessProbe:
          httpGet:
            path: /api/health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: authoring-service
spec:
  selector:
    app: authoring
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: authoring-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: multi-site-authoring
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

---

## æ€»ç»“

å¤šç«™ç‚¹éƒ¨ç½²è¿ç»´æ¶µç›–äº†ä»ç¯å¢ƒå‡†å¤‡åˆ°è‡ªåŠ¨åŒ–æ‰©å®¹çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼š

âœ… **å…¨é¢çš„ç¯å¢ƒé…ç½®** - å¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ç¯å¢ƒçš„æ ‡å‡†åŒ–é…ç½®  
âœ… **è‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹** - CI/CD pipeline å’Œè„šæœ¬åŒ–éƒ¨ç½²  
âœ… **å®Œå–„çš„ç›‘æ§ä½“ç³»** - Prometheus + Grafana + å‘Šè­¦è§„åˆ™  
âœ… **å¯é çš„å¤‡ä»½æ¢å¤** - æ•°æ®åº“å’Œæœç´¢å¼•æ“çš„å¤‡ä»½ç­–ç•¥  
âœ… **æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ** - åº”ç”¨ã€æ•°æ®åº“ã€æœç´¢å¼•æ“çš„ä¼˜åŒ–é…ç½®  
âœ… **æ•…éšœå¤„ç†é¢„æ¡ˆ** - å¸¸è§æ•…éšœçš„æ’æŸ¥å’Œè‡ªåŠ¨æ¢å¤æœºåˆ¶  
âœ… **å¼¹æ€§æ‰©å®¹èƒ½åŠ›** - æ”¯æŒæ°´å¹³å’Œå‚ç›´æ‰©å®¹çš„æ¶æ„è®¾è®¡  

è¿™å¥—è¿ç»´ä½“ç³»ä¸ºå¤šç«™ç‚¹æ¶æ„æä¾›äº†ä¼ä¸šçº§çš„å¯é æ€§å’Œå¯æ‰©å±•æ€§ä¿éšœã€‚
