
#!/usr/bin/env python3
"""
æœ¬åœ°è¿è¡Œçš„æ‰¹é‡å¯¼å…¥è„šæœ¬
è¿æ¥MySQLå¹¶é€šè¿‡SSHè°ƒç”¨è¿œç¨‹Djangoå‘½ä»¤å¯¼å…¥

ä½¿ç”¨æ–¹æ³•ï¼š
    python3 scripts/batch_import_final.py
"""

import pymysql
import json
import subprocess
import tempfile
import os
import time
from pathlib import Path

# é…ç½®
MYSQL_CONFIG = {
    'host': '121.41.73.49',
    'port': 3306,
    'user': 'jrhb',
    'password': '6VSPmPbuFGnZO1%C',
    'database': 'jrhb',
    'charset': 'utf8mb4'
}

REMOTE_HOST = 'root@121.40.167.71'
REMOTE_DIR = '/opt/idp-cms'
BATCH_SIZE = 1000
IMPORT_BATCH_SIZE = 500

def main():
    print('=' * 80)
    print('ğŸ“¦ å¼€å§‹æ‰¹é‡å¯¼å…¥æ–‡ç« ')
    print('=' * 80)
    print()
    
    start_time = time.time()
    
    # è¿æ¥MySQL
    print('è¿æ¥MySQLæ•°æ®åº“...')
    try:
        conn = pymysql.connect(**MYSQL_CONFIG)
        cursor = conn.cursor(pymysql.cursors.DictCursor)
        print('âœ“ MySQLè¿æ¥æˆåŠŸ')
    except Exception as e:
        print(f'âœ— MySQLè¿æ¥å¤±è´¥: {e}')
        return
    
    # æŸ¥è¯¢æ€»æ•°
    cursor.execute("SELECT COUNT(*) as total FROM article WHERE status = 1")
    total = cursor.fetchone()['total']
    print(f'âœ“ æ€»æ–‡ç« æ•°: {total:,}')
    print()
    
    # ç»Ÿè®¡
    stats = {
        'total': 0,
        'success': 0,
        'failed': 0,
        'batches': 0
    }
    
    # åˆ†æ‰¹å¤„ç†
    offset = 0
    while offset < total:
        batch_limit = min(BATCH_SIZE, total - offset)
        stats['batches'] += 1
        
        print(f'ğŸ“¦ æ‰¹æ¬¡ {stats["batches"]} | Offset: {offset:,} | Size: {batch_limit}')
        
        try:
            # æŸ¥è¯¢ä¸€æ‰¹æ•°æ®
            sql = """
                SELECT 
                    a.id, a.title, a.cate_id, a.img, a.add_time, a.last_time,
                    a.author, a.tags, a.status, a.fromurl, a.seo_title,
                    a.seo_desc, a.seo_keys, i.info
                FROM article a
                LEFT JOIN article_info i ON a.id = i.article_id
                WHERE a.status = 1
                ORDER BY a.add_time DESC
                LIMIT %s OFFSET %s
            """
            
            cursor.execute(sql, (batch_limit, offset))
            articles = cursor.fetchall()
            
            if not articles:
                print('âœ“ æ²¡æœ‰æ›´å¤šæ•°æ®')
                break
            
            # å¤„ç†æ—¶é—´æˆ³
            for article in articles:
                if article['add_time']:
                    article['add_time'] = int(article['add_time'])
                if article['last_time']:
                    article['last_time'] = int(article['last_time'])
            
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            temp_file = f'/tmp/batch_{stats["batches"]}.json'
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            
            # ä¸Šä¼ åˆ°è¿œç¨‹æœåŠ¡å™¨
            remote_file = f'{REMOTE_DIR}/data/migration/exports/batch_{stats["batches"]}.json'
            upload_cmd = f'scp {temp_file} {REMOTE_HOST}:{remote_file}'
            subprocess.run(upload_cmd, shell=True, check=True, capture_output=True)
            
            # è¿œç¨‹å¯¼å…¥
            import_cmd = (
                f'ssh {REMOTE_HOST} "cd {REMOTE_DIR} && '
                f'docker compose -f infra/production/docker-compose-ha-node1.yml exec authoring '
                f'python manage.py import_old_articles '
                f'--file={remote_file} '
                f'--batch-size={IMPORT_BATCH_SIZE} '
                f'--skip-inline-images"'
            )
            
            result = subprocess.run(import_cmd, shell=True, capture_output=True, text=True)
            
            if result.returncode == 0:
                stats['success'] += len(articles)
                print(f'âœ“ æ‰¹æ¬¡å®Œæˆ: {len(articles)} ç¯‡')
            else:
                stats['failed'] += len(articles)
                print(f'âœ— æ‰¹æ¬¡å¤±è´¥: {result.stderr[:200]}')
            
            # æ¸…ç†
            os.unlink(temp_file)
            cleanup_cmd = f'ssh {REMOTE_HOST} "rm -f {remote_file}"'
            subprocess.run(cleanup_cmd, shell=True, capture_output=True)
            
            stats['total'] += len(articles)
            offset += batch_limit
            
            # æ˜¾ç¤ºè¿›åº¦
            progress = (stats['total'] / total) * 100
            elapsed = time.time() - start_time
            avg_speed = stats['total'] / elapsed if elapsed > 0 else 0
            remaining = (total - stats['total']) / avg_speed if avg_speed > 0 else 0
            
            print(f'è¿›åº¦: {stats["total"]:,}/{total:,} ({progress:.1f}%) | '
                  f'é€Ÿåº¦: {avg_speed:.1f} ç¯‡/ç§’ | '
                  f'å‰©ä½™: {remaining/3600:.1f} å°æ—¶')
            print()
            
        except Exception as e:
            print(f'âœ— æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}')
            stats['failed'] += batch_limit
            offset += batch_limit
            continue
    
    cursor.close()
    conn.close()
    
    # æ€»ç»“
    elapsed = time.time() - start_time
    print()
    print('=' * 80)
    print('âœ… å¯¼å…¥å®Œæˆ')
    print('=' * 80)
    print(f'æ€»ç”¨æ—¶: {elapsed/3600:.2f} å°æ—¶')
    print(f'æ€»æ‰¹æ¬¡: {stats["batches"]}')
    print(f'æˆåŠŸ: {stats["success"]:,} ç¯‡')
    print(f'å¤±è´¥: {stats["failed"]:,} ç¯‡')
    print(f'å¹³å‡é€Ÿåº¦: {stats["total"]/elapsed:.1f} ç¯‡/ç§’')
    print('=' * 80)

if __name__ == '__main__':
    main()

