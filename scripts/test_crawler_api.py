#!/usr/bin/env python
"""
çˆ¬è™«APIæµ‹è¯•è„šæœ¬

ç”¨äºæµ‹è¯•çˆ¬è™«æ•°æ®å†™å…¥APIçš„åŠŸèƒ½æ˜¯å¦æ­£å¸¸
"""

import os
import sys
import requests
import json
from datetime import datetime, timezone

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class CrawlerAPITester:
    def __init__(self, base_url, api_key, client_name):
        self.base_url = base_url.rstrip('/')
        self.headers = {
            'Content-Type': 'application/json',
            'X-API-Key': api_key,
            'X-API-Client': client_name
        }
    
    def test_site_info(self, site_hostname):
        """æµ‹è¯•è·å–ç«™ç‚¹ä¿¡æ¯"""
        print(f"\nğŸ” æµ‹è¯•è·å–ç«™ç‚¹ä¿¡æ¯: {site_hostname}")
        
        url = f"{self.base_url}/api/crawler/sites/info"
        params = {'site': site_hostname}
        
        try:
            response = requests.get(url, headers=self.headers, params=params)
            
            if response.status_code == 200:
                data = response.json()
                print("âœ… ç«™ç‚¹ä¿¡æ¯è·å–æˆåŠŸ:")
                print(f"   ç«™ç‚¹åç§°: {data['site']['site_name']}")
                print(f"   é¢‘é“æ•°é‡: {len(data['channels'])}")
                print(f"   åœ°åŒºæ•°é‡: {len(data['regions'])}")
                return data
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                print(f"   é”™è¯¯ä¿¡æ¯: {response.text}")
                return None
                
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
            return None
    
    def test_check_duplicates(self, site_hostname, test_articles):
        """æµ‹è¯•é‡å¤æ£€æŸ¥"""
        print(f"\nğŸ” æµ‹è¯•é‡å¤æ–‡ç« æ£€æŸ¥")
        
        url = f"{self.base_url}/api/crawler/articles/check-duplicates"
        data = {
            'site': site_hostname,
            'articles': test_articles
        }
        
        try:
            response = requests.post(url, headers=self.headers, json=data)
            
            if response.status_code == 200:
                result = response.json()
                print("âœ… é‡å¤æ£€æŸ¥æˆåŠŸ:")
                for i, res in enumerate(result['results']):
                    status = "é‡å¤" if res['is_duplicate'] else "ä¸é‡å¤"
                    print(f"   æ–‡ç«  {i+1}: {status}")
                return result
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                print(f"   é”™è¯¯ä¿¡æ¯: {response.text}")
                return None
                
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
            return None
    
    def test_bulk_create(self, site_hostname, test_articles, dry_run=True):
        """æµ‹è¯•æ‰¹é‡åˆ›å»ºæ–‡ç« """
        mode = "è¯•è¿è¡Œ" if dry_run else "å®é™…åˆ›å»º"
        print(f"\nğŸ” æµ‹è¯•æ‰¹é‡åˆ›å»ºæ–‡ç«  ({mode})")
        
        url = f"{self.base_url}/api/crawler/articles/bulk"
        data = {
            'site': site_hostname,
            'articles': test_articles,
            'update_existing': True,
            'dry_run': dry_run
        }
        
        try:
            response = requests.post(url, headers=self.headers, json=data)
            
            if response.status_code in [200, 201]:
                result = response.json()
                print(f"âœ… æ‰¹é‡æ“ä½œæˆåŠŸ:")
                print(f"   å¤„ç†æ–‡ç« æ•°: {result['summary']['total']}")
                if not dry_run:
                    print(f"   åˆ›å»º: {result['summary']['created']}")
                    print(f"   æ›´æ–°: {result['summary']['updated']}")
                    print(f"   é”™è¯¯: {result['summary']['errors']}")
                return result
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                print(f"   é”™è¯¯ä¿¡æ¯: {response.text}")
                return None
                
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
            return None
    
    def run_full_test(self, site_hostname):
        """è¿è¡Œå®Œæ•´æµ‹è¯•æµç¨‹"""
        print("ğŸš€ å¼€å§‹çˆ¬è™«APIå®Œæ•´åŠŸèƒ½æµ‹è¯•")
        print("="*50)
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        now = datetime.now(timezone.utc).isoformat()
        test_articles = [
            {
                "title": f"çˆ¬è™«APIæµ‹è¯•æ–‡ç«  - {now[:19]}",
                "body": "<p>è¿™æ˜¯ä¸€ç¯‡é€šè¿‡çˆ¬è™«APIåˆ›å»ºçš„æµ‹è¯•æ–‡ç« ã€‚</p><p>ç”¨äºéªŒè¯APIåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚</p>",
                "excerpt": "çˆ¬è™«APIæµ‹è¯•æ–‡ç« æ‘˜è¦",
                "author_name": "APIæµ‹è¯•ç¨‹åº",
                "channel": "æµ‹è¯•",
                "language": "zh-CN",
                "topic_slug": "api-test",
                "external_article_url": f"https://test-source.com/article/{now[:10]}",
                "external_site": {
                    "domain": "test-source.com",
                    "name": "æµ‹è¯•æºç«™"
                },
                "publish_at": now,
                "has_video": False,
                "allow_aggregate": True,
                "is_featured": False,
                "weight": 0,
                "tags": ["API", "æµ‹è¯•", "çˆ¬è™«"],
                "live": False  # æµ‹è¯•æ—¶ä¸å‘å¸ƒ
            }
        ]
        
        # æµ‹è¯•æ­¥éª¤1: è·å–ç«™ç‚¹ä¿¡æ¯
        site_info = self.test_site_info(site_hostname)
        if not site_info:
            print("\nâŒ ç«™ç‚¹ä¿¡æ¯è·å–å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
            return False
        
        # æµ‹è¯•æ­¥éª¤2: æ£€æŸ¥é‡å¤
        self.test_check_duplicates(site_hostname, test_articles)
        
        # æµ‹è¯•æ­¥éª¤3: è¯•è¿è¡Œåˆ›å»º
        dry_result = self.test_bulk_create(site_hostname, test_articles, dry_run=True)
        if not dry_result:
            print("\nâŒ è¯•è¿è¡Œå¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
            return False
        
        # æµ‹è¯•æ­¥éª¤4: å®é™…åˆ›å»ºï¼ˆå¯é€‰ï¼‰
        print(f"\nâ“ æ˜¯å¦è¦æ‰§è¡Œå®é™…åˆ›å»ºæ“ä½œï¼Ÿ(y/N): ", end="")
        confirm = input().strip().lower()
        
        if confirm in ['y', 'yes']:
            actual_result = self.test_bulk_create(site_hostname, test_articles, dry_run=False)
            if actual_result and actual_result['summary']['created'] > 0:
                print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çˆ¬è™«APIåŠŸèƒ½æ­£å¸¸ã€‚")
                print("âš ï¸  æ³¨æ„ï¼šæµ‹è¯•æ–‡ç« å·²åˆ›å»ºä½†æœªå‘å¸ƒï¼Œå¯åœ¨åå°ç®¡ç†ç•Œé¢æŸ¥çœ‹ã€‚")
            else:
                print("\nâŒ å®é™…åˆ›å»ºå¤±è´¥")
                return False
        else:
            print("\nâœ… è¯•è¿è¡Œæµ‹è¯•é€šè¿‡ï¼çˆ¬è™«APIåŸºæœ¬åŠŸèƒ½æ­£å¸¸ã€‚")
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        return True


def main():
    """ä¸»å‡½æ•°"""
    print("çˆ¬è™«APIæµ‹è¯•å·¥å…·")
    print("="*50)
    
    # é…ç½®å‚æ•°
    BASE_URL = os.getenv('CMS_BASE_URL', 'http://localhost:8000')
    API_KEY = os.getenv('CRAWLER_API_KEY', 'test-api-key')
    CLIENT_NAME = os.getenv('CRAWLER_CLIENT_NAME', 'api_tester')
    SITE_HOSTNAME = os.getenv('TEST_SITE_HOSTNAME', 'localhost:8000')
    
    print(f"æµ‹è¯•é…ç½®:")
    print(f"  CMSåœ°å€: {BASE_URL}")
    print(f"  å®¢æˆ·ç«¯åç§°: {CLIENT_NAME}")
    print(f"  ç›®æ ‡ç«™ç‚¹: {SITE_HOSTNAME}")
    print(f"  APIå¯†é’¥: {'*' * len(API_KEY)}")
    
    # åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯
    tester = CrawlerAPITester(BASE_URL, API_KEY, CLIENT_NAME)
    
    # è¿è¡Œæµ‹è¯•
    success = tester.run_full_test(SITE_HOSTNAME)
    
    if success:
        print("\nâœ… æµ‹è¯•ç»“æœ: é€šè¿‡")
        sys.exit(0)
    else:
        print("\nâŒ æµ‹è¯•ç»“æœ: å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()
